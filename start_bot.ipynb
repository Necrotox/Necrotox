{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Necrotox/Necrotox/blob/main/start_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8-zPSBcWmQl",
        "outputId": "ccc6dffe-8716-4e7a-c0ae-895eddd87713"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-warn-script-location --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torch==2.7.1+cpu > /dev/null 2>&1\n",
        "\n",
        "!pip install -q --no-warn-script-location --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torchvision==0.22.0+cpu > /dev/null 2>&1\n",
        "!pip install -q --no-warn-script-location --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torchaudio==2.7.1+cpu > /dev/null 2>&1\n",
        "\n",
        "!pip install -q --no-warn-script-location \\\n",
        "  pytorch-lightning==2.5.2 pytorch-forecasting==1.4.0 > /dev/null 2>&1\n",
        "\n",
        "!pip install -q tinkoff-investments dill telebot --upgrade mplfinance > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "g3YVzfjxJVAK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1mri67_vog8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc0d45a4-7781-4136-ae52-f74389bdaa04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Данные 1 успешно загружены.\n",
            "Данные 2 успешно загружены.\n",
            "Данные 3 успешно загружены.\n",
            "Загрузка Моделей Память: 987.44 МБ\n",
            "Модель MiniBatchKMeans успешно загруженна Память: 990.91 МБ\n",
            "Скалер успешно загружен Память: 990.92 МБ\n",
            "Параметры для тикетов загруженны Память: 991.01 МБ\n",
            "Параметры для тикетов с фазами загруженны Память: 993.40 МБ\n",
            "[INIT] Активных тикеров: 1\n",
            "\n",
            "Обработка тикера MGTSP (1/1)\n",
            "       trade_bars_counter  MEDPRICE_max  MACD_Hist_mean  BB_Width_mean  \\\n",
            "121                     0        1397.0        0.000431       0.008380   \n",
            "122                     1        1397.0        0.000450       0.008610   \n",
            "123                     2        1397.0        0.000463       0.008876   \n",
            "124                     3        1397.0        0.000461       0.009173   \n",
            "125                     4        1397.0        0.000453       0.009484   \n",
            "...                   ...           ...             ...            ...   \n",
            "26170                 321         729.0       -0.000287       0.009941   \n",
            "26171                 322         729.0       -0.000258       0.009865   \n",
            "26172                 323         729.0       -0.000256       0.009825   \n",
            "26173                 324         729.0       -0.000274       0.009883   \n",
            "26174                 325         729.0       -0.000303       0.010008   \n",
            "\n",
            "       Overbought_Oversold_Index_mean  ma_pmax_norm_rage_pct_min  \\\n",
            "121                         56.024204                  -6.611758   \n",
            "122                         56.249882                  -8.389715   \n",
            "123                         56.278015                 -11.346225   \n",
            "124                         56.311878                 -16.938929   \n",
            "125                         56.482677                 -35.226227   \n",
            "...                               ...                        ...   \n",
            "26170                       47.110374                  -0.449794   \n",
            "26171                       46.594780                  -0.509265   \n",
            "26172                       45.376820                  -0.694230   \n",
            "26173                       44.929668                  -0.967768   \n",
            "26174                       44.540058                  -7.143075   \n",
            "\n",
            "       Price_MADist%_logsf_mean  ago_10_MACD  hp_trend_kurt  \\\n",
            "121                   -0.718321     0.000791       5.640642   \n",
            "122                   -0.771930     0.001053       5.621623   \n",
            "123                   -0.824841     0.001479       5.682632   \n",
            "124                   -0.869756     0.001796       6.508035   \n",
            "125                   -0.915650     0.001907       6.691656   \n",
            "...                         ...          ...            ...   \n",
            "26170                 -2.692368    -0.003995       0.781914   \n",
            "26171                 -2.615428    -0.003421       0.676425   \n",
            "26172                 -2.491244    -0.002933       1.085846   \n",
            "26173                 -2.397846    -0.002739       0.943181   \n",
            "26174                 -2.305325    -0.002111       0.943181   \n",
            "\n",
            "       ago50_Price_MADist%_logsf_std  ...  normalized_target  batch  \\\n",
            "121                         0.285108  ...          -0.500331    0.0   \n",
            "122                         0.285163  ...          -0.497837    0.0   \n",
            "123                         0.282017  ...          -0.489834    0.0   \n",
            "124                         0.279835  ...          -0.479020    0.0   \n",
            "125                         0.277702  ...          -0.476596    0.0   \n",
            "...                              ...  ...                ...    ...   \n",
            "26170                       1.917207  ...           0.114627   68.0   \n",
            "26171                       1.818833  ...           0.012108   68.0   \n",
            "26172                       1.554211  ...          -0.083254   68.0   \n",
            "26173                       1.431408  ...          -0.146211   68.0   \n",
            "26174                       1.359897  ...          -0.212486   68.0   \n",
            "\n",
            "                           time  buy_signal  sell_signal  \\\n",
            "121   2023-08-10 12:30:00+00:00        True        False   \n",
            "122   2023-08-10 12:45:00+00:00       False        False   \n",
            "123   2023-08-10 13:00:00+00:00       False        False   \n",
            "124   2023-08-10 13:15:00+00:00       False        False   \n",
            "125   2023-08-10 13:30:00+00:00       False        False   \n",
            "...                         ...         ...          ...   \n",
            "26170 2025-10-21 16:45:00+00:00       False        False   \n",
            "26171 2025-10-21 17:00:00+00:00       False        False   \n",
            "26172 2025-10-21 17:45:00+00:00       False        False   \n",
            "26173 2025-10-21 18:00:00+00:00       False        False   \n",
            "26174 2025-10-21 18:15:00+00:00       False         True   \n",
            "\n",
            "                 event_sell_time event_sell_price                 event_time  \\\n",
            "121    2023-09-08 12:00:00+00:00           1786.0  2023-08-10 12:30:00+00:00   \n",
            "122                          NaT              NaN                        NaT   \n",
            "123                          NaT              NaN                        NaT   \n",
            "124                          NaT              NaN                        NaT   \n",
            "125                          NaT              NaN                        NaT   \n",
            "...                          ...              ...                        ...   \n",
            "26170                        NaT              NaN                        NaT   \n",
            "26171                        NaT              NaN                        NaT   \n",
            "26172                        NaT              NaN                        NaT   \n",
            "26173                        NaT              NaN                        NaT   \n",
            "26174                        NaT              NaN                        NaT   \n",
            "\n",
            "       event_price        pnl  \n",
            "121         1390.0  27.720579  \n",
            "122            NaN        NaN  \n",
            "123            NaN        NaN  \n",
            "124            NaN        NaN  \n",
            "125            NaN        NaN  \n",
            "...            ...        ...  \n",
            "26170          NaN        NaN  \n",
            "26171          NaN        NaN  \n",
            "26172          NaN        NaN  \n",
            "26173          NaN        NaN  \n",
            "26174          NaN        NaN  \n",
            "\n",
            "[11431 rows x 27 columns]\n",
            "       trade_bars_counter  MEDPRICE_max  MACD_Hist_mean  BB_Width_mean  \\\n",
            "121                     0        1397.0        0.000431       0.008380   \n",
            "122                     1        1397.0        0.000450       0.008610   \n",
            "123                     2        1397.0        0.000463       0.008876   \n",
            "124                     3        1397.0        0.000461       0.009173   \n",
            "125                     4        1397.0        0.000453       0.009484   \n",
            "...                   ...           ...             ...            ...   \n",
            "26170                 321         729.0       -0.000287       0.009941   \n",
            "26171                 322         729.0       -0.000258       0.009865   \n",
            "26172                 323         729.0       -0.000256       0.009825   \n",
            "26173                 324         729.0       -0.000274       0.009883   \n",
            "26174                 325         729.0       -0.000303       0.010008   \n",
            "\n",
            "       Overbought_Oversold_Index_mean  ma_pmax_norm_rage_pct_min  \\\n",
            "121                         56.024204                  -6.611758   \n",
            "122                         56.249882                  -8.389715   \n",
            "123                         56.278015                 -11.346225   \n",
            "124                         56.311878                 -16.938929   \n",
            "125                         56.482677                 -35.226227   \n",
            "...                               ...                        ...   \n",
            "26170                       47.110374                  -0.449794   \n",
            "26171                       46.594780                  -0.509265   \n",
            "26172                       45.376820                  -0.694230   \n",
            "26173                       44.929668                  -0.967768   \n",
            "26174                       44.540058                  -7.143075   \n",
            "\n",
            "       Price_MADist%_logsf_mean  ago_10_MACD  hp_trend_kurt  \\\n",
            "121                   -0.718321     0.000791       5.640642   \n",
            "122                   -0.771930     0.001053       5.621623   \n",
            "123                   -0.824841     0.001479       5.682632   \n",
            "124                   -0.869756     0.001796       6.508035   \n",
            "125                   -0.915650     0.001907       6.691656   \n",
            "...                         ...          ...            ...   \n",
            "26170                 -2.692368    -0.003995       0.781914   \n",
            "26171                 -2.615428    -0.003421       0.676425   \n",
            "26172                 -2.491244    -0.002933       1.085846   \n",
            "26173                 -2.397846    -0.002739       0.943181   \n",
            "26174                 -2.305325    -0.002111       0.943181   \n",
            "\n",
            "       ago50_Price_MADist%_logsf_std  ...  normalized_target  batch  \\\n",
            "121                         0.285108  ...          -0.500331    0.0   \n",
            "122                         0.285163  ...          -0.497837    0.0   \n",
            "123                         0.282017  ...          -0.489834    0.0   \n",
            "124                         0.279835  ...          -0.479020    0.0   \n",
            "125                         0.277702  ...          -0.476596    0.0   \n",
            "...                              ...  ...                ...    ...   \n",
            "26170                       1.917207  ...           0.114627   68.0   \n",
            "26171                       1.818833  ...           0.012108   68.0   \n",
            "26172                       1.554211  ...          -0.083254   68.0   \n",
            "26173                       1.431408  ...          -0.146211   68.0   \n",
            "26174                       1.359897  ...          -0.212486   68.0   \n",
            "\n",
            "                       time_idx  buy_signal  sell_signal  \\\n",
            "121   2023-08-10 12:30:00+00:00        True        False   \n",
            "122   2023-08-10 12:45:00+00:00       False        False   \n",
            "123   2023-08-10 13:00:00+00:00       False        False   \n",
            "124   2023-08-10 13:15:00+00:00       False        False   \n",
            "125   2023-08-10 13:30:00+00:00       False        False   \n",
            "...                         ...         ...          ...   \n",
            "26170 2025-10-21 16:45:00+00:00       False        False   \n",
            "26171 2025-10-21 17:00:00+00:00       False        False   \n",
            "26172 2025-10-21 17:45:00+00:00       False        False   \n",
            "26173 2025-10-21 18:00:00+00:00       False        False   \n",
            "26174 2025-10-21 18:15:00+00:00       False         True   \n",
            "\n",
            "                 event_sell_time event_sell_price                 event_time  \\\n",
            "121    2023-09-08 12:00:00+00:00           1786.0  2023-08-10 12:30:00+00:00   \n",
            "122                          NaT              NaN                        NaT   \n",
            "123                          NaT              NaN                        NaT   \n",
            "124                          NaT              NaN                        NaT   \n",
            "125                          NaT              NaN                        NaT   \n",
            "...                          ...              ...                        ...   \n",
            "26170                        NaT              NaN                        NaT   \n",
            "26171                        NaT              NaN                        NaT   \n",
            "26172                        NaT              NaN                        NaT   \n",
            "26173                        NaT              NaN                        NaT   \n",
            "26174                        NaT              NaN                        NaT   \n",
            "\n",
            "       event_price        pnl  \n",
            "121         1390.0  27.720579  \n",
            "122            NaN        NaN  \n",
            "123            NaN        NaN  \n",
            "124            NaN        NaN  \n",
            "125            NaN        NaN  \n",
            "...            ...        ...  \n",
            "26170          NaN        NaN  \n",
            "26171          NaN        NaN  \n",
            "26172          NaN        NaN  \n",
            "26173          NaN        NaN  \n",
            "26174          NaN        NaN  \n",
            "\n",
            "[11431 rows x 27 columns]\n",
            "5000 [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.5734981321117372, -0.3198685973124989, -0.22577566094640591, 0.06643769849721462, 0.08453475418438451, 0.10552625544984867, 0.12070786990759397, 0.10422658135182672, 0.181955437558458, 0.2119908851344219, 0.21806422597016134, 0.22267443482084134, 0.24008461827780408, 0.24014371911437726, 0.24422750365663887, 0.2596975364840767, 0.26129303249189684, 0.2605171265175402, 0.26082397204286695, 0.26082397204286695, 0.2872823952910351, 0.2872823952910351, 0.29400345283354834, 0.278363842627427, 0.2797742104699645, 0.28931055513020426, 0.30685395876482985, 0.2822930462420291, 0.28223394540545604, 0.27955924954679096, 0.27955924954679096, 0.2245684067755566, 0.22880827274228982, 0.25045134141275666, 0.2270807265467411, 0.22775925854391033, 0.1690563301766072, 0.15955534869488003, 0.15955534869488003, 0.14020104967084285, 0.13482255834840662, 0.1274286241017905, 0.11783635140944979, 0.12323279715189442, 0.1232918979884676, 0.13086650846866732, 0.12330564213518666, 0.0941252177307696, 0.0027348249506821444, -0.009080536878818239, 0.008868059637443255, 0.02236312726671709, 0.024318410878638286, 0.01665134208999946, 0.05737947827342871, 0.11893393992038545, 0.13294207046752207, 0.15245598670498126, 0.19539223869980943, 0.1909043381742022, 0.20212014147147078, 0.19315882094847892, 0.19805863555666772, 0.26621653478206797, 0.2658696980530689, 0.2658696980530689, 0.2675278414121748, 0.26336055782757256, 0.28108952045614854, 0.2916451521990838, 0.2952070487899594, 0.2955655790721358, 0.2886485688317896, 0.20299675994130664, 0.184650729349452, 0.0808180372442438, 0.024393009936198064, 0.010525345830714947, -0.13924382134448413, -0.14146364334369477, -0.16928870241787788, -0.2011344834798946, -0.23361188957320908, -0.2614432640457367, -0.26989472935963377, -0.2694947996304806, -0.27358145705950215, -0.2773349436578542, -0.27358145705950215, -0.27119734347780333, -0.2681948360763564, -0.38794993392321464, -0.38081882426438796, -0.38429421686932635, -0.38081882426438796, -0.38081882426438796, -0.41194762075599717, -0.4405555758411304, -0.42478875028045787, -0.4407377516304918, -0.42497092606981923, -0.4437315923508154, -0.4437315923508154, -0.46595513689574936, -0.46595513689574936, -0.46296129617542575, -0.42420212547560804, -0.46518633630153816, -0.48244227061611855, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.34563646614035926, -0.34563646614035926, -0.3370734832086459, -0.3367176271910893, -0.3273764252580147, -0.34349826834544295, -0.2370103931866088, -0.17809325583507218, -0.02371879404979623, 0.07598694849208008, 0.03608994814040186, 0.02391857389505235, -0.002922472769353042, -0.059057593675164916, -0.1651097338263632, -0.2125644773189515, -0.29065439044381836, -0.2863411647601979, -0.29612669759624727, -0.3660766407121775, -0.3674068507979097, -0.33760842634422394, -0.5490592326884388, -0.5490592326884388, -0.6197697092221344, -0.5698601668427987, -0.6508865756657229, -0.6521644124423516, -0.2749445044299396, -0.2749445044299396, -0.2734718333769561, -0.2731356411817598, -0.2731356411817598, -0.27558017812250496, -0.2840297553952642, -0.2837330480082949, -0.2884190565962348, -0.2884190565962348, -0.38000316048615457, -0.6941326826903147, -0.7104839313749709, -0.6803370859769138, -0.6818318462663847, -0.718907625309396, -0.7018191056374105, -0.6852815961225575, -0.7219574601186982, -0.7036860990553429, -0.6945838593046267, -0.7317567708233909, -0.6949827733137043, -0.6958804509477142, -0.7317567708233909, -0.769117396310644, -0.7687288249217953, -0.7687288249217953, -0.7641782022946519, -0.7641782022946519, -0.7641782022946519, -0.7541353342132358, -0.7541353342132358, -0.7541353342132358, -0.7541353342132358, -0.7532443046804894, -0.7532443046804894, -0.7556888416212346, -0.7556888416212346, -0.7582880261192652, -0.7582880261192652, -0.7582880261192652, -0.7606824475302658, -0.7659010796449537, -0.7606824475302658, -0.7606824475302658, -0.7606824475302658, -0.7659010796449537, -0.7659010796449537, -0.7659010796449537, -0.7659010796449537, -0.7606824475302658, -0.8794181964708797, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.715272252302989, -0.32754918228505797, -0.3279237178211965, -0.2988119141922139, -0.02591068406687415, -0.02864859404107021, -0.014292199739571769, 0.26663024019188086, 0.2849776626776241, 0.32281709673045983, 0.33846470159593267, 0.33846470159593267, 0.35942660882805166, 0.08041976362399145, 0.07665838123889648, 0.07607725076128488, 0.06108230383819749, 0.07976025733121736, 0.06285833074536178, 0.05826371094521031, 0.05929598944480095, 0.03804912214590519, 0.03111874482729822, 0.031094598083624685, 0.031094598083624685, 0.040774143415521866, 0.037083267229041525, 0.037083267229041525, 0.031265579440343506, 0.024640291170244598, 0.023844825312136962, 0.022092582554433665, 0.03042907480201599, -0.00860279165357647, -0.037338803184227214, -0.03676129560577266, -0.07678518377823249, -0.07556163615172873, -0.07736631403323485, -0.07250860283449495, -0.0732467911471657, -0.07277522827717886, -0.06921232461507451, 0.0003341379208162132, 0.0064210499562388085, 0.019251225304073603, 0.023699223219094227, 0.026016133973540946, 0.02461824503535333, 0.02407264568266238, 0.023908507823670003, 0.04468378148209441, 0.04337588409233745, 0.04568632211070159, 0.08146985104052397, 0.08146985104052397, 0.0838113794703767, 0.08414769056763194, 0.08333343281158796, 0.08333343281158796, 0.08221332628986917, 0.05047533623186, 0.024169954572215997, -0.06301555409844614, -0.0622255017500843, -0.0622255017500843, -0.06741870014499107, -0.07382514548768092, -0.09825037048921607, -0.14209420181713284, -0.2564476858480682, -0.2575108182200882, -0.2575108182200882, -0.2568949229990879, -0.2635447846917591, -0.27405023604412887, -0.27602652049684473, -0.2768910130015045, -0.3088599651946931, -0.30813676072563984, -0.3083836202852975, -0.3608308943017334, -0.41112583427666866, -0.3850858297558283, -0.41456057441875493, -0.4141815157551379, -0.7765817173340624, -0.8419800531039369, -0.7347105325813454, -0.7400006006927998, -0.6987353748788716, -0.7404146414899583, -0.7355760378311338, -0.7408071065599349, -0.7359685029011104, -0.8818578598215937, -0.7656626512382222, -0.7671874140674306, -0.7725367449673844, -0.750068548036651, -0.7721442798974077, -0.4518703983001609, -0.42916869288419046, -0.45073294003543385, -0.45073294003543385, -0.4137207263496531, -0.4502960897194389, -0.4502960897194389, -0.40178669220530266, -0.45560499087901263, -0.4702811606679428, -0.47056359871285874, -0.47056359871285874, -0.415752747403075, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.33358132407733343, -0.30161581165116397, -0.10897132529001492, -0.010461534831497012, 0.0027903545877780676, 0.2839035077042193, 0.2836345362622178, 0.3168799370444921, 0.348130045911394, 0.348130045911394, 0.36381967897590606, 0.3622878144088127, 0.3579195560091132, 0.3579195560091132, 0.44665368921220877, 0.44665368921220877, 0.3605699427609101, 0.07469131917327221, 0.07463112580358552, 0.07463112580358552, 0.061126851601145685, 0.06271720625327111, 0.030665690104907162, 0.030605496735220457, 0.029328319555423178, 0.011647760835505665, 0.020645709332067767, 0.014719405045313882, -0.018270142683922887, -0.0605041181138598, -0.08026313892329193, -0.0808876127579243, -0.07027826536080672, -0.02849277466311879, -0.07149542304591988, -0.07048882459246619, -0.0794791281084214, -0.08358583262757749, -0.0839877188236267, -0.08321493207204303, -0.12927695304656475, -0.2504678584732931, -0.25018281977686724, -0.2564670658713721, -0.2752276177702321, -0.283259069010081, -0.3196660340044002, -0.33425630862179384, -0.3255353701696381, -0.3466276537292199, -0.31969593357890924, -0.30827400348954115, -0.31987109728815766, -0.3445056969094368, -0.3395000830180097, -0.3587402695331337, -0.3423249715662348, -0.36247644836931664, -0.3663564567224504, -0.3524146522966124, -0.3517691068945633, -0.3766062632852521, -0.3439200327818948, -0.3411975213966378, -0.34238679326750854, -0.36335033920972093, -0.34828818409679113, -0.34828818409679113, -0.3487227793870227, -0.3518086442521761, -0.4120326334010285, -0.4213381560611758, -0.37327612286835654, -0.41164649522116376, -0.3424357041452932, -0.4029285920777297, -0.4533378074759326, -0.4389914549460727, -0.4638782479350204, -0.43545923432771183, -0.4362261520464223, -0.42844401018828665, -0.41895918932614196, -0.4174735645972552, -0.4172618782829029, -0.418901662585157, -0.4372222140196754, -0.3868826411171532, -0.4182733540989549, -0.43755398408186863, -0.4345047563525572, -0.42726784206424245, -0.4529466885632742, -0.4402863162552091, -0.43443134206456524, -0.4442035242489378, -0.4319868051238202, -0.43475327455810653, -0.3998093284322512, -0.42694458968090904, -0.4391706548707681, -0.4391706548707681, -0.4460424574654899, -0.3973620777347425, -0.430260110329697, -0.43285929482772756, -0.4431465655804901, -0.44574575007852063, -0.4431465655804901, -0.3941591588106259, -0.4696659042044481, -0.4421746996927287, -0.7493294164542507, -0.7319429559567135, -0.7051873104995405, -0.7104593980086307, -0.7104593980086307, -0.7114175148258902, -0.737396799376479, -0.7079206526938495, -0.7114175148258902, -0.7164433585919631, -0.7196272710376437, -0.7164433585919631, -0.7170280865396133, -0.7196272710376437, -0.7161304089056032, -0.713685871964858, -0.7187295934036337, -0.7196272710376437, -0.7452569230760312, -0.714583549598868, -0.7833522417642492, -0.7426577385780007, -0.7664906380873151, -0.7135506692487268, -0.7163081558758319, -0.716892883823482, -0.7135506692487268, -0.7163081558758319, -0.7135506692487268, -0.7163081558758319, -0.4290388704924388, -0.4290388704924388, -0.4290388704924388, -0.4290388704924388, -0.7063069319876378, -0.7047640872227284, -0.7011491024969077, -0.7038382478073578, -0.703525298120998, -0.7049507444156564, -0.7049507444156564, -0.7541698444425047, -0.7108921434884421, -0.7194920683215126, -0.7194920683215126, -0.715995206189472, -0.34546248963971987, -0.4414064987750244, -0.3451713009526361, -0.4316601073029223, -0.34369862989965244, -0.3437451050293281, -0.395472779770059, -0.3437451050293281, -0.3920949915611724, -0.3434483976423588, -0.3436136088270355, -0.426809648041052, -0.410128013116101, -0.4397092926253948, -0.413374305122695, -0.4429555846319888, -0.4429555846319888, -0.4157044742743944, -0.4452857537836883, -0.7065714097207129, -0.6823839862556086, -0.6823839862556086, -0.6847601818796992, -0.7072036163879036, -0.6841445830349088, -0.6841445830349088, -0.6841445830349088, -0.7081617332051628, -0.7159041599395176, -0.7159041599395176, -0.7162171096258774, -0.7159041599395176, -0.7168018375735276, -0.7159041599395176, -0.7159041599395176, -0.7159041599395176, -0.7192463745142726, -0.7159041599395176, -0.7159041599395176, -0.7159041599395176, -0.7159041599395176, -0.7161953486266012, -0.7161953486266012, -0.7161953486266012, -0.7161953486266012, -0.3381810658695564, -0.3379320116560448, -0.3314454986805979, -0.34299213509176063, -0.6820878151325034, -0.7014245047360802, -0.6769770064092148, -0.6793532020333053, -0.7029030227261605, -0.7038611395434197, -0.6800262801541388, -0.6800180171898066, -0.5946367273004025, -0.3276076536661601, -0.32813498422547294, -0.32813498422547294, -0.3309632091854193, -0.27294807862763026, -0.272651371240661, -0.2732411607636293, -0.2732411607636293, -0.2615116459407002, -0.2629432659281288, -0.2644700322555335, -0.3120073699251557, -0.2810685444013805, -0.36135303727258394, -0.3373118038070456, -0.30922479441841744, -0.30922479441841744, -0.3114990853346953, -0.3297318676044776, -0.3297318676044776, -0.32776871655787143, -0.3280537552542973, -0.3280537552542973, -0.3280537552542973, -0.3295264263072809, -0.3295264263072809, -0.3295264263072809, -0.3303116694446422, -0.3303678234173636, -0.3839099464915234, -0.6888877541468198, -0.6908080683121902, -0.6908029010775109, -0.6915688989973073, -0.7064838608957746, -0.6925270158145667, -0.6951543945069127, -0.7027729958438549, -0.6685753089256624, -0.700980256880944, -0.700980256880944, -0.700980256880944, -0.701336839430144, -0.700980256880944, -0.700980256880944, -0.7037018766727994, -0.6992527508703158, -0.6992527508703158, -0.7492805855245134, -0.7485635545943969, -0.7519587875940376, -0.7524288601978373, -0.7524288601978373, -0.7548733971385825, -0.7642145990716571, -0.6985723968698875, -0.7455709666356898, -0.7663521022415319, -0.7409495973946743, -0.7639075653007869, -0.7635377105738221, -0.7468113096130191, -0.7468113096130191, -0.7001825945741814, -0.7254423918856043, -0.726101073795292, -0.7641963924835098, -0.726101073795292, -0.726101073795292, -0.7474699915227067, -0.7008412764838691, -0.7008412764838691, -0.7032858134246143, -0.6999435988498591, -0.6999435988498591, -0.6982160928392306, -0.6982160928392306, -0.6991137704732406, -0.6982160928392306, -0.6982160928392306, -0.6991137704732406, -0.6982160928392306, -0.6982160928392306, -0.6982160928392306, -0.6982160928392306, -0.6982160928392306, -0.39127577509143546, -0.39127577509143546, -0.4191316524333775, -0.4191316524333775, -0.4191316524333775, -0.4191316524333775, -0.42590042201463146, -0.4088911277159875, -0.41683581359001426, -0.46153778560828795, -0.42220863639663475, -0.42220863639663475, -0.3806757829231683, -0.43073805021831163, -0.42220863639663475, -0.4301719791740748, -0.4301719791740748, -0.34271233479971125, -0.33266217000474896, -0.40899011640197064, -0.40899011640197064, -0.37929130138757555, -0.4164647296721976, -0.41890926661294264, -0.43214644351102005, -0.4203467958931558, -0.4313207466555777, -0.4313207466555777, -0.4203467958931558, -0.4313207466555777, -0.39502835907010114, -0.4080510828376821, -0.4085127574340844, -0.41890198024885617, -0.37881394241968924, -0.4215011647468867, -0.37881394241968924, -0.42279133283390086, -0.37881394241968924, -0.4085127574340844, -0.41645744330811113, -0.37881394241968924, -0.4313207466555777, -0.4313207466555777, -0.4205534888741508, -0.42908290269582766, -0.4309151694751477, -0.6836817820516753, -0.7091689005836117, -0.7091689005836117, -0.696559156745478, -0.7128413753126465, -0.6931887946430679, -0.7328557451672658, -0.6959893543051685, -0.6958780949702399, -0.7616841289919125, -0.7616841289919125, -0.7449577280311095, -0.7458037019186371, -0.7458037019186371, -0.7458037019186371, -0.7458037019186371, -0.7265600925890128, -0.7629225679494167, -0.6999047421787324, -0.7265600925890128, -0.7239609080909822, -0.7239609080909822, -0.7432045174206066, -0.7239609080909822, -0.698177236168104, -0.698177236168104, -0.698177236168104, -0.6948350215933489, -0.6948350215933489, -0.6948350215933489, -0.6948350215933489, -0.7222334020803539, -0.6948350215933489, -0.7223765386508607, -0.7442193324785155, -0.7442193324785155, -0.6974067979811699, -0.7416201479804849, -0.6479466535393228, -0.6484670577878211, -0.6535107792265967, -0.6977241292259125, -0.6650677394772052, -0.663950867407337, -0.663950867407337, -0.663950867407337, -0.663950867407337, -0.663950867407337, -0.663950867407337, -0.663950867407337, -0.7296271134406984, -0.6693075385324725, -0.7230260471832123, -0.722673162824003, -0.8413474799788414, -0.7393146016090675, -0.8408529480393978, -0.8408529480393978, -0.7405475756802523, -0.8413588724264797, -0.8408529480393978, -0.8408529480393978, -0.7046625275655634, -0.7405475756802523, -0.666789355270546, -0.6662689510220476, -0.700509835659534, -0.666789355270546, -0.7360419994150136, -0.6645414450114192, -0.6650618492599175, -0.7343144934043851, -0.7343144934043851, -0.7093487397633798, -0.6901051304337555, -0.6901051304337555, -0.7346673777635945, -0.725173969606005, -0.7223760697521096, -0.7060875153262542, -0.7060875153262542, -0.7060875153262542, -0.7223760697521096, -0.8209176260945951, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.7111971039521354, -0.33392913819650705, -0.33268926605610777, -0.3164289885940272, -0.2706320048024731, -0.10820790952347098, -0.10349464875570374, -0.09945932333797919, -0.09799477769443556, -0.031212620490663305, -0.0013657550244259402, -0.06795297293404692, -0.07066707385852086, -0.0703413475792638, -0.06730492265024851, 0.00837843007828009, 0.014602551383517064, 0.006139731387269771, -0.0005601932669787243, -0.007243711614590283, 0.0006569644181344432, -0.027164899610336413, -0.06990945347023844, -0.07305557277597233, -0.02799723017961936, -0.02524754040764539, -0.02524754040764539, -0.0001502800307098054, 0.0005910460195211083, 0.014026324000007445, 0.006465457666526835, 0.00597309460276008, 0.0053600819839341625, -0.0007022601142449431, -6.988800544265386e-05, -0.0023606000646351696, -0.025633504628444333, -0.023195692514258865, 0.014130709612599768, 0.019035194503858578, 0.02477860653933594, 0.025469954987397653, 0.02506664798758479, 0.02639270680285359, 0.027788513015119774, 0.06666077121964659, 0.06755896800399444, 0.0680513310677612, 0.07371873357760589, 0.11573615463746623, 0.34566319936561674, 0.35207188396807415, 0.34590036637519544, 0.3478683152636637, 0.3898998674730208, 0.13955713587559646, 0.13686668223359189, 0.1217356284633789, 0.1217356284633789, 0.1196830600685588, 0.05323787090643724, -0.05712531606677347, -0.22436064692720203, -0.22429254517758088, -0.27428029212684235, -0.3609719601367106, -0.3144750842791826, -0.3185108376230182, -0.31129471529920877, -0.2870995842790565, -0.31916702065007746, -0.2719599128627226, -0.26538097104747693, -0.2610322245630492, -0.2612535897204473, -0.1456843288427154, -0.09869703989827543, -0.09869703989827543, -0.25910982282432615, -0.2596806745738207, -0.25092748300634, -0.2418586468946936, -0.25052982622792025, -0.25846084107438255, -0.27613539759925587, -0.2845952762781237, -0.2871618098099955, -0.28755627611989126, -0.31388876232300106, -0.3187914494670514, -0.39989766836788904, -0.37269341187777594, -0.3706686615643877, -0.3706686615643877, -0.38617277033312347, -0.4821301091768645, -0.7452331607782212, -0.725675281916982, -0.7430617424145192, -0.704905532086479, -0.7291774833511666, -0.7363445018247949, -0.7367094001756813, -0.6895928872382444, -0.6886952096042344, -0.697615606892477, -0.41605233591386853, -0.4159930002422677, -0.441355473007521, -0.4186027483676894, -0.44329905427506683, -0.3864593964125493, -0.4183060409807201, -0.42702342261364284, -0.4153914314361753, -0.4281814746759066, -0.4441733419650691, -0.44157415746703854, -0.45421690050105934, -0.6904480162279258, -0.6899806867505268, -0.7124812611544473, -0.7358662296910088, -0.7184797691934717, -0.7370490710823784, -0.7170634260868108, -0.7170634260868108, -0.7170634260868108, -0.697971990088907, -0.697971990088907, -0.697971990088907, -0.7108332465503877, -0.7082800436843167, -0.7091777213183268, -0.7082800436843167, -0.7082800436843167, -0.7091777213183268, -0.433136259768473, -0.7034125020473183, -0.3860937822147396, -0.4189410952942289, -0.4517730885248745, -0.43261534515035505, -0.698413727909209, -0.6965579334579395, -0.7007106253639688, -0.33212684196856285, -0.7030868209880593, -0.7040449378053187, -0.7016687421812282, -0.6758750957290348, -0.37882947131673017, -0.32572780237076093, -0.41319472572116966, -0.32708277804880365, -0.3668498474202678, -0.400811505120902, -0.3173379794119237, -0.2754743006146968, -0.2540256781151876, -0.2538139918008353, -0.25356713224117766, -0.2531637600462677, -0.2551810970591395, -0.2627936492021561, -0.26481932020900045, -0.2633024201660807, -0.2827901766211609, -0.2830579290332034, -0.33653214495122336, -0.30895158198465167, -0.33653214495122336, -0.37137613329341446, -0.2891632802039831, -0.3702496198483676, -0.38505173002774296, -0.38455216005909615, -0.2878949096411251, -0.35151506970612506, -0.35202346340744667, -0.2746388814121241, -0.36719644345991925, -0.35162553268633273, -0.3513288252993635, -0.2743421740251548, -0.2754560131739338, -0.35209762589357474, -0.38100582193846494, -0.28679310846123884, -0.3780933216316, -0.3243954473372101, -0.20914494189060598, -0.22629203462554687, -0.19619503158303034, -0.1717598385003047, -0.17279858047636332, -0.17662596093541083, -0.08702329071505249, -0.08654596811654876, -0.08483461120808032, -0.17439437725635373, -0.17497857047233664, -0.2144924933629773, -0.21901953208613853, -0.329256060006171, -0.3479333532522061, -0.3305919279962684, -0.3593766217371107, -0.3836808061492841, -0.40411044042531197, -0.40411044042531197, -0.40601355412188705, -0.4410277593793338, -0.6911555458366084, -0.4052447535276758, -0.27720445254796927, -0.34930079510698847, -0.3662932909874384, -0.27607682004955353, -0.2767314874377065, -0.2762885063639058, -0.2756467604223687, -0.2759917989769365, -0.2759917989769365, -0.2759917989769365, -0.2765489132567954, -0.2866698668070649, -0.2866698668070649, -0.36631608286951084, -0.36631608286951084, -0.40511052038251966, -0.3391952809918672, -0.36631608286951084, -0.2874430274702997, -0.35235423530010956, -0.3530089026882626, -0.27578011266258423, -0.3211482311367076, -0.35224752500964285, -0.36514539607925034, -0.27660870165714846, -0.40715317112721555, -0.392918530886347, -0.392918530886347, -0.42665432303141976, -0.3923338029386968, -0.3915650023444856, -0.3624415531131383, -0.3624415531131383, -0.39159362760854066, -0.4038745232351137, -0.3812357244433414, -0.3717764919453288, -0.4038745232351137, -0.36039271226384506, -0.36039271226384506, -0.3692672168142076, -0.3624065066719443, -0.3834756422587364, -0.3851290989161375, -0.3863627742447988, -0.3756422600530525, -0.3756422600530525, -0.37525368866420383, -0.37525368866420383, -0.7175712759629258, -0.6805214296174822, -0.6657561903316349, -0.7048262996327638, -0.6628363579336125, -0.6641074244372315, -0.7034099565261029, -0.6637944747508716, -0.6637944747508716, -0.6722598411944452, -0.671605173806292, -0.34501590578095703, -0.3791573128005417, -0.2863241826521016, -0.28541952698189854, -0.28460152331706806, -0.3710137494476231, -0.3972187319215301, -0.4167464316586233, -0.4167464316586233, -0.3944774875938158, -0.6915312420710789, -0.6731716659027563, -0.6734846155891162, -0.677992398467592, -0.7351997800472437, -0.6789505152848513, -0.6765059783441061, -0.6778709854555155, -0.6794178447622504, -0.6860187752899358, -0.6860187752899358, -0.6860187752899358, -0.6860187752899358, -0.6860187752899358, -0.6860187752899358, -0.33851377931007315, -0.6803155223962606, -0.6803155223962606, -0.6977578049012243, -0.6829147068942911, -0.6784597279449912, -0.8501702629417793, -0.6829720093689186, -0.6855711938669491, -0.6829720093689186, -0.6920174768373492, -0.6915356601383115, -0.6881934455635562, -0.6881934455635562, -0.6906379825043014, -0.6881934455635562, -0.692671095958095, -0.6876273745193192, -0.6876273745193192, -0.6876273745193192, -0.26922199448515866, -0.26827844251515925, -0.2680284542331093, -0.34055290035439706, -0.2669443545689744, -0.3553708775261444, -0.34094147174324574, -0.3258729876894346, -0.3397012123862771, -0.3104347115984564, -0.26643596086765287, -0.26643596086765287, -0.26643596086765287, -0.26703448816701414, -0.26703448816701414, -0.2659903615069572, -0.33980926416513363, -0.2659903615069572, -0.2659903615069572, -0.2665881812536288, -0.3100315560024752, -0.32827449035702133, -0.2774822954671331, -0.26735698184783996, -0.36804586964425107, -0.2776532763146727, -0.36804586964425107, -0.2724648011587886, -0.27142678751262905, -0.2704192394278312, -0.27118804002204244, -0.23171093445765778, -0.23269142136622128, -0.2350763603201842, -0.2350763603201842, -0.23473772930925974, -0.23436657368589434, -0.2786434418057892, -0.24379530329327997, -0.23250026470032945, -0.2293327183309909, -0.20141265860413385, -0.19411278410120114, -0.1808518870558051, -0.17951721188421943, -0.1789240688693482, -0.11738480474648556, -0.0933006578123067, -0.09467821979141414, -0.16794442325915288, -0.1778798527087669, -0.1657537265127356, -0.16514378403659125, -0.17456948425863997, -0.24009129758564696, -0.2442440967258576, -0.2336318414656453, -0.2304479364199088, -0.33274890327179796, -0.3371469212228085, -0.33905003491938357, -0.3382607930154989, -0.3382607930154989, -0.39257548865100755, -0.39257548865100755, -0.39742140422052086, -0.3542559177654299, -0.3967483042604814, -0.3696158326493329, -0.3523823755496338, -0.3208172905164128, -0.3965911079981418, -0.39399192350011125, -0.664527562947672, -0.6090477830017839, -0.6594838415088964, -0.6116314331880127, -0.37838464406759126, -0.6702618493118672, -0.6698599631158179, -0.666633087747479, -0.6829887282972973, -0.6707125323269567, -0.6720982091289327, -0.6720982091289327, -0.6720982091289327, -0.671709637740084, -0.6816504700689255, -0.6665131192868202, -0.7020452830423, -0.7027026876363096, -0.697658966197534, -0.7027026876363096, -0.6628438333721708, -0.677981184154276, -0.7009751816256812, -0.7009751816256812, -0.7009751816256812, -0.6805803686523065, -0.7009751816256812, -0.7009751816256812, -0.7009751816256812, -0.7016701796958578, -0.6812753667224831, -0.6270090056466302, -0.6240440644573868, -0.6582963946477067, -0.6734337454298119, -0.6240440644573868, -0.6784774668685877, -0.6378029985528778, -0.6922364009640785, -0.6770990501819734, -0.6378029985528778, -0.6364533330945481, -0.6378029985528778, -0.6378029985528778, -0.6371080004827011, -0.6371080004827011, -0.31062782387908955, -0.33047998540660484, -0.2579713607821767, -0.25678354067253384, -0.2569765334642846, -0.2563218660761316, -0.3368796482498183, -0.30048813451416817, -0.25503869494271814, -0.25503869494271814, -0.253135581246143, -0.25247817665213346, -0.2513940769879986, -0.2513940769879986, -0.2513940769879986, -0.2513940769879986, -0.2518936469566454, -0.25055281290961917, -0.25055281290961917, -0.25055281290961917, -0.25055281290961917, -0.25055281290961917, -0.2514220654119543, -0.25055281290961917, -0.252064305875962, -0.25205880163739935, -0.25205880163739935, -0.26170595378024053, -0.2624747543744518, -0.2624747543744518, -0.2635188810345087, -0.26386461325783644, -0.26416050228013455, -0.31881608049948384, -0.26339170168592335, -0.3408315440212466, -0.3508252186803825, -0.3664656127461529, -0.3508252186803825, -0.3412334302172958, -0.34383261471532633, -0.33878889327655076, -0.2521179708193706, -0.2521179708193706, -0.39861487958999625, -0.3988502839081765, -0.36611771245650954, -0.3782865585664082, -0.3782865585664082, -0.6156135505963876, -0.6145483562983703, -0.6145483562983703, -0.5905047872109631, -0.6353794009005068, -0.5912771680499799, -0.6380211335686832, -0.5918044735155606, -0.6389362635431192, -0.6586150833628162, -0.6705171737541343, -0.6586150833628162, -0.6586150833628162, -0.6586150833628162, -0.6586150833628162, -0.6750523871556232, -0.6586150833628162, -0.659807925400196, -0.659807925400196, -0.6750523871556232, -0.659807925400196, -0.659807925400196, -0.6705171737541343, -0.659807925400196, -0.659807925400196, -0.657903738972342, -0.6416151845464867, -0.6416151845464867, -0.6605029234703725, -0.5856747080857968, -0.5852822430158202, -0.5865196781944352, -0.5835547370051918, -0.6583829523897674, -0.5891188626924657, -0.6420943979639122, -0.6228507886342874, -0.5891188626924657, -0.5885984584439674, -0.5891188626924657, -0.6420943979639122, -0.6328650389180497, -0.6228507886342874, -0.6583829523897674, -0.6420943979639122, -0.6328650389180497, -0.652108648247674, -0.6328650389180497, -0.7126650327844886, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.575950348425428, -0.575950348425428, -0.575950348425428, -0.3386931640631308, -0.2370727756643067, -0.010296790311308616, 0.2649455653858513, 0.2549555156883022, 0.25621033989253084, 0.2673354936794605, 0.34619066962114964, 0.35635546167993365, 0.35809962741374374, 0.35954117086877524, 0.36468611721637445, 0.44901832275785497, 0.4493260421581766, 0.4517355142011604, 0.4521440522490731, 0.11246457556259201, 0.09737780114866387, 0.04945658500554018, -0.05686296656868629, -0.1772496916702844, -0.20730251177285816, -0.14667282163956163, -0.07222067464644559, -0.07862534019246423, -0.17050796469077953, -0.09587544192286294, -0.18828864426495284, -0.16125514779906797, -0.15580951578340585, -0.1859427364968788, -0.16156286719938961, -0.18036958845269524, -0.21898220141534805, -0.25644288132675563, -0.36741963215478607, -0.37263087780358317, -0.36760180794414743, -0.3672623996584758, -0.7688236543209891, -0.7273286979800324, -0.7265598973858212, -0.7326187660914867, -0.7249071998395223, -0.7238396325434704, -0.7213950956027252, -0.7230052933070191, -0.7284843401144055, -0.7581411600647437, -0.7581411600647437, -0.757752588675895, -0.757752588675895, -0.757752588675895, -0.757752588675895, -0.752530337582147, -0.4118083710093292, -0.41685209244810484, -0.41685209244810484, -0.7291873058740131, -0.7291873058740131, -0.7280044644826436, -0.8408627811071008, -0.7332945325940978, -0.7344773739854675, -0.7279172390705846, -0.7279172390705846, -0.8446836897204704, -0.8791301267204767, -0.7659010796449537, -0.7659010796449537, -0.7575740590209228, -0.7549748745228922, -0.8766693497945485, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.27829301416697977, -0.2766551319293195, -0.2766551319293195, -0.2766551319293195, -0.2554131685247414, -0.25216522522540374, -0.19985517650988804, -0.17447945298112538, -0.08539022607517212, -0.034260575449950925, -0.012415729943627763, -0.012521000820706027, -0.011783011906471089, 0.01628610482822873, -0.00896320753072481, -0.06154516125980881, -0.06870154583354766, -0.06692300972413574, -0.070541966188134, -0.0629970400899511, -0.06611465759348564, -0.06006202941894306, -0.06269019914115541, -0.009178440830133161, -0.061763169824898295, -0.06327307493171434, -0.07432837483240275, -0.07365192827825859, -0.16511017215720294, -0.16559792021236078, -0.16935400157983904, -0.1658192853697589, -0.16242704565564192, -0.21272154431706836, -0.2341742554967059, -0.23649419298139002, -0.23654722769400482, -0.2379415222370305, -0.24308901565301969, -0.24322335121083108, -0.342387755387341, -0.36421333518261323, -0.36460050121011295, -0.31906114489973314, -0.36460050121011295, -0.39528334935734055, -0.3787297799149899, -0.38033997761928384, -0.35014286745666207, -0.2798191032224659, -0.2808425278923942, -0.2790503026282547, -0.28105421420674653, -0.25656056964205975, -0.2547683443779202, -0.25033002871631743, -0.24862821033379232, -0.2660509088671436, -0.2601594677891872, -0.2577403709239493, -0.2572794817692161, -0.22883879149149414, -0.25526459785636724, -0.25367806501200496, -0.2506552453645591, -0.24921709059200245, -0.24664713861841142, -0.24759571559814753, -0.21722138102383917, -0.24969565990769776, -0.24825792312234546, -0.2610142534500327, -0.3114239835627097, -0.35621866263721824, -0.359489523536277, -0.3384793066601669, -0.33888274161984333, -0.31561725485441855, -0.3883492666774232, -0.4293107632283843, -0.7383575033149038, -0.694010063431229, -0.694010063431229, -0.7357313438965707, -0.7094800245624469, -0.6770790025511834, -0.6764942746035332, -0.36685453961384556, -0.35768266843806074, -0.3675138370075361, -0.3562099973850771, -0.3562099973850771, -0.3562099973850771, -0.3653818685608619, -0.3254757350348031, -0.3251790276478338, -0.35668020771681824, -0.34380985142710757, -0.3709306533047512, -0.3808713250747472, -0.39882341459454, -0.3807653595070031, -0.41693993033282706, -0.7029401963813128, -0.3715511337102081, -0.4003826080663122, -0.3674680712224375, -0.6758443495185307, -0.6734602517610349, -0.6907642363616636, -0.7069730481345698, -0.7087005541451983, -0.37005338112557123, -0.3674035403094333, -0.3668050130100721, -0.36740671954410586, -0.4023044576020679, -0.366912633335092, -0.41693993033282706, -0.3664586653027414, -0.42236614916299914, -0.36527240647438153, -0.6934062439590468, -0.6908070594610163, -0.6588221387167441, -0.6611983343408345, -0.7152787085154101, -0.6612587735240838, -0.6612587735240838, -0.28432799233916056, -0.2893717137779362, -0.6862123481355495, -0.6827780236816539, -0.6651888753236654, -0.6666069541304966, -0.6978039940631903, -0.7218701696964419, -0.7218701696964419, -0.8399904167434651, -0.7284095415099259, -0.7612783151351246, -0.7526071592616064, -0.7526071592616064, -0.7526071592616064, -0.7612783151351246, -0.7526071592616064, -0.7526071592616064, -0.7526071592616064, -0.7612783151351246, -0.8752655045648504, -0.7612783151351246, -0.7612783151351246, -0.8749774348144473, -0.8749774348144473, -0.7612783151351246, -0.8725166578885191, -0.7612783151351246, -0.8725166578885191, -0.7612783151351246, -0.8725166578885191, -0.7612783151351246, -0.8752655045648504, -0.8749774348144473, -0.8749774348144473, -0.8752655045648504, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.6751338808700734, -0.28301390388944914, -0.23739424491986422, -0.08644982252393925, 0.0003586183332165184, 0.0011061993597322184, -0.016636859202266607, 0.00893115011222529, 0.315464404859749, 0.3409775097254355, 0.3386060937229805, 0.3380521819407278, 0.3341411261659507, 0.07954184908386323, 0.07954184908386323, 0.08103252180451079, 0.08402735128880547, 0.08251172346419729, 0.2905733559874196, 0.3346043162323764, 0.3375719322151195, 0.339364157479259, 0.339364157479259, 0.36222246420015675, 0.359074275035672, 0.08238515294261027, 0.08055034497247558, 0.065268899569384, 0.06705460754254655, 0.06739874279203367, 0.036881379389872376, 0.04386320225079057, 0.04386320225079057, 0.047419244209308126, 0.047419244209308126, 0.048521034747312994, 0.048232993299064136, 0.06172519814082823, 0.06272773876943537, 0.04771523483767179, 0.06745612216581191, 0.06022400179707235, 0.06319357525898056, 0.06025264713494233, 0.04766126555901527, 0.06183036768038952, 0.06183036768038952, 0.05762444340956695, 0.060867509382463256, 0.06126939557851248, 0.06126939557851248, 0.06294746148807427, 0.06274219842020955, 0.05994743252746289, 0.058466871963166786, 0.056495409065015836, 0.05711303555722891, 0.05798874580250918, 0.05745590842888476, 0.034758128479793024, 0.026342967024376357, 0.007667447678200524, -0.06357672536860884, -0.0648841686207809, -0.0648841686207809, -0.06978847005673665, -0.06930072200157883, -0.15801635994653762, -0.1665210776522646, -0.17568732157352565, -0.1757911271934285, -0.17596854134441228, -0.19751848274370934, -0.19643695509840656, -0.22683385040558085, -0.23998173286077387, -0.2202723250299431, -0.2143100509933353, -0.19806163102487281, -0.21963598214279892, -0.20109075522373887, -0.19390511411773037, -0.17870842617968102, -0.17562709535432428, -0.18368739195686082, -0.1811497680919184, -0.17854725967954485, -0.1764824928982266, -0.1894739107187256, -0.1886010086177276, -0.20666669010932723, -0.25988474307490933, -0.2680115466512121, -0.2733206379191018, -0.3898285682764183, -0.3651849755785732, -0.39127737108052696, -0.3908887996916783, -0.3908887996916783, -0.44611253639556936, -0.44611253639556936, -0.44611253639556936, -0.44611253639556936, -0.4313249374723522, -0.40245406485645285, -0.42712766908218575, -0.4305561368781409, -0.44534373580135816, -0.44534373580135816, -0.4462910660505417, -0.8319811814874591, -0.7251041260348444, -0.7239428603706566, -0.7311378756130564, -0.7235286017818621, -0.7247114431732319, -0.732320717004426, -0.7243189781032553, -0.724140448448283, -0.4156468125287989, -0.39167707876183855, -0.41450935426407187, -0.38849581549297013, -0.41393822031291416, -0.3765710698449455, -0.3535983943964212, -0.4516100144928552, -0.8364163114384043, -0.8364163114384043, -0.8364163114384043, -0.8417063795498588, -0.6663901595665276, -0.7264146585263327, -0.7328409557688815, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.3465315503729815, -0.19929797377361544, -0.10704848027469373, -0.11989495197569515, -0.06873774644593819, -0.07519004141116185, -0.008142888799390959, 0.009658408855806065, 0.012728946968780595, 0.027668527333238685, 0.027668527333238685, 0.027668527333238685, 0.028888921540862207, 0.033411889996535396, 0.050294685304960784, 0.05060763499132066, 0.05000664385671193, 0.05000664385671193, 0.04715261056100378, 0.06009231012790002, 0.05844705963316521, 0.05844705963316521, 0.04788177092204243, 0.04158199945179111, 0.03148951613354206, 0.027033453609814385, 0.044182982289126815, 0.040983728642738915, 0.026221813075785957, 0.02751803310739749, 0.025305502670088437, 0.016408999923937198, 0.016408999923937198, -0.0034387493121074524, -0.006220438975277808, -0.006220438975277808, -0.020071160740276123, 0.0009248897253167077, -0.0013089550750460498, -0.0022003864809021535, -0.0007931158201286113, -0.0023055978810973057, -0.0022003864809021535, -0.0015381245780812375, -0.061094869626785904, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.017821885736009263, 0.03946512431081589, 0.02630774938992858, 0.02630774938992858, 0.026742376799341265, 0.04193000913700916, 0.06353076390663089, 0.06325834033483932, 0.3375447493742845, 0.3407847889288192, 0.3407847889288192, 0.3378171729460761, 0.058881564453069954, 0.0804932610741819, 0.06654161403893333, 0.06676297919633147, 0.06676297919633147, 0.043056863192026215, 0.042901123745286276, 0.019274392003606963, 0.02035091532169306, 0.013197513861820477, 0.013800522325283768, 0.021123376535705042, 0.020971059800197096, 0.010657985495879624, 0.025753607784995816, 0.025753607784995816, 0.029103024384251656, 0.029103024384251656, 0.029714729580473324, 0.04973169402396947, 0.05016632143338216, 0.05016632143338216, 0.04872915339536234, 0.046694365350318096, 0.059529318846908694, 0.057884068352173954, 0.044218716147277265, 0.046502724451682395, 0.03194580593630019, 0.02297027834208814, 0.024602903547651128, 0.018147163429197387, 0.030546411850075124, 0.015011010309287212, 0.009464352599172082, -0.061087830488204264, -0.060349642175533534, -0.06631155434660246, -0.15386219083261313, -0.15386219083261313, -0.15280905360715938, -0.15954197150723168, -0.20772789831507735, -0.24211551284093244, -0.23121824745345623, -0.24201548417341243, -0.24467686070054356, -0.25040128287265295, -0.25040128287265295, -0.2622430502169565, -0.2561418278163512, -0.25822501480492577, -0.2749053741199224, -0.2393072447351158, -0.37772897125713245, -0.3733232940969361, -0.37746121884509, -0.3801074288502857, -0.36802561883263885, -0.39173118371798393, -0.37818158641548344, -0.2560850532076106, -0.28951772461525405, -0.2560850532076106, -0.45938661211974363, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.1738912296252283, -0.0051767195721211245, 0.005437895351345076, 0.018524337720537874, 0.2456031064434812, 0.2465432831411869, 0.2738215460755069, 0.2969781950379845, 0.2969781950379845, 0.31876219650080456, 0.31811153538426234, 0.31811153538426234, 0.3172045675754779, 0.4060914611032827, 0.40193782332648886, 0.40368198906029895, 0.40368198906029895, 0.40368198906029895, 0.40368198906029895, 0.4033742696599773, 0.4006438206868129, 0.39460184320771435, 0.39634600894152444, 0.3958837559748526, 0.3958837559748526, 0.394301643942982, 0.394301643942982, 0.394301643942982, 0.3975663285408654, 0.3993054483970615, 0.4001815677201411, 0.3150755021470181, 0.31686581433603583, 0.0948570591684359, 0.07843035884429142, 0.07626114821838782, 0.06338361404706516, 0.06338361404706516, 0.034045212501465874, 0.013421826069720914, 0.013638028753075078, 0.01188535715115547, 0.028510757236741735, 0.026612072348252358, 0.012282770924100683, 0.012282770924100683, 0.01945696555146091, 0.009206464309367599, -0.008668269755045154, -0.07078454475636341, -0.07530852400246199, -0.15744469835267916, -0.1656715995491274, -0.16650924885084223, -0.201966949797657, -0.22909206050373235, -0.24517993285143191, -0.24528789212600174, -0.24528789212600174, -0.24987983431808117, -0.23578262321881746, -0.25375698582147066, -0.3517039418022366, -0.355179334407175, -0.37692499520473605, -0.3835411423835182, -0.6676026583204491, -0.7230663036066103, -0.7230663036066103, -0.733744633849389, -0.8430824662279323, -0.8436195787994818, -0.8436195787994818, -0.8436195787994818, -0.8430584184016395, -0.8783278745175377, -0.8783278745175377, -0.8783278745175377, -0.8783278745175377, -0.8783278745175377, -0.8783278745175377, -0.8786159442679408, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.671024721646969, -0.3853691260158739, -0.3619174817878918, -0.13463269753632418, -0.04793211025720713, -0.03306196597168237, -0.029278362005096308, 0.23262449274348307, 0.26764150875024184, 0.2750824427604346, 0.29015382024239744, 0.2892468524336131, 0.3099825879592816, 0.31066523147631675, 0.3099825879592816, 0.31066523147631675, 0.315292460183815, 0.39861642519115337, 0.39861642519115337, 0.31168201805265844, 0.09210326950472059, 0.07661768278371238, 0.06343420771054252, 0.06343420771054252, 0.06348766514653689, 0.01930651984450616, 0.026282557991326227, 0.018721691657845596, 0.033548404316540346, 0.02900841298897288, 0.016363130608932346, 0.013137122567982301, 0.02697936258611651, 0.0248598707473021, 0.02490677425566406, 0.024065812453423048, 0.01931945659212503, 0.02337648096911739, 0.021552727099915535, 0.02341721445326251, 0.024388286600729205, 0.023895923536962453, 0.023342011754709673, 0.022404923116502953, 0.0234350961005428, 0.023632123672118963, 0.0008086326873907798, -0.00888692647403545, -0.033925595248202184, -0.02672681715470649, -0.0691833598124034, -0.0691833598124034, -0.07005056287139996, -0.08186231046828793, -0.08186231046828793, -0.24249492486979046, -0.24305608526763264, -0.24305608526763264, -0.2346638908602123, -0.2346638908602123, -0.3398200347059134, -0.424397973894954, -0.4257350851771, -0.7841828881201135, -0.7841828881201135, -0.7876556129615542, -0.8757194230591663, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, -0.7415374064418888, -0.38545739824261765, -0.3634076274066138, 0.008458902236656247, -0.031800363788326895, -0.019090140820248427, 0.19872540477489434, 0.23587987198786958, 0.2721506106418011, 0.29869657661534943, 0.294840401969669, 0.3100390462963622, 0.3066742306379105, 0.3066742306379105, 0.30895653362010755, 0.3900917262171104, 0.38906328229983705, 0.3900917262171104, 0.3900917262171104, 0.3900917262171104, 0.3922333817772481, 0.39290426853641175, 0.3967899067964273, 0.3974520287561928, 0.3974520287561928, 0.3138863268975949, 0.0768214877743573, 0.062063147386832274, 0.033902973758782345, -0.04186577915701595, -0.04005250529668121, 0.023848578920754475, 0.022411496757673052, 0.004800062523464307, 0.004109317511181702, 0.0120421599177917, 0.018416547350806715, 0.019206674596045983, 0.039949755612694295, 0.05668899278737116, 0.06836811738956235, 0.06846736729896011, 0.06846736729896011, 0.058241128268053347, 0.08376454497247729, 0.07758098002996719, 0.06008875366213272, 0.06413140428379598, 0.06230445842096376, 0.07862951183287259, 0.04850813485769764, 0.019516457338588387, -0.07771902953412663, -0.2521696891629232, -0.2524486434352169, -0.2695634107484538, -0.3137831398842777, -0.3118951078251617, -0.43875747021961115, -0.43875747021961115, -0.4393655685105424, -0.44585117374041466, -0.3391076611956206, -0.38886152603987195, -0.31808741522628503, -0.32434200459436596, -0.3173933888714629, -0.2622576996429216, -0.2674135621119045, -0.2607364822255147, -0.2601765298108766, -0.2583735020982968, -0.2553717679846488, -0.253644734469717, -0.2760899160072723, -0.29284426510299266, -0.3163997433730489, -0.31871968085773306, -0.33261909020162644, -0.35178898108662154, -0.44080263323571195, -0.4468829316950055, -0.44105087602384435, -0.4516018947545831, -0.7054884702450577, -0.7701625649498223, -0.7689414835808843, -0.7647453052990815, -0.7653549623724107, -0.7721435314129604, -0.7645855731790414, -0.7730522247844593, -0.7803816978014506, -0.8106781857023123, -0.7969192516068216, -0.7969192516068216, -0.7902498381265641, -0.7902498381265641, -0.7869782812919704, -0.7969192516068216, -0.7869782812919704, -0.7604509516195728, -0.477824511172342, -0.477824511172342, -0.47552733154099114, -0.4888814567951014, -0.47145534267895467, -0.4424532217188227, -0.41622386483952206, -0.35281494681887876, -0.34906827317344546, -0.3259091276439244, -0.3105356336411302, -0.3115190436149329, -0.27620287672363897, -0.2485333376688326, -0.24635377778123146, -0.2470064137800419, -0.24549271507165438, -0.25555007540876473, -0.25641221276281767, -0.2604461936219902, -0.2334437706548554, -0.09781566171711864, -0.08658590706446812, -0.09497331623268644, -0.09101824394599645, -0.08927649599986894, -0.06680087252750774, -0.0633598039077933, -0.06677470894078272, -0.0633336403210683, -0.010125167851384002, -0.013566236471098404, -0.018906292225140727, -0.034284869846415406, -0.26486993557779737, -0.32512773402680456, -0.392221925613873, -0.3843542689702074, -0.3893476189674567, -0.4281857972224212, -0.4314323448461834, -0.4606105352903046, -0.4606105352903046, -0.44878090664685316, -0.44878090664685316, -0.44907761403382246, -0.44907761403382246, -0.44907761403382246, -0.45913786423732095, -0.45913786423732095, -0.45733827799194937, -0.45733827799194937, -0.45733827799194937, -0.458810949044933, -0.458810949044933, -0.458810949044933, -0.4588318715350575, -0.458810949044933, -0.4588318715350575, -0.4333937959877873, -0.43305150350516536, -0.40636334378477706, -0.4057735542618088, -0.7634151906108492, -0.738385727076005, -0.7384659584350046, -0.7401721331771273, -0.7408421540590951, -0.7413983846803052, -0.7413983846803052, -0.7665835241886239, -0.7425663740183696, -0.7749713219736512, -0.7765815196779452, -0.776927251901273, -0.776927251901273, -0.776927251901273, -0.8067261384082645, -0.776927251901273, -0.7778249295352829, -0.8146921435316096, -0.7782238435443606, -0.8101508802367314, -0.8101508802367314, -0.803481466756474, -0.8101508802367314, -0.8101508802367314, -0.7736825802494824, -0.7736825802494824, -0.7736825802494824, -0.7736825802494824, -0.7736825802494824, -0.7736825802494824, -0.7736825802494824, -0.7745802578834925, -0.8002099099218802, -0.7745802578834925, -0.7902498381265641, -0.7604509516195728, -0.7604509516195728, -0.7604509516195728, -0.7604509516195728, -0.7604509516195728, -0.7587234456089443, -0.7587234456089443, -0.7721400872218132, -0.772482379704435, -0.772482379704435, -0.772482379704435, -0.772482379704435, -0.7719550742388541, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7717044161610883, -0.7744130543906874, -0.6667657682699564, -0.7713405972838413, -0.7713405972838413, -0.7738705504807979, -0.7738705504807979, -0.7738705504807979, -0.7751288593566485, -0.7751288593566485, -0.7755174307454973, -0.7755174307454973, -0.7755174307454973, -0.7755174307454973, -0.7755174307454973, -0.7751288593566485, -0.7751288593566485, -0.7751288593566485, -0.7751288593566485, -0.7751288593566485, -0.7751288593566485, -0.7751288593566485, -0.7755174307454973, -0.7755174307454973, -0.7755174307454973, -0.7713647388394679, -0.7713647388394679, -0.7713647388394679, -0.7705390419840256, -0.7713647388394679, -0.7701933097606978, -0.7701933097606978, -0.7705390419840256, -0.7705390419840256, -0.7705390419840256, -0.7708519916703854, -0.7709034718172477, -0.7689350008848472, -0.7689350008848472, -0.3834662173778708, -0.3834662173778708, -0.3830045427814686, -0.3830045427814686, -0.39704099478586113, -0.39389865699497284, -0.4471804641020166, -0.7556026929541759, -0.7568610018300267, -0.7563936723526276, -0.7587698679767179, -0.7587698679767179, -0.7741008342564559, -0.7600877496373466, -0.7626181445831266, -0.7538084118297698, -0.7621666870386263, -0.7625089795212483, -0.7641191772255422, -0.76446490944887, -0.76446490944887, -0.76446490944887, -0.76446490944887, -0.7623951109556197, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.762348832049393, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.7627374034382417, -0.9143849649560718, -0.9172155966089648, -0.9172155966089648, -0.9143849649560718, -0.7940326753056288, -0.8068887770391499, -0.9143849649560718, -0.9143849649560718, -0.9143849649560718, -0.9143849649560718, -0.9143849649560718, -0.9102322730500424, -0.8027360851331204, -0.9102322730500424, -0.9102322730500424, -0.8027360851331204, -0.7937792873499835, -0.7937792873499835, -0.7999659756032471, -0.7937792873499835, -0.7937792873499835, -0.7932132163057465, -0.7993999045590101, -0.7927753697707988, -0.7932132163057465, -0.7932132163057465, -0.7993999045590101, -0.910580864923465, -0.910580864923465, -0.910580864923465, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import contextlib\n",
        "import functools\n",
        "import statsmodels.api as sm\n",
        "import inspect\n",
        "import zipfile\n",
        "import pickle\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "import io\n",
        "import time\n",
        "import typing\n",
        "import logging\n",
        "import pytz\n",
        "import gzip\n",
        "import dill\n",
        "import lzma\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import hashlib\n",
        "import random\n",
        "import joblib\n",
        "import math\n",
        "import psutil\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import scipy.sparse.linalg as spla\n",
        "\n",
        "\n",
        "from numba import jit, njit\n",
        "from grpc import StatusCode\n",
        "from grpc.aio import AioRpcError\n",
        "from dataclasses import dataclass\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from collections.abc import Sequence\n",
        "from tinkoff.invest import AsyncClient, CandleInterval\n",
        "from tinkoff.invest.exceptions import RequestError\n",
        "from tinkoff.invest import (\n",
        "    AsyncClient,\n",
        "    #CandleInstrument,\n",
        "    MarketDataRequest,\n",
        "    #SubscribeCandlesRequest,\n",
        "    #SubscriptionAction,\n",
        "    SubscriptionInterval,\n",
        ")\n",
        "from tinkoff.invest.schemas import (\n",
        "    SubscribeCandlesRequest,\n",
        "    SubscriptionAction,\n",
        "    CandleInstrument,\n",
        ")\n",
        "from tinkoff.invest.market_data_stream.market_data_stream_manager import MarketDataRequest\n",
        "from tinkoff.invest.schemas import Candle\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from scipy.stats import norm\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from collections import defaultdict, deque\n",
        "from multiprocessing import Process\n",
        "from multiprocessing import Process\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import mplfinance as mpf\n",
        "import telebot\n",
        "import datetime\n",
        "import matplotlib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.signal import lfilter\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats import norm, pearsonr\n",
        "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import BaseCrossValidator\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "from bisect import bisect_left\n",
        "\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data.encoders import EncoderNormalizer\n",
        "from pytorch_forecasting.metrics import Metric, QuantileLoss\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import RichProgressBar, TQDMProgressBar\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "from torchmetrics import MeanSquaredError\n",
        "from torchmetrics.functional import mean_squared_error as torchmetrics_mse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import Any, Dict, Iterable, Iterator, List, Literal, Mapping, Optional, Tuple\n",
        "import os, gc, warnings, typing as tp\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "matplotlib.use(\"agg\")\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import psutil\n",
        "import os.path\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "TOKEN = 't.bWjxPlU6vz77qoqS754OKy0QorLDaZP-CE091dhGl56v7GHrqgF-mQAdWaeRg2kDRJmmxzvaaOwKUTxW6dnOKg'\n",
        "\n",
        "TICKERS1 =  ['RAGR', 'MGNT', 'MSTT', 'VRSB', 'PRFN', 'MTLR', 'LIFE', 'UPRO', 'GECO',\n",
        "       'BANE', 'MTLRP', 'GEMC', 'NVTK', 'TRNFP', 'TRMK', 'LSNGP', 'OBNEP',\n",
        "       'SNGSP', 'UWGN', 'MRKP', 'KZOSP', 'YDEX', 'NLMK', 'IRKT', 'CNTLP',\n",
        "       'LENT', 'KLSB', 'SELG', 'NMTP', 'UNAC', 'VKCO', 'MRKU', 'UGLD', 'NTZL',\n",
        "       'BANEP', 'FLOT', 'TGKJ', 'MAGN', 'ROSN', 'TGKB', 'AFKS', 'TTLK', 'HEAD',\n",
        "       'KZIZ', 'NOMP', 'OKEY', 'ABRD', 'NSVZ', 'MRKV', 'LSNG', 'MRKC', 'SVAV',\n",
        "       'ETLN', 'MRKZ', 'LNZL', 'CNRU', 'BSPB', 'RBCM', 'PMSB', 'LSRG', 'RNFT',\n",
        "       'MOEX', 'GTRK', 'NKHP', 'LKOH', 'SBERP', 'SBER', 'PLZL', 'RENI', 'MDMG',\n",
        "       'AFLT', 'FESH', 'OBNE', 'X5', 'MGTSP', 'DVEC', 'KROT', 'TATNP', 'OZPH',\n",
        "       'TGKN', 'TATN', 'PMSBP', 'TGKBP', 'SPBE', 'LNZLP', 'CHMK', 'KZIZP',\n",
        "       'RKKE', 'FRHC']\n",
        "\n",
        "TICKERS = ['MGTSP']#, 'NOMP', 'OBNEP']\n",
        "\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/t_ml/data/'\n",
        "path_to_save = '/content/drive/MyDrive/t_ml/data/'\n",
        "CANDLE_INTERVAL = CandleInterval.CANDLE_INTERVAL_15_MIN\n",
        "TIMEFRAME_MINUTES = 15\n",
        "HISTORY_DATA_POINTS = 5000\n",
        "\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}open_price.txt', 'r') as f:\n",
        "        open_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}close_price.txt', 'r') as f:\n",
        "        close_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}high_price.txt', 'r') as f:\n",
        "        high_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}low_price.txt', 'r') as f:\n",
        "        low_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}volume.txt', 'r') as f:\n",
        "        volume = json.loads(f.read())\n",
        "\n",
        "    print(\"Данные 1 успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке данных: {e}\")\n",
        "    open_price, close_price, high_price, low_price, volume = {}, {}, {}, {}, {}\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}time_last_kline_start.txt', 'r') as f:\n",
        "        time_last_kline_start = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}time_last_kline_end.txt', 'r') as f:\n",
        "        time_last_kline_end = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}ma.txt', 'r') as f:\n",
        "        ma = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}open_trades.txt', 'r') as f:\n",
        "        open_trades = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}trading_data.txt', 'r') as f:\n",
        "        trading_data = json.loads(f.read())\n",
        "\n",
        "    print(\"Данные 2 успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке данных: {e}\")\n",
        "    time_last_kline_start, time_last_kline_end, ma, open_trades, trading_data = {}, {}, {}, {}, {}\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "\n",
        "    with open(f'{path_to_save}pmax.txt', 'r') as f:\n",
        "        pmax = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}signals.txt', 'r') as f:\n",
        "        signals = json.loads(f.read())\n",
        "\n",
        "    print(\"Данные 3 успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке данных: {e}\")\n",
        "    pmax, signals = {}, {}\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "print('Загрузка Моделей', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "try:\n",
        "    kmeans_global = joblib.load(f'{path_to_save}models/kmeans_global.pkl')\n",
        "    print('Модель MiniBatchKMeans успешно загруженна', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели MiniBatchKMeans: {e}\")\n",
        "    kmeans_global = joblib.load(f'{path_to_save}models/kmeans_global.pkl')\n",
        "    print('Модель MiniBatchKMeans загруженна со второй попытки', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    scaler_global = joblib.load(f'{path_to_save}models/scaler_global.pkl')\n",
        "    print('Скалер успешно загружен', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели MiniBatchKMeans: {e}\")\n",
        "    scaler_global = joblib.load(f'{path_to_save}models/scaler_global.pkl')\n",
        "    print('Скалер загружен со второй попытки', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "'''\n",
        "try:\n",
        "    with open(f'{path_to_save}/models/regression_model_general.dill', 'rb') as file:\n",
        "        regression_model = dill.load(file)\n",
        "        print('Модель выхода из сделки загруженна', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели регрессии: {e}\")\n",
        "    with open(f'{path_to_save}/models/regression_model_general.dill', 'rb') as file:\n",
        "        regression_model = dill.load(file)\n",
        "        print('Модель выхода из сделки загруженна с второйпопытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "try:\n",
        "    with open(f'{path_to_save}/models/global_model.dill', 'rb') as file:\n",
        "        classifier_model = dill.load(file)\n",
        "        print('Глобальная модель поиска входа для всех кластеров заргуженна',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели поиска входа для 0 кластера: {e}\")\n",
        "    with open(f'{path_to_save}/ful_tickers_params.txt', 'rb') as file:\n",
        "        classifier_model = dill.load(file)\n",
        "        print('Глобальная модель поиска входа для всех кластеров загруженна с второй попытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "'''\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}/ful_tickers_params_new.txt', 'r') as file:\n",
        "      ticker_params = json.load(file)\n",
        "      print('Параметры для тикетов загруженны',\n",
        "            f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке параметров тикетов: {e}\")\n",
        "    with open(f'{path_to_save}/ful_tickers_params_new.txt', 'r') as file:\n",
        "        ticker_params = json.load(file)\n",
        "        print('Параметры для тикетов загруженны со второй попытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    #transformers_path = f'{path_to_save}/models/neuros/'\n",
        "    #models_path = f'{path_to_save}/models/final_models/'\n",
        "\n",
        "    with open(f'{path_to_save}/models/final_cols.pkl', 'rb') as f:\n",
        "        final_cols = pickle.load(f)\n",
        "\n",
        "    with open(f'{path_to_save}/models/final_params.pkl', 'rb') as f:\n",
        "        final_params = pickle.load(f)\n",
        "\n",
        "    with open(f'{path_to_save}/models/best_methods.json', 'rb') as f:\n",
        "        methods = json.load(f)\n",
        "\n",
        "    with open(f'{path_to_save}close_preds.txt', 'r') as f:\n",
        "        close_preds = json.loads(f.read())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при определении пути на модели и трансформеры: {e}\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}/phase_ful_tickers_params.txt', 'r') as file:\n",
        "      phase_ticker_params = json.load(file)\n",
        "      print('Параметры для тикетов с фазами загруженны',\n",
        "            f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке параметров тикетов с фазами: {e}\")\n",
        "    with open(f'{path_to_save}/phase_ful_tickers_params.txt', 'r') as file:\n",
        "        phase_ticker_params = json.load(file)\n",
        "        print('Параметры для тикетов с фазами загруженны со второй попытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "\n",
        "def _whittaker_smooth(y: np.ndarray, lam: float = 50.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Whittaker–Eilers smoothing (penalized least squares, D2).\n",
        "    Линейный безфазовый фильтр, хорошо убирает шум, не смещая тренд.\n",
        "    \"\"\"\n",
        "    import scipy.sparse as sp\n",
        "    import scipy.sparse.linalg as spla\n",
        "\n",
        "    n = len(y)\n",
        "    if n <= 2:\n",
        "        return y.copy()\n",
        "\n",
        "    E = sp.eye(n, format=\"csc\")\n",
        "    # Вторая разность (D2): размер (n-2) x n\n",
        "    diagonals = [np.ones(n), -2*np.ones(n), np.ones(n)]\n",
        "    D2 = sp.diags(diagonals, [0, 1, 2], shape=(n-2, n), format=\"csc\")\n",
        "    coef = E + lam * (D2.T @ D2)\n",
        "    z = spla.spsolve(coef, y.astype(float))\n",
        "    return z\n",
        "\n",
        "def _rearrange_preserving_marginal(z_raw: np.ndarray, z_smooth: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Distribution-preserving smoothing via monotone rearrangement:\n",
        "    - сортируем исходный z_raw -> z_sorted,\n",
        "    - берём порядок (argsort) сглаженного z_smooth,\n",
        "    - раскладываем z_sorted по этому порядку.\n",
        "    В итоге: гладко по времени (за счёт порядка z_smooth), но эмпирическое\n",
        "    распределение строго совпадает с исходным (z_raw).\n",
        "    \"\"\"\n",
        "    if len(z_raw) == 0:\n",
        "        return z_raw\n",
        "    order = np.argsort(z_smooth)\n",
        "    z_sorted = np.sort(z_raw)\n",
        "    out = np.empty_like(z_raw)\n",
        "    out[order] = z_sorted\n",
        "    return out\n",
        "\n",
        "def calculate_smoothed_target_qnorm(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    batch_start: int = 0,\n",
        "    epsilon: float = 1e-6,\n",
        "    round_decimals: int = 1,\n",
        "\n",
        "    # базовая нормализация по окну (buy->sell)\n",
        "    tight_spread_thr: float = 1e-4,\n",
        "\n",
        "    # сглаживание в z-домене\n",
        "    smooth_method: Literal[\"gauss\", \"savgol\", \"whittaker\"] = \"gauss\",\n",
        "    gauss_sigma: float = 2.0,\n",
        "    savgol_window: int = 11,   # нечётное\n",
        "    savgol_poly: int = 3,\n",
        "    whittaker_lambda: float = 50.0,\n",
        "\n",
        "    # квантильная нормализация и глобальный маппинг\n",
        "    clip_z: float = 2.5,       # клип в z-подобной шкале перед [-1,1]\n",
        "    tanh_scale: float | None = None,\n",
        "\n",
        "    # джиттер перед ECDF\n",
        "    dequant_jitter: float = 1e-4,\n",
        "\n",
        "    # опция \"обязательно растянуть каждый батч\" (робастная эквализация по квантилям)\n",
        "    per_batch_equalize: bool = False,\n",
        "    per_batch_q: float = 0.01,  # напр. 1% и 99% -> [-1, 1]\n",
        "\n",
        "    random_state: int | None = 42,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Возвращает df с колонками:\n",
        "      - normalized_target ∈ [-1, 1] (единая глобальная шкала)\n",
        "      - batch (int64): идентификатор события (buy→sell)\n",
        "\n",
        "    Алгоритм:\n",
        "      1) Внутри каждого события: base(0..1) -> ECDF -> z_raw ~ N(0,1).\n",
        "      2) Сглаживаем (Gaussian/Savitzky–Golay/Whittaker) -> z_smooth.\n",
        "      3) Rearrangement: z_preserved = rearrange(z_raw, order-of z_smooth).\n",
        "         Это убирает шум, но полностью сохраняет исходное распределение.\n",
        "      4) Глобальная квантильная нормализация: приводим все батчи к общей\n",
        "         маргинальной функции Q_global(p).\n",
        "      5) Единая глобальная шкала (median/MAD), клип до [-clip_z, clip_z],\n",
        "         линейный маппинг в [-1, 1], опционально tanh.\n",
        "      6) (Опционально) per-batch эквализация по квантилям до [-1, 1].\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(random_state)\n",
        "    df = df.copy()\n",
        "    df[\"normalized_target\"] = np.nan\n",
        "    df[\"batch\"] = np.nan\n",
        "    df[\"event_sell_time\"] = pd.to_datetime(df[\"event_sell_time\"], utc=True)\n",
        "\n",
        "    if savgol_window % 2 == 0:\n",
        "        savgol_window += 1\n",
        "    use_savgol = (savgol_window >= savgol_poly + 2)\n",
        "\n",
        "    all_indices: list[np.ndarray] = []\n",
        "    all_z_preserved: list[np.ndarray] = []\n",
        "    batch = batch_start\n",
        "\n",
        "    buy_rows = df.index[df[\"buy_signal\"] == True]\n",
        "    for start_i in buy_rows:\n",
        "        sell_time = df.at[start_i, \"event_sell_time\"]\n",
        "        if pd.isna(sell_time):\n",
        "            continue\n",
        "        sell_rows = df.index[df[\"time\"] == sell_time]\n",
        "        if len(sell_rows) == 0:\n",
        "            continue\n",
        "        end_i = sell_rows[0]\n",
        "\n",
        "        mask = (df.index >= start_i) & (df.index <= end_i)\n",
        "        idx = df.index[mask]\n",
        "        if idx.empty:\n",
        "            continue\n",
        "\n",
        "        high_s = df.loc[idx, \"high\"]\n",
        "        low_s  = df.loc[idx, \"low\"]\n",
        "\n",
        "        # Робастные полки только для базовой формы (0..1)\n",
        "        max_p = np.round(high_s.quantile(0.92), round_decimals)\n",
        "        min_p = np.round(low_s .quantile(0.08), round_decimals)\n",
        "        if max_p - min_p < tight_spread_thr:\n",
        "            max_p, min_p = float(high_s.max()), float(low_s.min())\n",
        "\n",
        "        use_profit_norm = (max_p - min_p) < tight_spread_thr\n",
        "\n",
        "        # 1) base 0..1\n",
        "        if not use_profit_norm:\n",
        "            base = (df.loc[idx, \"close\"] - min_p) / (max_p - min_p + 1e-12)\n",
        "        else:\n",
        "            buy_price = df.at[start_i, \"close\"]\n",
        "            max_prof = (high_s.max() - buy_price) / max(buy_price, 1e-12)\n",
        "            max_prof = max(max_prof, epsilon)\n",
        "            base = (df.loc[idx, \"close\"] - buy_price) / (buy_price * max_prof)\n",
        "            base = 0.5 + 0.5 * base\n",
        "\n",
        "        base = np.clip(base.to_numpy(float), epsilon, 1 - epsilon)\n",
        "\n",
        "        # Разбиваем дубликаты\n",
        "        if dequant_jitter and len(base) > 0:\n",
        "            base = base + rng.normal(scale=dequant_jitter, size=base.shape)\n",
        "            base = np.clip(base, epsilon, 1 - epsilon)\n",
        "\n",
        "        # 2) ECDF -> z_raw ~ N(0,1)\n",
        "        n = len(base)\n",
        "        ranks = pd.Series(base, index=idx).rank(method=\"first\").to_numpy()\n",
        "        ecdf = (ranks - 0.5) / max(n, 1)\n",
        "        ecdf = np.clip(ecdf, epsilon, 1 - epsilon)\n",
        "        z_raw = norm.ppf(ecdf)\n",
        "\n",
        "        # 3) Сглаживание (без фазовых сдвигов)\n",
        "        if n > 2:\n",
        "            if smooth_method == \"gauss\":\n",
        "                z_sm = gaussian_filter1d(z_raw, sigma=gauss_sigma, mode=\"nearest\")\n",
        "            elif smooth_method == \"savgol\" and use_savgol and n >= savgol_window:\n",
        "                z_sm = savgol_filter(z_raw, window_length=savgol_window, polyorder=savgol_poly, mode=\"interp\")\n",
        "            elif smooth_method == \"whittaker\":\n",
        "                z_sm = _whittaker_smooth(z_raw, lam=whittaker_lambda)\n",
        "            else:\n",
        "                # запасной вариант, если окно слишком короткое\n",
        "                z_sm = z_raw.copy()\n",
        "        else:\n",
        "            z_sm = z_raw.copy()\n",
        "\n",
        "        # 4) Rearrangement: сохраняем распределение z_raw, но используем порядок z_sm\n",
        "        z_preserved = _rearrange_preserving_marginal(z_raw, z_sm)\n",
        "        #z_preserved = np.where(z_preserved > 0, z_preserved * 1.2, z_preserved)  # Усиливаем положительные пики\n",
        "        #z_preserved = np.clip(z_preserved, -clip_z * 1.5, clip_z * 1.5)\n",
        "\n",
        "        all_indices.append(idx.to_numpy())\n",
        "        all_z_preserved.append(z_preserved)\n",
        "        df.loc[idx, \"batch\"] = batch\n",
        "        batch += 1\n",
        "\n",
        "    # Если событий не нашлось\n",
        "    if len(all_indices) == 0:\n",
        "        df[\"batch\"] = df[\"batch\"].fillna(batch_start).astype(\"int64\", errors=\"ignore\")\n",
        "        df[\"normalized_target\"] = df[\"normalized_target\"].fillna(0.0)\n",
        "        return df\n",
        "\n",
        "    # 5) Глобальная квантильная нормализация: одна общая маргинальная функция\n",
        "    z_pool = np.concatenate(all_z_preserved, axis=0)\n",
        "    z_pool_sorted = np.sort(z_pool)\n",
        "    N = len(z_pool_sorted)\n",
        "    # сетка перцентилей соответствующая отсортированным значениям\n",
        "    p_pool = (np.arange(N) + 0.5) / N\n",
        "\n",
        "    def q_global(p: np.ndarray) -> np.ndarray:\n",
        "        p = np.clip(p, p_pool[0], p_pool[-1])\n",
        "        return np.interp(p, p_pool, z_pool_sorted)\n",
        "\n",
        "    all_z_qn: list[np.ndarray] = []\n",
        "    for z_preserved in all_z_preserved:\n",
        "        n = len(z_preserved)\n",
        "        # перцентиль внутри батча\n",
        "        p_batch = (pd.Series(z_preserved).rank(method=\"first\").to_numpy() - 0.5) / max(n, 1)\n",
        "        z_qn = q_global(p_batch)  # теперь у батча та же маргиналка, что и у пула\n",
        "        all_z_qn.append(z_qn)\n",
        "\n",
        "    # 6) Единая глобальная шкала -> [-1, 1]\n",
        "    g_med = np.median(z_pool)\n",
        "    g_mad = np.median(np.abs(z_pool - g_med)) + 1e-12\n",
        "    scale = 1.4826 * g_mad\n",
        "\n",
        "    pos = 0\n",
        "    for idx, z_qn in zip(all_indices, all_z_qn):\n",
        "        z_g = (z_qn - g_med) / scale\n",
        "        y = np.clip(z_g, -clip_z, clip_z) / clip_z\n",
        "        if tanh_scale:\n",
        "            y = np.tanh(y * tanh_scale) / np.tanh(tanh_scale)\n",
        "\n",
        "        # 7) (опционально) робастная per-batch эквализация до [-1,1]\n",
        "        if per_batch_equalize and len(y) >= 3:\n",
        "            q = per_batch_q\n",
        "            lo, hi = np.quantile(y, [q, 1 - q])\n",
        "            if hi - lo > 1e-12:\n",
        "                y = (y - lo) / (hi - lo)  # [0..1]\n",
        "                y = 2.0 * np.clip(y, 0.0, 1.0) - 1.0  # [-1..1]\n",
        "            y = np.clip(y, -1.0, 1.0)\n",
        "\n",
        "        df.loc[idx, \"normalized_target\"] = y\n",
        "        pos += len(idx)\n",
        "\n",
        "    df[\"batch\"] = df[\"batch\"].astype(\"int64\", errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "time_last_kline_dt_cache = {}\n",
        "\n",
        "def _to_utc_floor_minute(ts):\n",
        "    return pd.to_datetime(ts, utc=True).floor('T')\n",
        "\n",
        "def _iso_utc(ts):\n",
        "    return pd.Timestamp(ts, tz='UTC').floor('T').isoformat()\n",
        "\n",
        "def _get_dt_scale_for_ticker(ticker):\n",
        "    times_iso = time_last_kline_start.get(ticker, [])\n",
        "    if not times_iso:\n",
        "        return pd.DatetimeIndex([])\n",
        "    if ticker not in time_last_kline_dt_cache or len(time_last_kline_dt_cache[ticker]) != len(times_iso) or (\n",
        "        len(times_iso) and time_last_kline_dt_cache[ticker][-1] != _to_utc_floor_minute(times_iso[-1])\n",
        "    ):\n",
        "        time_last_kline_dt_cache[ticker] = pd.DatetimeIndex(pd.to_datetime(times_iso, utc=True))\n",
        "    return time_last_kline_dt_cache[ticker]\n",
        "\n",
        "\n",
        "def _slice_indices_by_time(ticker, start_time_utc=None, end_time_utc=None):\n",
        "    \"\"\"\n",
        "    Возвращает (start_idx, end_idx) по шкале time_last_kline_start[ticker], границы включительные.\n",
        "    Если start_time_utc/end_time_utc None — граница опущена.\n",
        "    Возвращает None, если шкала пуста или нет пересечения.\n",
        "    \"\"\"\n",
        "    dt_scale = _get_dt_scale_for_ticker(ticker)\n",
        "    if len(dt_scale) == 0:\n",
        "        return None\n",
        "\n",
        "    if start_time_utc is None:\n",
        "        start_idx = 0\n",
        "    else:\n",
        "        start_ts = _to_utc_floor_minute(start_time_utc)\n",
        "        start_idx = int(dt_scale.searchsorted(start_ts, side='left'))\n",
        "        if start_idx >= len(dt_scale):\n",
        "            return None\n",
        "\n",
        "    if end_time_utc is None:\n",
        "        end_idx = len(dt_scale) - 1\n",
        "    else:\n",
        "        end_ts = _to_utc_floor_minute(end_time_utc)\n",
        "        # хотим включительно — берем правую границу и вычитаем 1, но если точное совпадение c началом свечи — ок\n",
        "        end_idx = int(dt_scale.searchsorted(end_ts, side='right') - 1)\n",
        "        if end_idx < 0:\n",
        "            return None\n",
        "\n",
        "    if start_idx > end_idx:\n",
        "        return None\n",
        "    return start_idx, end_idx\n",
        "\n",
        "def _now_monotonic():\n",
        "    return time.monotonic()\n",
        "\n",
        "def _exponential_backoff(attempt: int, base=1.0, cap=15.0, jitter=True):\n",
        "    # attempt: 0,1,2,...\n",
        "    delay = min(cap, base * (2 ** attempt))\n",
        "    if jitter:\n",
        "        # Полуджиттер: равномерный [delay/2, delay]\n",
        "        return random.uniform(delay / 2, delay)\n",
        "    return delay\n",
        "\n",
        "def _is_retryable_grpc_error(e: AioRpcError) -> bool:\n",
        "    code = e.code()\n",
        "    # UNAVAILABLE и CANCELLED — точно перезапускаем\n",
        "    if code in (StatusCode.UNAVAILABLE, StatusCode.CANCELLED, StatusCode.DEADLINE_EXCEEDED):\n",
        "        return True\n",
        "    # INTERNAL иногда тоже имеет смысл ретраить, но осторожно\n",
        "    if code == StatusCode.INTERNAL:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _is_retryable_generic_error(err: Exception) -> bool:\n",
        "    s = str(err)\n",
        "    needles = [\n",
        "        \"Remote end closed connection without response\",\n",
        "        \"RST_STREAM\",\n",
        "        \"Connection reset by peer\",\n",
        "        \"Server disconnected\",\n",
        "        \"Transport closed\",\n",
        "        \"EOF occurred in violation of protocol\",\n",
        "        \"Stream removed\",\n",
        "        \"recvmsg\",\n",
        "        \"Broken pipe\",\n",
        "        \"Timed out\",\n",
        "    ]\n",
        "    return any(n in s for n in needles)\n",
        "\n",
        "@dataclass\n",
        "class TransformerEntry:\n",
        "    pipe: Pipeline  # preprocessing -> to_dense -> model\n",
        "\n",
        "@dataclass\n",
        "class LGBEntry:\n",
        "    best_method: str\n",
        "    pipe_reg: Optional[Pipeline]\n",
        "    pipe_rank: Optional[Pipeline]\n",
        "    meta: Dict[str, Any]\n",
        "    threshold: float\n",
        "    calib_reg: Dict[str, float]\n",
        "    w_reg: Optional[float]\n",
        "\n",
        "class ModelsRuntimeCache:\n",
        "    def __init__(self):\n",
        "        self.transformers_path: str = \"\"\n",
        "        self.models_path: str = \"\"\n",
        "        self.final_cols: Dict[str, list] = {}\n",
        "        self.final_params: Dict[str, Any] = {}\n",
        "        self.methods: Dict[str, str] = {}\n",
        "        self.scaler_global = None\n",
        "        self.kmeans_global = None\n",
        "        self.transformers: Dict[str, TransformerEntry] = {}\n",
        "        self.lgb_models: Dict[str, LGBEntry] = {}\n",
        "\n",
        "    def init_metadata(self, path_to_save: str):\n",
        "        # Нормализуем пути\n",
        "        self.transformers_path = os.path.join(path_to_save, \"models\", \"neuros\")\n",
        "        self.models_path = os.path.join(path_to_save, \"models\", \"final_models\")\n",
        "\n",
        "        # Метаданные\n",
        "        with open(os.path.join(path_to_save, \"models\", \"final_cols.pkl\"), \"rb\") as f:\n",
        "            self.final_cols = pickle.load(f)\n",
        "        with open(os.path.join(path_to_save, \"models\", \"final_params.pkl\"), \"rb\") as f:\n",
        "            self.final_params = pickle.load(f)\n",
        "        with open(os.path.join(path_to_save, \"models\", \"best_methods.json\"), \"rb\") as f:\n",
        "            self.methods = json.load(f)\n",
        "\n",
        "        # Глобальные объекты\n",
        "        self.scaler_global = joblib.load(os.path.join(path_to_save, \"models\", \"scaler_global.pkl\"))\n",
        "        self.kmeans_global = joblib.load(os.path.join(path_to_save, \"models\", \"kmeans_global.pkl\"))\n",
        "\n",
        "    def _has_transformer_on_disk(self, ticker: str) -> bool:\n",
        "        base = os.path.join(self.transformers_path, ticker)\n",
        "        return os.path.isdir(base)\n",
        "\n",
        "    def _has_lgb_on_disk(self, ticker: str) -> bool:\n",
        "        # Если у вас load_model_for_ticker сам умеет искать файлы — можно опустить эту проверку.\n",
        "        # Оставим мягкую проверку по каталогу тикера.\n",
        "        base = os.path.join(self.models_path, ticker)\n",
        "        return os.path.isdir(base)\n",
        "\n",
        "    def load_transformer_for(self, ticker: str, device: Optional[str] = \"cpu\") -> Optional[Pipeline]:\n",
        "        if ticker in self.transformers:\n",
        "            return self.transformers[ticker].pipe\n",
        "        if not self._has_transformer_on_disk(ticker):\n",
        "            return None\n",
        "        base = os.path.join(self.transformers_path, ticker)\n",
        "        pipe = load_transformer_exact(base, device=device or \"cpu\")\n",
        "        self.transformers[ticker] = TransformerEntry(pipe=pipe)\n",
        "        return pipe\n",
        "\n",
        "    def load_lgb_for(self, ticker: str) -> Optional[LGBEntry]:\n",
        "        if ticker in self.lgb_models:\n",
        "            return self.lgb_models[ticker]\n",
        "        if not self._has_lgb_on_disk(ticker):\n",
        "            return None\n",
        "        data = load_model_for_ticker(ticker, models_dir=self.models_path)\n",
        "        entry = LGBEntry(\n",
        "            best_method=data[\"best_method\"],\n",
        "            pipe_reg=data.get(\"pipe_reg\"),\n",
        "            pipe_rank=data.get(\"pipe_rank\"),\n",
        "            meta=data.get(\"meta\", {}),\n",
        "            threshold=data.get(\"threshold\", 0.0),\n",
        "            calib_reg=data.get(\"calib_reg\", {\"a\": 1.0, \"b\": 0.0}),\n",
        "            w_reg=data.get(\"w_reg\"),\n",
        "        )\n",
        "        self.lgb_models[ticker] = entry\n",
        "        return entry\n",
        "\n",
        "    def preload_all(self, tickers: list, device: str = \"cpu\"):\n",
        "        \"\"\"Аккуратно предзагружает модели только для тех тикеров, которые есть во всех справочниках\n",
        "        и у которых реально есть файлы на диске. Остальные — тихо пропускает.\"\"\"\n",
        "        loaded, skipped = [], []\n",
        "\n",
        "        for t in tickers:\n",
        "            # Проверим наличие описаний в метаданных\n",
        "            has_params = t in self.final_params\n",
        "            has_cols = t in self.final_cols\n",
        "            # Выбираем метод, но отсутствие метода не критично — дефолт 'regressor'\n",
        "            method = self.methods.get(t, \"regressor\")\n",
        "\n",
        "            if not (has_params and has_cols):\n",
        "                skipped.append((t, \"no_meta\"))\n",
        "                continue\n",
        "\n",
        "            # Трансформер\n",
        "            tr_ok = self.load_transformer_for(t, device=device) is not None\n",
        "            # LGB\n",
        "            lgb_ok = self.load_lgb_for(t) is not None\n",
        "\n",
        "            if tr_ok and lgb_ok:\n",
        "                loaded.append(t)\n",
        "            else:\n",
        "                reason = (\"no_transformer\" if not tr_ok else \"no_lgb\")\n",
        "                skipped.append((t, reason))\n",
        "\n",
        "        return {\"loaded\": loaded, \"skipped\": skipped}\n",
        "\n",
        "def to_dense(X):\n",
        "    \"\"\"Преобразует разреженную матрицу в плотную.\"\"\"\n",
        "    if issparse(X):\n",
        "        return X.toarray()\n",
        "    return X\n",
        "\n",
        "class ToDenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Преобразует разреженную матрицу в плотную numpy-массив.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        if issparse(X):\n",
        "            return X.toarray()\n",
        "        return X\n",
        "    def get_feature_names_out(self, input_features: tp.Sequence[str] | None = None):\n",
        "        return np.asarray(input_features) if input_features is not None else np.array([])\n",
        "\n",
        "def deep_elbow(imp: np.ndarray, win: int = 5, eps: float = 0.02) -> int:\n",
        "    \"\"\"\n",
        "    Берём окно длиной win, считаем средний относительный спад.\n",
        "    Первое место, где спад < eps, считаем плато.\n",
        "    \"\"\"\n",
        "    if len(imp) <= win:\n",
        "        return len(imp)\n",
        "    dif = np.abs(np.diff(imp) / (imp[:-1] + 1e-9))\n",
        "    # скользящее среднее\n",
        "    m = np.convolve(dif, np.ones(win) / win, mode=\"valid\")\n",
        "    flat = np.nonzero(m < eps)[0]\n",
        "    return int(flat[0] + win) if flat.size else len(imp)\n",
        "\n",
        "# ───────────────────────────────────────────────────────\n",
        "# 2.  корреляционная чистка (быстрая, исправленная)\n",
        "# ───────────────────────────────────────────────────────\n",
        "def corr_prune(df: pd.DataFrame, feats: list[str], thr=.95) -> list[str]:\n",
        "    if len(feats) < 2 or thr >= 1:\n",
        "        return feats\n",
        "    X  = df[feats].apply(pd.to_numeric, errors='ignore')\n",
        "    C  = X.corr().abs().to_numpy()\n",
        "    keep = []\n",
        "    for i in range(len(feats)):\n",
        "        if not keep or C[i, keep].max() < thr:\n",
        "            keep.append(i)\n",
        "    return [feats[i] for i in keep]\n",
        "\n",
        "# ───────────────────────────────────────────────────────\n",
        "# 3.  универсальный быстрый селектор\n",
        "# ───────────────────────────────────────────────────────\n",
        "def fast_feature_select(\n",
        "        res           : pd.DataFrame,      # feature / importance\n",
        "        df_full       : pd.DataFrame,      # датасет для corr-prune\n",
        "        target_col    : str = \"target\",\n",
        "        *,\n",
        "        method        : str = \"elbow\",     # elbow | deep_elbow | percentile | quantile | top_k\n",
        "        top_k         : int = 150,         # для method=\"top_k\"\n",
        "        perc_limit    : float = .90,       # для method=\"percentile\"\n",
        "        quantile_q    : float = .10,       # для method=\"quantile\"\n",
        "        elbow_eps     : float = .05,       # (> flat %) для (shallow) elbow\n",
        "        deep_win      : int = 5,           # окно для deep_elbow\n",
        "        deep_eps      : float = .02,       # порог для deep_elbow\n",
        "        corr_thr      : float = .95        # корреляционный порог\n",
        ") -> list[str]:\n",
        "\n",
        "    ranked = res.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    feats  = ranked.feature.to_numpy()\n",
        "    imps   = ranked.importance.to_numpy()\n",
        "\n",
        "    # ---------- 1) сколько оставить  ----------\n",
        "    if method == \"elbow\":                 # одношаговое колено\n",
        "        k = np.argmax(np.abs(np.diff(imps) / (imps[:-1] + 1e-9)) < elbow_eps) + 1\n",
        "        if k == 1:        # колено не найдено\n",
        "            k = len(imps)\n",
        "    elif method == \"deep_elbow\":\n",
        "        k = deep_elbow(imps, win=deep_win, eps=deep_eps)\n",
        "    elif method == \"percentile\":          # кумулятивная доля\n",
        "        cum = np.cumsum(imps)\n",
        "        k   = np.searchsorted(cum / cum[-1], perc_limit) + 1\n",
        "    elif method == \"quantile\":\n",
        "        thr = np.quantile(imps, 1 - quantile_q)\n",
        "        k   = int((imps >= thr).sum())\n",
        "    elif method == \"top_k\":\n",
        "        k = min(top_k, len(feats))\n",
        "    else:\n",
        "        raise ValueError(\"unknown method\")\n",
        "\n",
        "    selected = feats[:k].tolist()\n",
        "\n",
        "    # ---------- 2) корреляционная чистка ----------\n",
        "    selected = corr_prune(\n",
        "        df_full.drop(columns=[target_col], errors='ignore'),\n",
        "        selected,\n",
        "        thr=corr_thr\n",
        "    )\n",
        "\n",
        "    return selected\n",
        "\n",
        "def patch_feature_timings(cls):\n",
        "    \"\"\"\n",
        "    Оборачивает все методы cls, начинающиеся на _feat_,\n",
        "    и складывает затраченное время в self._timings[method_name].\n",
        "    Вызывать сразу после объявления класса.\n",
        "    \"\"\"\n",
        "    def timed(func):\n",
        "        def wrapper(self, *args, **kwargs):\n",
        "            t0 = time.perf_counter()\n",
        "            result = func(self, *args, **kwargs)\n",
        "            dt = time.perf_counter() - t0\n",
        "            # заводим словарь при первом же вызове\n",
        "            if not hasattr(self, \"_timings\"):\n",
        "                self._timings = {}\n",
        "            self._timings[func.__name__] = dt\n",
        "            return result\n",
        "        return wrapper\n",
        "\n",
        "    for name, method in inspect.getmembers(cls, inspect.isfunction):\n",
        "        if name.startswith(\"_feat_\"):             # ← только расчётные функции\n",
        "            setattr(cls, name, timed(method))\n",
        "\n",
        "    return cls\n",
        "\n",
        "def _slope(y):\n",
        "        x = np.arange(len(y))\n",
        "        # линейная регрессия «по формуле»\n",
        "        xm, ym = x.mean(), y.mean()\n",
        "        beta = ((x - xm) * (y - ym)).sum() / ((x - xm)**2).sum()\n",
        "        return beta\n",
        "\n",
        "@njit\n",
        "def _rolling_entropy_exact_numba(x, window):\n",
        "    \"\"\" Точная реализация rolling entropy с bins='auto' для каждого окна. \"\"\"\n",
        "    n = len(x)\n",
        "    res = np.empty(n, dtype=np.float32)\n",
        "    res[:] = np.nan\n",
        "    if window < 2:\n",
        "        return res\n",
        "    for end in range(window - 1, n):\n",
        "        win = x[end - window + 1 : end + 1]\n",
        "        a_min = np.min(win)\n",
        "        a_max = np.max(win)\n",
        "        if a_min == a_max:\n",
        "            res[end] = 0.0\n",
        "            continue\n",
        "        sorted_win = np.sort(win)\n",
        "        idx25 = int(0.25 * window)\n",
        "        idx75 = int(0.75 * window)\n",
        "        q25 = sorted_win[idx25]\n",
        "        q75 = sorted_win[idx75]\n",
        "        iqr = q75 - q25\n",
        "        sturges = int(np.ceil(np.log2(window) + 1))\n",
        "        if iqr > 0:\n",
        "            bin_width = 2.0 * iqr / (window ** (1.0 / 3.0))\n",
        "            fd = int(np.ceil((a_max - a_min) / bin_width))\n",
        "        else:\n",
        "            fd = 1\n",
        "        nbins = max(sturges, fd, 1)\n",
        "        edges = np.empty(nbins + 1, dtype=np.float32)\n",
        "        step = (a_max - a_min) / nbins\n",
        "        edges[0] = a_min\n",
        "        for i in range(1, nbins):\n",
        "            edges[i] = a_min + i * step\n",
        "        edges[nbins] = a_max\n",
        "        counts = np.zeros(nbins, dtype=np.int32)  # int32 достаточно для window<=1e9\n",
        "        for val in win:\n",
        "            idx = np.searchsorted(edges, val, side='right') - 1\n",
        "            if 0 <= idx < nbins:\n",
        "                counts[idx] += 1\n",
        "        ent = 0.0\n",
        "        total = float(window)\n",
        "        for c in counts:\n",
        "            if c > 0:\n",
        "                p = c / total\n",
        "                ent -= p * np.log(p + 1e-10)\n",
        "        res[end] = ent\n",
        "    return res\n",
        "\n",
        "@jit(nopython=True)\n",
        "def rolling_autocorr(arr, window):\n",
        "    n = len(arr)\n",
        "    result = np.full(n, 0.0)\n",
        "    for i in range(n):\n",
        "        start = max(0, i - window + 1)\n",
        "        w = i - start + 1\n",
        "        if w <= 1:\n",
        "            continue\n",
        "        s = arr[start: i + 1]\n",
        "        a = s[1:]\n",
        "        b = s[:-1]\n",
        "        n_pts = w - 1\n",
        "        mean_a = np.sum(a) / n_pts\n",
        "        mean_b = np.sum(b) / n_pts\n",
        "        cov = np.dot(a, b) / n_pts - mean_a * mean_b\n",
        "        var_a = np.dot(a, a) / n_pts - mean_a * mean_a\n",
        "        var_b = np.dot(b, b) / n_pts - mean_b * mean_b\n",
        "        if var_a <= 0 or var_b <= 0:\n",
        "            result[i] = np.nan\n",
        "        else:\n",
        "            std_a = np.sqrt(var_a)\n",
        "            std_b = np.sqrt(var_b)\n",
        "            result[i] = cov / (std_a * std_b)\n",
        "    return result\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "@patch_feature_timings\n",
        "class FeatureCalculatorForRegression:\n",
        "    \"\"\"\n",
        "    df  -- исходный OHLCV-DataFrame.\n",
        "    required_features -- список имён колонок, которые нужны модели.\n",
        "    params -- { primitive_name: {... гиперпараметры ...}, 'stat_window': int }.\n",
        "    \"\"\"\n",
        "\n",
        "    _PRIMITIVES = {\n",
        "        \"MEDPRICE\":               \"_feat_base\",\n",
        "        \"MACD\":                   \"_feat_macd\",\n",
        "        \"MACD_Hist\":              \"_feat_macd\",\n",
        "        \"Overbought_Oversold\":    \"_feat_overbought\",\n",
        "        \"Overbought_Oversold_Index_mean\": \"_feat_overbought\",\n",
        "        \"Price_MADist%\":          \"_feat_madist\",\n",
        "        \"Mean_Reversion\":         \"_feat_mean_reversion\",\n",
        "        \"Fear_Greed\":             \"_feat_fear_greed\",\n",
        "        \"perc_var_open_close\":    \"_feat_price_variation\",\n",
        "        \"pmax_norm\":              \"_feat_pmax_ma\",\n",
        "        \"ma_norm\":                \"_feat_pmax_ma\",\n",
        "        \"ma_pmax_norm_rage\":      \"_feat_pmax_ma\",\n",
        "        \"ma_pmax_norm_rage_pct\":  \"_feat_pmax_ma\",\n",
        "        \"slope_trend\":            \"_feat_slope\",\n",
        "        \"ema_trend\":              \"_feat_ema_trend\",\n",
        "        \"hp_trend\":               \"_feat_hp_trend\",\n",
        "        \"trade_bars_counter\":     \"_feat_trade_duration\",\n",
        "        \"ROC\":                    \"_feat_roc\",\n",
        "        \"ATR_norm\":               \"_feat_atr\",\n",
        "        \"BB_Width\":               \"_feat_bb_width\",\n",
        "        \"Asset_Growth\":           \"_feat_asset_growth\",\n",
        "        \"ema_acceleration\":       \"_feat_ema_acceleration\",\n",
        "        \"price_change\":           \"_feat_price_change\",\n",
        "        \"Asset_To_Equity_Ratio\":  \"_feat_asset_to_equity_ratio\",\n",
        "        \"volume_ratio\":           \"_feat_fear_greed_index\",\n",
        "        \"WILLR\":                  \"_feat_willr\",\n",
        "        \"kf_trend\":               \"_feat_kf_trend\",\n",
        "        \"Fractal_Dim\":            \"_feat_fractal_dim\",\n",
        "        \"Peak_Exhaustion_Score\":  \"_feat_peak_exhaustion\",\n",
        "        \"%B_BB\":                  \"_feat_bb_percent\",\n",
        "        \"Kurtosis_roll\":          \"_feat_kurtosis_roll\",\n",
        "        \"OBV_div\":                \"_feat_obv_div\",\n",
        "        \"RSI_slope\":              \"_feat_rsi_slope\",\n",
        "        \"Vol_Decay\":              \"_feat_vol_decay\",\n",
        "        \"Accel_Decay\":            \"_feat_accel_decay\",\n",
        "        \"Entropy_roll\":           \"_feat_entropy_roll\",\n",
        "        \"Wavelet_Var_Ratio\":      \"_feat_wavelet_var\",\n",
        "        \"Autocorr_Lag1\":          \"_feat_autocorr\",\n",
        "        \"Beta_Market\":            \"_feat_beta\",\n",
        "        \"PSC\":                    \"_feat_peak_squeeze_curvature\",\n",
        "        \"PSC_raw\":                \"_feat_peak_squeeze_curvature\",\n",
        "        \"PSC_z\":                  \"_feat_peak_squeeze_curvature\",\n",
        "        \"PSC_sigmoid\":            \"_feat_peak_squeeze_curvature\",\n",
        "    }\n",
        "\n",
        "    # СТАРАЯ: r\"^ago_(\\d+)_\"\n",
        "    # НОВАЯ: умеет и \"ago50_\", и \"ago_50_\"\n",
        "    _LAG_RE  = re.compile(r\"^ago_?(\\d+)_\")\n",
        "    _STAT_RE = re.compile(r\"_(mean|min|max|std|skew|kurt|quantile(\\d{2}))$\")\n",
        "    _LOGSF   = \"_logsf\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, ticker):\n",
        "        self.df = df.copy()\n",
        "        self.ticker = ticker\n",
        "        f64 = self.df.select_dtypes(\"float64\").columns\n",
        "        self.df[f64] = self.df[f64].astype(np.float32)\n",
        "        if \"time\" in self.df:\n",
        "            ts = pd.to_datetime(self.df[\"time\"], utc=True, errors=\"coerce\")\n",
        "            self.df[\"hour\"]        = ts.dt.hour.astype(\"int8\")\n",
        "            self.df[\"day_of_week\"] = ts.dt.day_of_week.astype(\"int8\")\n",
        "\n",
        "    def calculate_features(\n",
        "        self,\n",
        "        required_features: Iterable[str],\n",
        "        params: Mapping[str, Mapping[str, Any]] | None = None\n",
        "    ) -> pd.DataFrame:\n",
        "        saved_cols = ['regime', 'normalized_target', 'batch', 'time', 'open', 'close', 'high', 'low', 'volume', 'buy_signal',\n",
        "                      'sell_signal', 'event_sell_time', 'event_sell_price', 'event_time', 'event_price', 'event_sell_time',\n",
        "                      'event_sell_price', 'target', 'pnl', 'ma', 'pmax']\n",
        "        self._params      = defaultdict(dict, params or {})\n",
        "        self._stat_window = self._params.get(\"stat_window\", 50)\n",
        "        for col in required_features:\n",
        "            self._ensure_column(col)\n",
        "        out = self.df[list(required_features)].copy()\n",
        "        f64 = out.select_dtypes(\"float64\").columns\n",
        "        out[f64] = out[f64].astype(np.float32)\n",
        "\n",
        "        for mandatory_col in saved_cols:\n",
        "            if mandatory_col in self.df.columns:\n",
        "                out[mandatory_col] = self.df[mandatory_col]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def calculate_all_possible_features(self, params: Mapping[str, Mapping[str, Any]] | None = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Вычисляет все возможные фичи, исключая lag-версии для указанных колонок.\n",
        "        Все бесконечные значения (np.inf/-np.inf) заменяются на 0.\n",
        "        Порядок вычислений:\n",
        "        1. Все базовые примитивы\n",
        "        2. Lag-версии фич (кроме исключенных)\n",
        "        3. Статистики для всех фич\n",
        "        \"\"\"\n",
        "        # Инициализация параметров\n",
        "        if not hasattr(self, '_params'):\n",
        "            self._params = defaultdict(dict, params or {})\n",
        "        self._stat_window = self._params.get(\"stat_window\", 50)\n",
        "\n",
        "        # Колонки, для которых не нужно создавать lag-версии\n",
        "        EXCLUDE_FROM_LAGS = {\n",
        "            'time', 'open', 'close', 'high', 'low', 'volume',\n",
        "            'ma', 'pmax', 'buy_signal', 'sell_signal', 'regime',\n",
        "            'event_time', 'event_price', 'event_sell_time',\n",
        "            'event_sell_price', 'pnl', 'target', 'normalized_target',\n",
        "            'batch', 'hour', 'day_of_week', 'trade_bars_counter'\n",
        "        }\n",
        "\n",
        "        # 1. Вычисляем все базовые примитивы\n",
        "        all_primitives = list(self._PRIMITIVES.keys())\n",
        "        for primitive in all_primitives:\n",
        "            method_name = self._PRIMITIVES[primitive]\n",
        "            primitive_params = self._params.get(primitive, {})\n",
        "            try:\n",
        "                getattr(self, method_name)(**primitive_params)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при вычислении примитива {primitive}: {str(e)}\")\n",
        "\n",
        "        # 2. Добавляем lag-версии только для разрешенных фич\n",
        "        numeric_cols = [\n",
        "            col for col in self.df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
        "            if col not in EXCLUDE_FROM_LAGS and  # Исключаем указанные колонки\n",
        "            not self._LAG_RE.match(col) and      # Исключаем уже lag-фичи\n",
        "            not col.endswith(self._LOGSF) and    # Исключаем logsf-фичи\n",
        "            not self._STAT_RE.search(col)        # Исключаем статистики\n",
        "        ]\n",
        "\n",
        "        lag_periods = [1, 2, 3, 5, 10, 20, 50]  # Стандартные лаги\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            for lag in lag_periods:\n",
        "                lag_col = f\"ago_{lag}_{col}\"\n",
        "                if lag_col not in self.df.columns:\n",
        "                    self.df[lag_col] = self.df[col].shift(lag)\n",
        "\n",
        "        # 3. Добавляем статистики для всех фич (кроме исключенных)\n",
        "        all_cols_for_stats = [\n",
        "            col for col in self.df.columns\n",
        "            if col not in EXCLUDE_FROM_LAGS and\n",
        "            not col.endswith(self._LOGSF) and\n",
        "            not self._STAT_RE.search(col)\n",
        "        ]\n",
        "\n",
        "        stats = ['mean', 'std', 'min', 'max', 'skew', 'kurt']\n",
        "\n",
        "        for col in all_cols_for_stats:\n",
        "            for stat in stats:\n",
        "                stat_col = f\"{col}_{stat}\"\n",
        "                if stat_col not in self.df.columns:\n",
        "                    try:\n",
        "                        self._add_stat(col, stat)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Ошибка при вычислении статистики {stat} для {col}: {str(e)}\")\n",
        "\n",
        "        # 4. Добавляем logsf-версии только для разрешенных фич\n",
        "        main_cols_for_logsf = [\n",
        "            col for col in numeric_cols\n",
        "            if not col.startswith('ago_') and\n",
        "            not col.endswith(self._LOGSF) and\n",
        "            col not in EXCLUDE_FROM_LAGS\n",
        "        ]\n",
        "\n",
        "        for col in main_cols_for_logsf:\n",
        "            logsf_col = f\"{col}{self._LOGSF}\"\n",
        "            if logsf_col not in self.df.columns:\n",
        "                try:\n",
        "                    self.df[logsf_col] = norm.logsf(self.df[col])\n",
        "                except Exception as e:\n",
        "                    print(f\"Ошибка при вычислении logsf для {col}: {str(e)}\")\n",
        "\n",
        "        # 5. Заменяем бесконечные значения на 0\n",
        "        numeric_cols_all = self.df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
        "        self.df[numeric_cols_all] = self.df[numeric_cols_all].replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        # Сохраняем все оригинальные колонки\n",
        "        for col in EXCLUDE_FROM_LAGS:\n",
        "            if col in self.df.columns and col not in self.df:\n",
        "                self.df[col] = self.df[col]\n",
        "\n",
        "        return self.df.copy()\n",
        "\n",
        "    def _ensure_column(self, name: str):\n",
        "        if name in self.df:\n",
        "            return\n",
        "\n",
        "        # 1) lag-префикс \"ago50_\" или \"ago_50_\"\n",
        "        m = self._LAG_RE.match(name)\n",
        "        if m:\n",
        "            lag  = int(m.group(1))\n",
        "            base = name[m.end():]\n",
        "            self._ensure_column(base)\n",
        "            self.df[name] = self.df[base].shift(lag)\n",
        "            return\n",
        "\n",
        "        # 2) _logsf\n",
        "        if name.endswith(self._LOGSF):\n",
        "            base = name[:-len(self._LOGSF)]\n",
        "            self._ensure_column(base)\n",
        "            self.df[name] = norm.logsf(self.df[base])\n",
        "            return\n",
        "\n",
        "        # 3) статистический суффикс\n",
        "        m = self._STAT_RE.search(name)\n",
        "        if m:\n",
        "            stat = m.group(1)\n",
        "            base = name[:m.start()]\n",
        "            self._ensure_column(base)\n",
        "            self._add_stat(base, stat)\n",
        "            return\n",
        "\n",
        "        # 4) примитив\n",
        "        prim = name\n",
        "        if prim.startswith(\"Overbought_Oversold\"):\n",
        "            prim = \"Overbought_Oversold\"\n",
        "        if prim.startswith(\"Fear_Greed\"):\n",
        "            prim = \"Fear_Greed\"\n",
        "        if prim not in self._PRIMITIVES:\n",
        "            raise KeyError(f\"Не знаю, как получить примитив «{prim}» для «{name}»\")\n",
        "        getattr(self, self._PRIMITIVES[prim])(**self._params.get(prim, {}))\n",
        "        if name not in self.df:\n",
        "            raise RuntimeError(f\"После _feat_{prim}() нет колонки «{name}»\")\n",
        "\n",
        "    def _add_stat(self, base: str, stat: str):\n",
        "        col = f\"{base}_{stat}\"\n",
        "        if col in self.df:\n",
        "            return\n",
        "        s = self.df[base]; w = self._stat_window\n",
        "        if stat == \"mean\":\n",
        "            self.df[col] = s.rolling(w).mean()\n",
        "        elif stat == \"std\":\n",
        "            self.df[col] = s.rolling(w).std()\n",
        "        elif stat == \"min\":\n",
        "            self.df[col] = s.rolling(w).min()\n",
        "        elif stat == \"max\":\n",
        "            self.df[col] = s.rolling(w).max()\n",
        "        elif stat == \"skew\":\n",
        "            self.df[col] = s.rolling(w).skew()\n",
        "        elif stat == \"kurt\":\n",
        "            self.df[col] = s.rolling(w).kurt()\n",
        "        elif stat.startswith(\"quantile\"):\n",
        "            q = int(stat[-2:]) / 100\n",
        "            self.df[col] = s.rolling(w).quantile(q)\n",
        "        else:\n",
        "            raise ValueError(f\"Неизвестная stat «{stat}»\")\n",
        "\n",
        "    # ---------------------- ПРИМИТИВЫ ----------------------\n",
        "\n",
        "    def _feat_base(self, medprice: int = 50):\n",
        "        if \"MEDPRICE\" in self.df:\n",
        "            return\n",
        "        self.df[\"MEDPRICE\"]      = (self.df[\"high\"] + self.df[\"low\"]) / 2\n",
        "        self.df[\"MEDPRICE_std\"] = self.df[\"MEDPRICE\"].rolling(medprice).std()\n",
        "\n",
        "    def _feat_macd(self, fast: int = 12, slow: int = 26, signal: int = 9):\n",
        "        \"\"\"\n",
        "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
        "        \"\"\"\n",
        "        if {\"MACD\",\"MACD_Hist\"}.issubset(self.df.columns):\n",
        "            return\n",
        "\n",
        "        close = self.df['close']\n",
        "        # Создаем множества для уникальных периодов\n",
        "        ema_cache_fp = close.ewm(span=fast, adjust=False).mean()\n",
        "\n",
        "        ema_cache_sp = close.ewm(span=slow, adjust=False).mean()\n",
        "        rolling_cache = close.rolling(window=slow).mean()\n",
        "\n",
        "        # Основной цикл вычислений\n",
        "        ema_fast = ema_cache_fp\n",
        "        ema_slow = ema_cache_sp\n",
        "        rolling_mean = rolling_cache\n",
        "        macd = ema_fast - ema_slow\n",
        "        macd_norm = macd / rolling_mean\n",
        "        self.df[f'MACD'] = macd_norm\n",
        "        signal = macd.ewm(span=signal, adjust=False).mean()\n",
        "        signal_norm = signal / rolling_mean\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        self.df[f'MACD_Hist'] = macd_norm - signal_norm\n",
        "\n",
        "    def _feat_overbought(self, rsi_p: int = 14, stoch_p: int = 14):\n",
        "        name = \"Overbought_Oversold_Index\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        c   = self.df[\"close\"]; d = c.diff()\n",
        "        g   = d.clip(lower=0); l = (-d).clip(lower=0)\n",
        "        rs  = g.rolling(rsi_p).mean() / (l.rolling(rsi_p).mean().add(1e-10))\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        lo  = self.df[\"low\"].rolling(stoch_p).min()\n",
        "        hi  = self.df[\"high\"].rolling(stoch_p).max()\n",
        "        st  = 100*(c - lo)/(hi - lo + 1e-10)\n",
        "        self.df[name] = (rsi + st)/2\n",
        "\n",
        "    def _feat_madist(self, span_lenght: int = 200):\n",
        "        name = \"Price_MADist%\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ema = self.df[\"close\"].ewm(span=span_lenght, adjust=False).mean()\n",
        "        self.df[name] = (self.df[\"close\"]/ema - 1)*100\n",
        "\n",
        "    def _feat_mean_reversion(self, window: int = 20):\n",
        "        name = \"Mean_Reversion\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ma = self.df[\"close\"].rolling(window).mean()\n",
        "        self.df[name] = self.df[\"close\"] - ma\n",
        "\n",
        "    def _feat_fear_greed(self, window: int = 14):\n",
        "        name = \"Fear_Greed_Index\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        v  = self.df[\"close\"].pct_change().rolling(window).std()\n",
        "        vc = self.df[\"volume\"].pct_change().rolling(window).mean()\n",
        "        tr = self.df[\"close\"]/self.df[\"close\"].rolling(window).mean()\n",
        "        self.df[name] = (v + vc + tr)/3*100\n",
        "\n",
        "    def _feat_price_variation(self):\n",
        "        name = \"perc_var_open_close\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        eps = 1e-10\n",
        "        self.df[name] = (self.df[\"close\"]-self.df[\"open\"])/(self.df[\"open\"]+eps)*100\n",
        "\n",
        "    def _feat_pmax_ma(self,\n",
        "        pmax_ma_length: int = 10,\n",
        "        pmax_ma_length_roll: int = 50,\n",
        "        pct_window: int = 5\n",
        "    ):\n",
        "        need = {\n",
        "            \"pmax_norm\", \"ma_norm\",\n",
        "            \"ma_pmax_norm_rage\", \"ma_pmax_norm_rage_pct\"\n",
        "        }\n",
        "        if need.issubset(self.df.columns):\n",
        "            return\n",
        "        if {\"pmax\",\"ma\"}.difference(self.df.columns):\n",
        "            raise ValueError(\"Нужны 'pmax' и 'ma'\")\n",
        "        c = self.df[\"close\"]\n",
        "        self.df[\"pmax_norm\"]             = (c-self.df[\"pmax\"])/self.df[\"pmax\"]\n",
        "        self.df[\"ma_norm\"]               = (c-self.df[\"ma\"])/self.df[\"ma\"]\n",
        "        self.df[\"ma_pmax_norm_rage\"]     = self.df[\"ma_norm\"] - self.df[\"pmax_norm\"]\n",
        "        # новый примитив — pct-динамика\n",
        "        self.df[\"ma_pmax_norm_rage_pct\"] = \\\n",
        "          self.df[\"ma_pmax_norm_rage\"].pct_change(pct_window).fillna(0)\n",
        "\n",
        "    def _feat_slope(self, slope_lag: int = 300, pct_window: int = 6):\n",
        "        name = \"slope_trend\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
        "        self.df[name] = r.rolling(slope_lag, min_periods=slope_lag)\\\n",
        "                         .apply(_slope, raw=True)\n",
        "\n",
        "    def _feat_ema_trend(self, span: int = 300, pct_window: int = 6):\n",
        "        name = \"ema_trend\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
        "        e = r.ewm(span=span, adjust=False).mean()\n",
        "        self.df[name] = e.diff().fillna(0)\n",
        "\n",
        "    def _feat_asset_to_equity_ratio(self):\n",
        "        \"\"\"\n",
        "        Вычисление коэффициента соотношения активов и собственного капитала.\n",
        "        \"\"\"\n",
        "        name = \"Asset_To_Equity_Ratio\"\n",
        "        asset = self.df['close']\n",
        "        equity = self.df['low']\n",
        "        # Добавляем в DataFrame\n",
        "        self.df[name] = asset / (equity + 1e-10)\n",
        "\n",
        "    def _feat_hp_trend(self, lamb: float = 1600):\n",
        "        name = \"hp_trend\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        y    = np.log(self.df[\"close\"]).fillna(method=\"ffill\")\n",
        "        coef = lamb/(1+lamb)\n",
        "        tr   = np.empty(len(y), dtype=float)\n",
        "        tr[0] = y.iloc[0]\n",
        "        for i in range(1, len(y)):\n",
        "            tr[i] = coef*y.iloc[i] + (1-coef)*tr[i-1]\n",
        "        self.df[name] = np.append([0], np.diff(tr))\n",
        "\n",
        "    def _feat_kf_trend(self,\n",
        "        pct_window: int = 6,\n",
        "        obs_var: float = 1e-4, # σ² ε_t (шум наблюдения)\n",
        "        level_var: float = 1e-5 # σ² η_t (шум уровня)\n",
        "        ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Добавляет к DataFrame колонки:\n",
        "        kf_trend — one-sided Калман-оценка тренда доходностей\n",
        "        kf_trend_logsf — лог-survival-function (z-score) тренда\n",
        "        Полностью каузально, обновляется тик-за-тиком.\n",
        "        \"\"\"\n",
        "\n",
        "        name = 'kf_trend'\n",
        "        # 1. Доходности\n",
        "        r = self.df['close'].pct_change(pct_window).fillna(0)\n",
        "        # 2. Local-level модель: y_t = μ_t + ε_t ;  μ_t = μ_{t-1} + η_t\n",
        "        mod = sm.tsa.UnobservedComponents(r, level='llevel')\n",
        "\n",
        "        # 3. Параметры модели в log-шкале (требование statsmodels)\n",
        "        params = np.log([obs_var, level_var])\n",
        "\n",
        "        # 4. Только forward-filter → нет look-ahead bias\n",
        "        res = mod.filter(params)                       # <— односторонний Калман\n",
        "        trend = pd.Series(res.filtered_state[0], index=self.df.index)\n",
        "\n",
        "        # 5. Запись результата\n",
        "        self.df['kf_trend'] = trend\n",
        "\n",
        "    def _feat_willr(self, window=14):\n",
        "        \"\"\"\n",
        "        Вычисление %R по методу Уильямса (WILLR).\n",
        "        \"\"\"\n",
        "        name = 'WILLR'\n",
        "        high = self.df['high']\n",
        "        low = self.df['low']\n",
        "        close = self.df['close']\n",
        "\n",
        "        highest_high = high.rolling(window).max()\n",
        "        lowest_low = low.rolling(window).min()\n",
        "\n",
        "        willr = ((highest_high - close) / (highest_high - lowest_low)) * -100\n",
        "\n",
        "        # Добавляем в DataFrame\n",
        "        self.df[name] = willr\n",
        "\n",
        "    def _feat_fear_greed_index(self, window: int = 14):\n",
        "        \"\"\"\n",
        "        Расчет объема как отношение последнего объема к скользящему среднему.\n",
        "        \"\"\"\n",
        "        name = \"volume_ratio\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        s = self.df[\"volume\"]\n",
        "        self.df[name] = s / s.rolling(window).mean()\n",
        "\n",
        "    def _feat_trade_duration(self):\n",
        "        name = \"trade_bars_counter\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = np.nan\n",
        "        entries = self.df.index[self.df[\"event_time\"].notna()]\n",
        "        last    = self.df.index[-1]\n",
        "        for st in entries:\n",
        "            sell = self.df.at[st, \"event_sell_time\"]\n",
        "            ends = self.df.index[self.df[\"time\"] == sell]\n",
        "            end  = ends[0] if len(ends) else last\n",
        "            s,e  = self.df.index.get_loc(st), self.df.index.get_loc(end)\n",
        "            self.df.loc[self.df.index[s:e+1], name] = np.arange(e-s+1, dtype=np.float32)\n",
        "\n",
        "    def _feat_roc(self, window: int = 5):\n",
        "        name = \"ROC\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = self.df[\"close\"].pct_change(window)\n",
        "\n",
        "    def _feat_atr(self, atr_window: int = 14):\n",
        "        name = \"ATR_norm\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        h,l,c = self.df[\"high\"], self.df[\"low\"], self.df[\"close\"]\n",
        "        tr1 = h-l\n",
        "        tr2 = (h-c.shift()).abs()\n",
        "        tr3 = (l-c.shift()).abs()\n",
        "        tr  = pd.concat([tr1,tr2,tr3], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_window).mean()\n",
        "        self.df[name] = atr/c\n",
        "\n",
        "    def _feat_bb_width(self, bb_window: int = 20):\n",
        "        name = \"BB_Width\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        c   = self.df[\"close\"]\n",
        "        ma  = c.rolling(bb_window).mean()\n",
        "        std = c.rolling(bb_window).std()\n",
        "        self.df[name] = 2*std/ma\n",
        "\n",
        "    def _feat_asset_growth(self, window: int = 3):\n",
        "        name = \"Asset_Growth\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = self.df[\"close\"].pct_change(window).fillna(0)*100\n",
        "\n",
        "    def _feat_ema_acceleration(self, pct_window: int = 3, ema_window: int = 300):\n",
        "        name = \"ema_acceleration\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
        "        e = r.ewm(span=ema_window).mean()\n",
        "        self.df[name] = e.diff(4)\n",
        "\n",
        "    def _feat_price_change(self, window: int = 1):\n",
        "        name = \"price_change\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = self.df[\"close\"].pct_change(window).fillna(0)\n",
        "\n",
        "    def _feat_peak_exhaustion(\n",
        "        self,\n",
        "        price_win: int = 60,    # окно \"локального максимума\"\n",
        "        mom_win:   int = 10,    # окно для momentum\n",
        "        vol_win:   int = 20,\n",
        "        atr_win:   int = 14,\n",
        "        z_win:     int = 100    # z-score нормализация\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Peak-Exhaustion Score  ~ 0…1\n",
        "        1 → почти наверху, импульс затух, объём падает, ATR высок.\n",
        "        \"\"\"\n",
        "        name = \"Peak_Exhaustion_Score\"\n",
        "        c = self.df[\"close\"]\n",
        "\n",
        "        # 1) расстояние до локального max\n",
        "        roll_max = c.rolling(price_win).max()\n",
        "        dist_max = (roll_max - c) / roll_max          # 0 — на max, >0 — ниже\n",
        "\n",
        "        # 2) ослабевающий импульс\n",
        "        roc_now  = c.pct_change(mom_win)\n",
        "        roc_hist = roc_now.rolling(price_win).max()   # max импульса в окне\n",
        "        momentum_div = 1 - (roc_now / (roc_hist + 1e-12))   # 0 → свежий high\n",
        "\n",
        "        # 3) сушащийся объём\n",
        "        vol_ratio = self.df[\"volume\"] / \\\n",
        "            self.df[\"volume\"].rolling(vol_win).mean()\n",
        "\n",
        "        # 4) расширенный спред (ATR/price)\n",
        "        tr  = pd.concat([\n",
        "                self.df[\"high\"]  - self.df[\"low\"],\n",
        "                (self.df[\"high\"] - c.shift()).abs(),\n",
        "                (self.df[\"low\"]  - c.shift()).abs()\n",
        "            ], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_win).mean()\n",
        "        atr_norm = atr / c\n",
        "\n",
        "        # 5) агрегируем, переводим в z-score, squash σ → 0…1\n",
        "        raw = (dist_max + momentum_div + (1/vol_ratio) + atr_norm) / 4\n",
        "        z   = (raw - raw.rolling(z_win).mean()) / (raw.rolling(z_win).std() + 1e-9)\n",
        "        self.df[name] = 1 / (1 + np.exp(-z))   # σ(z)\n",
        "\n",
        "    def _feat_fractal_dim(self, short_win=20, long_win=40):\n",
        "        \"\"\"Вычисляет фрактальную размерность на основе отношения ATR разных периодов\"\"\"\n",
        "        name = \"Fractal_Dim\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "\n",
        "        # Вычисляем ATR для короткого периода\n",
        "        h, l, c = self.df['high'], self.df['low'], self.df['close']\n",
        "        tr1 = h - l\n",
        "        tr2 = (h - c.shift()).abs()\n",
        "        tr3 = (l - c.shift()).abs()\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        atr_short = tr.rolling(short_win).mean()\n",
        "\n",
        "        # Вычисляем ATR для длинного периода\n",
        "        atr_long = tr.rolling(long_win).mean()\n",
        "\n",
        "        # Вычисляем фрактальную размерность\n",
        "        ratio = atr_long / (atr_short + 1e-10)  # Добавляем небольшое значение для избежания деления на 0\n",
        "        self.df[name] = np.log(ratio) / np.log(2)\n",
        "\n",
        "    def _feat_bb_percent(self, window=20, std_mult=2):\n",
        "        name = \"%B_BB\"\n",
        "        if name in self.df: return\n",
        "        ma = self.df[\"close\"].rolling(window).mean()\n",
        "        std = self.df[\"close\"].rolling(window).std()\n",
        "        self.df[name] = (self.df[\"close\"] - (ma - std_mult * std)) / (4 * std)\n",
        "\n",
        "    def _feat_kurtosis_roll(self, window=50):\n",
        "        name = \"Kurtosis_roll\"\n",
        "        if name in self.df: return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        self.df[name] = ret.rolling(window).kurt()\n",
        "\n",
        "    def _feat_obv_div(self, window=10):\n",
        "        name = \"OBV_div\"\n",
        "        if name in self.df: return\n",
        "        sign = np.sign(self.df[\"close\"].diff())\n",
        "        obv = (sign * self.df[\"volume\"]).cumsum()\n",
        "        price_chg = self.df[\"close\"].pct_change(window)\n",
        "        obv_chg = obv.pct_change(window)\n",
        "        self.df[name] = price_chg - obv_chg\n",
        "\n",
        "    def _feat_rsi_slope(self, rsi_p=14, diff_win=5):\n",
        "        name = \"RSI_slope\"\n",
        "        if name in self.df: return\n",
        "        delta = self.df[\"close\"].diff()\n",
        "        gain = delta.clip(lower=0).rolling(rsi_p).mean()\n",
        "        loss = -delta.clip(lower=0).rolling(rsi_p).mean()\n",
        "        rsi = 100 - 100 / (1 + gain / (loss + 1e-10))\n",
        "        self.df[name] = rsi.diff(diff_win)\n",
        "\n",
        "    def _feat_vol_decay(self, window=20):\n",
        "        name = \"Vol_Decay\"\n",
        "        if name in self.df: return\n",
        "        vol_ema = self.df[\"volume\"].ewm(span=window).mean()\n",
        "        self.df[name] = self.df[\"volume\"] / vol_ema - 1\n",
        "\n",
        "    def _feat_accel_decay(self, window=10):\n",
        "        name = \"Accel_Decay\"\n",
        "        if name in self.df: return\n",
        "        vel = self.df[\"close\"].diff(window)\n",
        "        accel = vel.diff(window)\n",
        "        self.df[name] = accel / (vel.abs() + 1e-10)\n",
        "\n",
        "    def _feat_entropy_roll(self, window: int = 50):\n",
        "        name = \"Entropy_roll\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0.0).to_numpy(dtype=np.float32)\n",
        "        ent = _rolling_entropy_exact_numba(ret, window)\n",
        "        self.df[name] = ent\n",
        "\n",
        "    def _feat_wavelet_var(self, short_win=10, long_win=50):\n",
        "        name = \"Wavelet_Var_Ratio\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        var_short = ret.rolling(short_win).var()\n",
        "        var_long = ret.rolling(long_win).var()\n",
        "        self.df[name] = var_short / (var_long + 1e-10)\n",
        "\n",
        "    def _feat_autocorr(self, window=50):\n",
        "        name = \"Autocorr_Lag1\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        arr = ret.to_numpy()  # Use to_numpy() for compatibility\n",
        "        autocorrs = rolling_autocorr(arr, window)\n",
        "        self.df[name] = autocorrs\n",
        "\n",
        "    def _feat_beta(self, window=50):\n",
        "        name = \"Beta_Market\"\n",
        "        if name in self.df or \"market_close\" not in self.df:\n",
        "            return\n",
        "        ret_stock = self.df[\"close\"].pct_change().fillna(0)\n",
        "        ret_market = self.df[\"market_close\"].pct_change().fillna(0)\n",
        "        cov = ret_stock.rolling(window).cov(ret_market)\n",
        "        var_market = ret_market.rolling(window).var()\n",
        "        self.df[name] = cov / (var_market + 1e-10)\n",
        "\n",
        "    def _feat_peak_squeeze_curvature(self,\n",
        "                                vel_win: int = 5,\n",
        "                                acc_win: int = 5,\n",
        "                                vol_win: int = 20,\n",
        "                                atr_win: int = 14,\n",
        "                                z_win : int = 60):\n",
        "        \"\"\"\n",
        "        Возвращает 3 колонки:\n",
        "        PSC_raw, PSC_z, PSC_sigmoid ∈ [0,1]\n",
        "        \"\"\"\n",
        "        name = \"PSC\"\n",
        "        cols_need = {\"PSC_raw\",\"PSC_z\",\"PSC_sigmoid\"}\n",
        "        if cols_need.issubset(self.df.columns): return\n",
        "        c = self.df['close']\n",
        "\n",
        "        # 1) speed & accel\n",
        "        speed  = c.pct_change(vel_win).fillna(0)\n",
        "        accel  = speed.diff(acc_win).fillna(0)\n",
        "        curvature = accel / (speed.abs() + 1e-10)\n",
        "\n",
        "        # 2) squeeze = ATR_norm ↘ & HV_norm ↘\n",
        "        h,l = self.df['high'], self.df['low']\n",
        "        tr = pd.concat([h-l, (h-c.shift()).abs(), (l-c.shift()).abs()], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_win).mean()\n",
        "        hv  = c.pct_change().rolling(vol_win).std()\n",
        "        squeeze = - (atr / (c+1e-10)).diff().clip(upper=0)   # падение ATR\n",
        "        squeeze += - hv.diff().clip(upper=0)                 # падение HV\n",
        "        squeeze /= 2\n",
        "\n",
        "        # 3) агрегируем\n",
        "        raw = 0.6*curvature + 0.4*squeeze\n",
        "\n",
        "        # 4) z-score + σ(z)\n",
        "        mu  = raw.rolling(z_win).mean()\n",
        "        std = raw.rolling(z_win).std()\n",
        "        z = (raw - mu)/(std + 1e-9)\n",
        "        sigm = 1/(1+np.exp(-z))\n",
        "\n",
        "        self.df[f'{name}_raw']     = raw\n",
        "        self.df[f'{name}_z']       = z.clip(-5, 5)\n",
        "        self.df[f'{name}_sigmoid'] = sigm\n",
        "\n",
        "\n",
        "def calculate_metrics(test_data, y_test, y_pred, target_column='normalized_target'):\n",
        "    \"\"\"\n",
        "    Функция для расчета метрик по данным теста и предсказаниям модели.\n",
        "\n",
        "    Параметры:\n",
        "    - test_data: pd.DataFrame — тестовые данные с колонками batch, high, close и другими.\n",
        "    - y_test: pd.Series или np.array — фактические значения целевой переменной.\n",
        "    - y_pred: pd.Series или np.array — предсказанные моделью значения.\n",
        "    - target_column: str — название колонки целевой переменной в test_data.\n",
        "\n",
        "    Возвращает:\n",
        "    - avg_mse: float — среднеквадратическая ошибка.\n",
        "    - avg_r2: float — средняя R².\n",
        "    - std_r2: float — стандартное отклонение R².\n",
        "    - corr_mean: float — средняя корреляция.\n",
        "    - corr_std: float — стандартное отклонение корреляции.\n",
        "    - avg_missed: float — средний процент упущенной прибыли.\n",
        "    \"\"\"\n",
        "    # Инициализация метрик\n",
        "    mse_scores, r2_scores, corr_scores, missed_pnl = [], [], [], []\n",
        "\n",
        "    # Расчет корреляции целевой переменной и предсказаний\n",
        "    y_pred_series = pd.Series(y_pred, index=y_test.index)\n",
        "    corr_score = test_data[target_column].corr(y_pred_series)\n",
        "    corr_scores.append(corr_score)\n",
        "\n",
        "    # MSE и R2\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "    r2_scores.append(r2_score(y_test, y_pred))\n",
        "\n",
        "    # Расчет missed_pnl для каждого batch\n",
        "    \"\"\"for batch in test_data['batch'].unique():\n",
        "        mask = test_data['batch'] == batch\n",
        "        max_high = test_data.loc[mask, 'high'].max()\n",
        "        pred = y_pred[mask]  # предполагается, что y_pred соответствует normalized_target\n",
        "        sell_idx = np.argmin(pred)  # продажа на минимальном предсказанном значении\n",
        "        sell_price = test_data.loc[mask].iloc[sell_idx]['close']\n",
        "        missed = (max_high - sell_price) / (max_high - test_data.loc[mask].iloc[0]['close'])  # % упущенной прибыли\n",
        "        missed_pnl.append(missed)\"\"\"\n",
        "\n",
        "    # Усреднение метрик\n",
        "    avg_mse = float(np.mean(mse_scores))\n",
        "    avg_r2 = float(np.mean(r2_scores))\n",
        "    std_r2 = float(np.std(r2_scores))\n",
        "    corr_mean = float(np.mean(corr_scores))\n",
        "    corr_std = float(np.std(corr_scores))\n",
        "    #avg_missed = float(np.mean(missed_pnl))\n",
        "\n",
        "    # Проверка на корректность результатов\n",
        "    if np.isfinite([avg_mse, avg_r2, std_r2, corr_mean, corr_std]).all(): #avg_missed\n",
        "        return avg_mse, avg_r2, std_r2, corr_mean, corr_std, #avg_missed\n",
        "    else:\n",
        "        return float('inf'), float('inf'), float('inf'), float('inf'), float('inf'), float('inf')\n",
        "\n",
        "def prepare_data(df, target_col):\n",
        "    \"\"\"\n",
        "    Подготавливает данные: разделяет на числовые и категориальные признаки, создает конвейер преобразования.\n",
        "    \"\"\"\n",
        "    if type(target_col) == str:\n",
        "        df.dropna(inplace=True)\n",
        "        X = df.drop([target_col, 'batch'], axis=1)\n",
        "        y = df[target_col]\n",
        "    elif type(target_col) == list:\n",
        "        df.dropna(inplace=True)\n",
        "        X = df.drop(target_col+['batch'], axis=1)\n",
        "        y = df[target_col]\n",
        "\n",
        "    # Разделение на числовые и категориальные признаки\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Создание конвейера преобразования\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('normalize', PowerTransformer(method='yeo-johnson')),\n",
        "            ]), numeric_features),\n",
        "            ('cat', Pipeline([\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
        "            ]), categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return X, y, preprocessing\n",
        "\n",
        "\n",
        "def calculate_indicators_pred(df, features, params=None, ticker=None, mode=None, multy=False):\n",
        "\n",
        "    fc = FeatureCalculatorForRegression(df, ticker)\n",
        "    if mode==None:\n",
        "        df1 = fc.calculate_features(params=params, required_features=features)\n",
        "    else:\n",
        "        df1 = fc.calculate_all_possible_features()\n",
        "    features = ['open', 'close', 'high', 'low', 'volume', 'ma', 'pmax'] #time\n",
        "    df1['regime'] = df1['regime'].astype('object')\n",
        "    df1 = df1[df1['trade_bars_counter']>=0]\n",
        "    df1['trade_bars_counter'] = df1['trade_bars_counter'].astype('int')\n",
        "    df1 = df1.drop(features, axis=1)\n",
        "    #df1 = df1.dropna()\n",
        "    return df1, fc._timings\n",
        "\n",
        "def sample_feature_params(params) -> dict:\n",
        "        \"\"\"\n",
        "        Draws *one* sample of the whole feature-engineering hyper-parameter set.\n",
        "        Rule of thumb for ranges:\n",
        "          • lower bound = ‘sane minimum‘ from domain knowledge\n",
        "          • upper bound = ‘sane maximum’\n",
        "        Adjust them if you feel the search space is too wide or too narrow.\n",
        "        \"\"\"\n",
        "        # ---- helpers for monotone constraints ----------------------------------\n",
        "        fast  = params['macd_fast']\n",
        "        slow  = params['macd_slow']\n",
        "\n",
        "        slope_lag_min = params['slope_lag_min']\n",
        "        slope_lag     = params['slope_lag']\n",
        "\n",
        "        # ---- finally compose the nested dict -----------------------------------\n",
        "        return {\n",
        "            'base': {\n",
        "                'medprice': params['medprice']\n",
        "            },\n",
        "            'macd': {\n",
        "                'fast'      : fast,\n",
        "                'slow'      : slow,\n",
        "                'signal'    : params['macd_signal'],\n",
        "                'macd_roll' : params['macd_roll']\n",
        "            },\n",
        "            'overbought': {\n",
        "                'rsi_p'         : params['rsi_p'],\n",
        "                'stoch_p'       : params['stoch_p'],\n",
        "                'oversold_roll' : params['oversold_roll']\n",
        "            },\n",
        "            'madist': {\n",
        "                'span_lenght'   : params['madist_span'],\n",
        "                'madist_lenght' : params['madist_len']\n",
        "            },\n",
        "            'mean_reversion': {\n",
        "                'window' : params['mr_window']\n",
        "            },\n",
        "            'fear_greed': {\n",
        "                'greed_pct'    : params['fg_greed_pct'],\n",
        "                'volume_ratio_scr' : params['fg_vol_ratio'],\n",
        "                'window'       : params['fg_window'],\n",
        "                'greed_roll'   : params['fg_roll']\n",
        "            },\n",
        "            'price_variation': {\n",
        "                'variation_lenght': params['pv_len']\n",
        "            },\n",
        "            'pmax_ma': {\n",
        "                'pmax_ma_lenght'      : params['pmax_len'],\n",
        "                'pmax_ma_lenght_roll' : params['pmax_roll']\n",
        "            },\n",
        "            'slope': {\n",
        "                'slope_lag'     : slope_lag,\n",
        "                'slope_lag_min' : slope_lag_min,\n",
        "                'sloap_pct'     : params['slope_pct'],\n",
        "                'sloap_roll'    : params['slope_roll']\n",
        "            },\n",
        "            # _trade_duration_features – no params\n",
        "        }\n",
        "\n",
        "def build_feature_params(\n",
        "    flat_params: Dict[str, Any],\n",
        "    extra_alias: Optional[Dict[str, Tuple[str, str | None]]] = None\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Преобразует «плоский» словарь от Optuna в структуру,\n",
        "    которую понимает FeatureCalculatorForRegression.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Базовая явная таблица соответствий\n",
        "    alias: Dict[str, Tuple[str, str | None]] = {\n",
        "        'hp_lamb'          : ('hp_trend'           , 'lamb'),\n",
        "        'ea_pct'           : ('ema_acceleration'   , 'pct_window'),\n",
        "        'ea_ema'           : ('ema_acceleration'   , 'ema_window'),\n",
        "        'mr_window'        : ('Mean_Reversion'     , 'window'),\n",
        "        'ag_window'        : ('Asset_Growth'       , 'window'),\n",
        "        'medprice'         : ('MEDPRICE'           , 'medprice'),\n",
        "        'bb_window'        : ('BB_Width'           , 'bb_window'),\n",
        "        'macd_fast'        : ('MACD'               , 'fast'),\n",
        "        'macd_slow'        : ('MACD'               , 'slow'),\n",
        "        'macd_signal'      : ('MACD'               , 'signal'),\n",
        "        'fg_window'        : ('Fear_Greed'         , 'window'),\n",
        "        'atr_window'       : ('ATR_norm'           , 'atr_window'),\n",
        "        'vr_window'        : ('volume_ratio'       , 'window'),\n",
        "        'madist_span'      : ('Price_MADist%'      , 'span_lenght'),\n",
        "        'slope_lag'        : ('slope_trend'        , 'slope_lag'),\n",
        "        'slope_lag_min'    : ('slope_trend'        , 'slope_lag'),\n",
        "        'rsi_p'            : ('Overbought_Oversold', 'rsi_p'),\n",
        "        'stoch_p'          : ('Overbought_Oversold', 'stoch_p'),\n",
        "        'pmax_len'         : ('pmax_norm'          , 'pmax_ma_length'),\n",
        "        'pmax_roll'        : ('pmax_norm'          , 'pmax_ma_length_roll'),\n",
        "        'pc_window'        : ('pmax_norm'          , 'pct_window'),\n",
        "        'ema_trend_span'   : ('ema_trend'          , 'span'),\n",
        "        'ema_trend_pct'    : ('ema_trend'          , 'pct_window'),\n",
        "        'stat_window'      : ('stat_window', None),\n",
        "        # --- новые алиасы ---\n",
        "        'roc_window'        : ('ROC'          , 'window'),\n",
        "        'willr_window'      : ('WILLR'          , 'window'),\n",
        "        'fractal_short_win' : ('Fractal_Dim', 'short_win'),\n",
        "        'fractal_long_win'  : ('Fractal_Dim', 'long_win'),\n",
        "        'peak_price_win'    : ('Peak_Exhaustion_Score', 'price_win'),\n",
        "        'peak_mom_win'      : ('Peak_Exhaustion_Score', 'mom_win'),\n",
        "        'peak_vol_win'      : ('Peak_Exhaustion_Score', 'vol_win'),\n",
        "        'peak_atr_win'      : ('Peak_Exhaustion_Score', 'atr_win'),\n",
        "        'peak_z_win'        : ('Peak_Exhaustion_Score', 'z_win'),\n",
        "        'bb_window'         : ('%B_BB', 'window'),\n",
        "        'bb_std_mult'       : ('%B_BB', 'std_mult'),\n",
        "        'kurt_window'       : ('Kurtosis_roll', 'window'),\n",
        "        'obv_window'        : ('OBV_div', 'window'),\n",
        "        'rsi_slope_rsi_p'   : ('RSI_slope', 'rsi_p'),\n",
        "        'rsi_diff_win'      : ('RSI_slope', 'diff_win'),\n",
        "        'voldec_window'     : ('Vol_Decay', 'window'),\n",
        "        'acceldec_window'   : ('Accel_Decay', 'window'),\n",
        "        'ent_window'        : ('Entropy_roll', 'window'),\n",
        "        'wlt_short_win'     : ('Wavelet_Var_Ratio', 'short_win'),\n",
        "        'wlt_long_win'      : ('Wavelet_Var_Ratio', 'long_win'),\n",
        "        'acorr_window'      : ('Autocorr_Lag1', 'window'),\n",
        "        'beta_window'       : ('Beta_Market', 'window'),\n",
        "        'psc_vel_win'       : ('PSC', 'vel_win'),\n",
        "        'psc_acc_win'       : ('PSC', 'acc_win'),\n",
        "        'psc_vol_win'       : ('PSC', 'vol_win'),\n",
        "        'psc_atr_win'       : ('PSC', 'atr_win'),\n",
        "        'psc_z_win'         : ('PSC', 'z_win'),\n",
        "    }\n",
        "\n",
        "    # 2. Пользовательские переопределения\n",
        "    if extra_alias:\n",
        "        alias.update(extra_alias)\n",
        "\n",
        "    # 3. Автоматический разбор префиксов (fallback)\n",
        "    prefix_map: Dict[str, str] = {\n",
        "        'macd'        : 'MACD',\n",
        "        'hp'          : 'hp_trend',\n",
        "        'ea'          : 'ema_acceleration',\n",
        "        'mr'          : 'Mean_Reversion',\n",
        "        'ag'          : 'Asset_Growth',\n",
        "        'bb'          : 'BB_Width',\n",
        "        'fg'          : 'Fear_Greed',\n",
        "        'atr'         : 'ATR_norm',\n",
        "        'vr'          : 'volume_ratio',\n",
        "        'madist'      : 'Price_MADist%',\n",
        "        'slope'       : 'slope_trend',\n",
        "        'pmax'        : 'pmax_norm',\n",
        "        'ema_trend'   : 'ema_trend',\n",
        "        'rsi'         : 'Overbought_Oversold',\n",
        "        'stoch'       : 'Overbought_Oversold',\n",
        "        'fractal'     : 'Fractal_Dim',\n",
        "        'peak'        : 'Peak_Exhaustion_Score',\n",
        "        'bb'          : '%B_BB',\n",
        "        'kurt'        : 'Kurtosis_roll',\n",
        "        'obv'         : 'OBV_div',\n",
        "        'rsi_slope'   : 'RSI_slope',\n",
        "        'voldec'      : 'Vol_Decay',\n",
        "        'acceldec'    : 'Accel_Decay',\n",
        "        'entropy'     : 'Entropy_roll',\n",
        "        'wavelet'     : 'Wavelet_Var_Ratio',\n",
        "        'acorr'       : 'Autocorr_Lag1',\n",
        "        'beta'        : 'Beta_Market',\n",
        "        'psc'         : 'PSC',\n",
        "    }\n",
        "\n",
        "    nested: Dict[str, Dict[str, Any]] = defaultdict(dict)\n",
        "\n",
        "    for key, val in flat_params.items():\n",
        "\n",
        "        # 3.1 Явное соответствие\n",
        "        if key in alias:\n",
        "            prim, arg = alias[key]\n",
        "            if prim == 'stat_window' or arg is None:\n",
        "                nested['stat_window'] = val\n",
        "            else:\n",
        "                nested[prim][arg] = val\n",
        "            continue\n",
        "\n",
        "        # 3.2 Игнорируем вспомогательные ключи вида *_min, *_max, если\n",
        "        #     они не нужны никакому примитиву.\n",
        "        if key.endswith('_min') or key.endswith('_max'):\n",
        "            continue\n",
        "\n",
        "        # 3.3 Fallback-разбор _\n",
        "        if '_' in key:\n",
        "            prefix, arg = key.split('_', 1)\n",
        "            if prefix in prefix_map:\n",
        "                nested[prefix_map[prefix]][arg] = val\n",
        "                continue\n",
        "\n",
        "        # 3.4 Неизвестный ключ — игнорируем или логируем\n",
        "        # print(f'Warning: parameter \"{key}\" was not mapped')\n",
        "\n",
        "    return {p: d for p, d in nested.items()}\n",
        "\n",
        "_ORIG_INTERP = F.interpolate\n",
        "\n",
        "\n",
        "def _collapse_pred_to_bt(y_pred: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Приводим предсказание к виду (B, T), считая последнюю ось временем (горизонтом).\n",
        "    Все промежуточные оси (кроме batch=ось 0 и time=последняя ось) усредняем.\n",
        "    Пример: (B, 1, 4, 10) -> mean по осям (1,2) -> (B,10)\n",
        "    (B, 10) -> ок\n",
        "    (B, 1, 10) -> squeeze -> (B,10)\n",
        "    \"\"\"\n",
        "    if not isinstance(y_pred, torch.Tensor):\n",
        "        raise TypeError(f\"y_pred must be a tensor, got {type(y_pred)}\")\n",
        "\n",
        "    # Сначала уберём все единичные оси\n",
        "    if any(s == 1 for s in y_pred.shape[1:-1]):\n",
        "        # squeeze не трогает последнюю ось, если она не равна 1\n",
        "        y_pred = y_pred.squeeze()\n",
        "        # Если squeeze убрал не только единичные, но и привёл к (B, T) — хорошо.\n",
        "\n",
        "    if y_pred.dim() == 1:\n",
        "        # (B,) — интерпретируем как T=1, сделаем (B,1)\n",
        "        y_pred = y_pred.unsqueeze(-1)\n",
        "        return y_pred\n",
        "\n",
        "    if y_pred.dim() == 2:\n",
        "        # (B, T) — уже как надо\n",
        "        return y_pred\n",
        "\n",
        "    # Если размерностей больше 2: считаем last dim = time, batch = 0\n",
        "    # Все промежуточные оси схлопываем усреднением\n",
        "    reduce_dims = tuple(range(1, y_pred.dim() - 1))\n",
        "    if len(reduce_dims) > 0:\n",
        "        y_pred = y_pred.mean(dim=reduce_dims)\n",
        "    # На выходе (B, T)\n",
        "    if y_pred.dim() != 2:\n",
        "        # На всякий случай добьёмся (B, T)\n",
        "        y_pred = y_pred.view(y_pred.size(0), -1)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def _install_safe_interpolate_patch():\n",
        "    \"\"\"\n",
        "    Патч делает F.interpolate детерминированным при включённом torch.use_deterministic_algorithms(True)\n",
        "    для CUDA и режимов linear/bilinear/bicubic, прогоняя вычисление на CPU.\n",
        "    Идемпотентен и не меняет сигнатуру.\n",
        "    \"\"\"\n",
        "    if getattr(F.interpolate, \"_is_deterministic_wrapper\", False):\n",
        "        return\n",
        "\n",
        "    _orig_interpolate = F.interpolate\n",
        "\n",
        "    # Какие режимы считаем потенциально недетерминируемыми на CUDA\n",
        "    _CUDA_UNSAFE_MODES = {\"linear\", \"bilinear\", \"bicubic\"}  # 1d/2d/2d\n",
        "\n",
        "    def _needs_cpu_fallback(input, mode):\n",
        "        if not torch.is_tensor(input):\n",
        "            return False\n",
        "        if input.is_cuda and mode in _CUDA_UNSAFE_MODES:\n",
        "            # upsample_linear1d_backward_out_cuda и др. — недетерминируемы\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    @functools.wraps(_orig_interpolate)\n",
        "    def _deterministic_interpolate(\n",
        "        input: torch.Tensor,\n",
        "        size=None,\n",
        "        scale_factor=None,\n",
        "        mode=\"nearest\",\n",
        "        align_corners=None,\n",
        "        recompute_scale_factor=None,\n",
        "        antialias=False,\n",
        "    ):\n",
        "        # Если не нужно, просто вызовем оригинал\n",
        "        if not _needs_cpu_fallback(input, mode):\n",
        "            return _orig_interpolate(\n",
        "                input,\n",
        "                size=size,\n",
        "                scale_factor=scale_factor,\n",
        "                mode=mode,\n",
        "                align_corners=align_corners,\n",
        "                recompute_scale_factor=recompute_scale_factor,\n",
        "                antialias=antialias,\n",
        "            )\n",
        "\n",
        "        # CUDA + linear/bilinear/bicubic → CPU fallback\n",
        "        x = input\n",
        "        dev = x.device\n",
        "        orig_dtype = x.dtype\n",
        "\n",
        "        # Для стабильности переводим в float32 на CPU\n",
        "        x_cpu = x.detach().to(\"cpu\", dtype=torch.float32).requires_grad_(x.requires_grad)\n",
        "\n",
        "        y_cpu = _orig_interpolate(\n",
        "            x_cpu,\n",
        "            size=size,\n",
        "            scale_factor=scale_factor,\n",
        "            mode=mode,\n",
        "            align_corners=align_corners,\n",
        "            recompute_scale_factor=recompute_scale_factor,\n",
        "            antialias=antialias,\n",
        "        )\n",
        "\n",
        "        # Возвращаем на исходное устройство и тип\n",
        "        y = y_cpu.to(dev, dtype=orig_dtype)\n",
        "\n",
        "        return y\n",
        "\n",
        "    _deterministic_interpolate._is_deterministic_wrapper = True  # type: ignore[attr-defined]\n",
        "    F.interpolate = _deterministic_interpolate\n",
        "\n",
        "\n",
        "_install_safe_interpolate_patch()\n",
        "\n",
        "\n",
        "def _unpack_pf_batch(batch):\n",
        "    \"\"\"\n",
        "    Унифицированная распаковка батча из TimeSeriesDataSet.to_dataloader(...)\n",
        "    Возвращает: x (dict), y (Tensor|None), weight (Tensor|None)\n",
        "    \"\"\"\n",
        "    if isinstance(batch, (list, tuple)):\n",
        "        if len(batch) == 3:\n",
        "            x, y, weight = batch\n",
        "        elif len(batch) == 2:\n",
        "            x, y = batch\n",
        "            weight = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected batch tuple length: {len(batch)}\")\n",
        "    elif isinstance(batch, dict):\n",
        "        # На всякий случай поддержим dict → возьмём таргет из decoder_target, если есть\n",
        "        x = batch\n",
        "        y = batch.get(\"decoder_target\", None)\n",
        "        weight = None\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
        "    return x, y, weight\n",
        "\n",
        "\n",
        "# F.interpolate = _deterministic_interpolate\n",
        "\n",
        "\n",
        "def _extract_pred_tensor(y_pred):\n",
        "    # извлекаем тензор предикта из любых обёрток\n",
        "    if isinstance(y_pred, dict):\n",
        "        for key in (\"prediction\", \"output\", \"decoder_output\"):\n",
        "            if key in y_pred and torch.is_tensor(y_pred[key]):\n",
        "                return y_pred[key]\n",
        "        # если не нашли — попробуем fallback: первый тензор в dict\n",
        "        for v in y_pred.values():\n",
        "            if torch.is_tensor(v):\n",
        "                return v\n",
        "        raise ValueError(\"Could not extract prediction tensor from dict y_pred.\")\n",
        "\n",
        "    if isinstance(y_pred, (list, tuple)):\n",
        "        # обычно y_pred[0] — предсказание\n",
        "        return y_pred[0]\n",
        "\n",
        "    if torch.is_tensor(y_pred):\n",
        "        return y_pred\n",
        "\n",
        "    raise TypeError(f\"Unsupported y_pred type: {type(y_pred)}\")\n",
        "\n",
        "\n",
        "class EventTimeSeriesSplit(BaseCrossValidator):\n",
        "    \"\"\"\n",
        "    Кросс-валидация по событиям (batch), хронологическая, с эмбарго.\n",
        "    groups: массив той же длины, что и df, со значениями batch\n",
        "    times:  массив pd.Timestamp (или sortable), та же длина, что и df\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_splits: int = 5, embargo_events: int = 1, min_train_events: int = 5):\n",
        "        self.n_splits = n_splits\n",
        "        self.embargo_events = embargo_events\n",
        "        self.min_train_events = min_train_events\n",
        "\n",
        "    def get_n_splits(self, X=None, y=None, groups=None):\n",
        "        return self.n_splits\n",
        "\n",
        "    def split(self, X, y=None, groups=None, times: Optional[pd.Series] = None) -> Iterator[\n",
        "        Tuple[np.ndarray, np.ndarray]]:\n",
        "        if groups is None or times is None:\n",
        "            raise ValueError(\"Pass groups=batch and times=time columns\")\n",
        "\n",
        "        groups = np.asarray(groups)\n",
        "        times = pd.to_datetime(times)\n",
        "\n",
        "        # порядок событий по старт-времени\n",
        "        df_tmp = pd.DataFrame({\"group\": groups, \"time\": times}).reset_index(names=\"row_idx\")\n",
        "        first_time = df_tmp.groupby(\"group\")[\"time\"].min().sort_values()\n",
        "        uniq_groups = first_time.index.to_numpy()\n",
        "\n",
        "        n_events = len(uniq_groups)\n",
        "        if n_events < (self.n_splits + self.min_train_events):\n",
        "            # уменьшаем число сплитов, если событий мало\n",
        "            eff_splits = max(1, n_events - self.min_train_events)\n",
        "        else:\n",
        "            eff_splits = self.n_splits\n",
        "\n",
        "        # на каждой итерации расширяем train вправо\n",
        "        for split_idx in range(1, eff_splits + 1):\n",
        "            # доля событий для валидации\n",
        "            val_events = max(1, n_events // (eff_splits + 1))\n",
        "            train_end = n_events - (eff_splits - split_idx + 1) * val_events\n",
        "\n",
        "            if train_end < self.min_train_events:\n",
        "                continue\n",
        "\n",
        "            # эмбарго\n",
        "            embargoed_end = max(0, train_end - self.embargo_events)\n",
        "\n",
        "            train_groups = uniq_groups[:embargoed_end]\n",
        "            val_groups = uniq_groups[train_end: train_end + val_events]\n",
        "\n",
        "            train_idx = df_tmp.index[df_tmp[\"group\"].isin(train_groups)].to_numpy()\n",
        "            val_idx = df_tmp.index[df_tmp[\"group\"].isin(val_groups)].to_numpy()\n",
        "\n",
        "            # индексы исходной X (если это DataFrame — у вас совпадают позиции с row_idx)\n",
        "            yield (train_idx, val_idx)\n",
        "\n",
        "\n",
        "class MinimalRichProgressBar(RichProgressBar):\n",
        "    def on_validation_start(self, trainer, pl_module):\n",
        "        pass\n",
        "\n",
        "    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        pass\n",
        "\n",
        "\n",
        "class NoValidationBar(TQDMProgressBar):\n",
        "    def init_validation_tqdm(self):\n",
        "        # возвращаем полностью отключённый tqdm для валидации\n",
        "        return tqdm_class(disable=True)\n",
        "\n",
        "class CustomTFT(TemporalFusionTransformer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.mask_prob = kwargs.pop(\"mask_prob\", 0.05)\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._val_preds = []\n",
        "        self._val_trues = []\n",
        "        self._val_gids = []\n",
        "        self.scheduled_prob = 0.0\n",
        "\n",
        "        safe_val = torch.tensor(\n",
        "            torch.finfo(torch.float16).min,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        m = self.multihead_attn.attention\n",
        "        if hasattr(m, \"mask_bias\") and not isinstance(m.mask_bias, torch.Tensor):\n",
        "            delattr(m, \"mask_bias\")\n",
        "        m.register_buffer(\"mask_bias\", safe_val)\n",
        "\n",
        "    def on_epoch_start(self, trainer, pl_module):\n",
        "        if trainer.max_epochs > 0:\n",
        "            self.scheduled_prob = min(1.0, trainer.current_epoch / (trainer.max_epochs * 0.8))\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, weight = _unpack_pf_batch(batch)\n",
        "\n",
        "        if torch.rand(1).item() < self.mask_prob and \"encoder_target\" in x:\n",
        "            enc = x[\"encoder_target\"]\n",
        "            noise = torch.normal(0, 0.15, size=enc.shape, device=enc.device)\n",
        "            x = {**x, \"encoder_target\": noise}\n",
        "            del enc, noise\n",
        "\n",
        "        if torch.rand(1).item() < self.scheduled_prob and y is not None:\n",
        "            with torch.no_grad():\n",
        "                out = self(x)\n",
        "                y_pred = self.loss.to_prediction(out)\n",
        "                y_bt = _collapse_pred_to_bt(y_pred)\n",
        "                dec_tgt = y_bt if y_bt.dim() == 2 else y_bt.unsqueeze(-1)\n",
        "                x['decoder_target'] = dec_tgt.detach()\n",
        "                del out, y_pred, y_bt, dec_tgt\n",
        "\n",
        "        batch = (x, y, weight) if weight is not None else (x, y)\n",
        "        result = super().training_step(batch, batch_idx)\n",
        "\n",
        "        if batch_idx % 50 == 0:  # Rare for speed\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, weight = _unpack_pf_batch(batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_temp = self(x)\n",
        "            y_pred_temp = self.loss.to_prediction(out_temp)\n",
        "            y_bt_temp = _collapse_pred_to_bt(y_pred_temp)\n",
        "            del out_temp, y_pred_temp\n",
        "\n",
        "        if \"decoder_target\" in x:\n",
        "            enc_tgt = x.get(\"encoder_target\")\n",
        "            if enc_tgt is not None:\n",
        "                batch_mean = enc_tgt.mean(dim=-1, keepdim=True)\n",
        "                batch_std = enc_tgt.std(dim=-1, keepdim=True) + 1e-8\n",
        "                mean_tensor = batch_mean.expand_as(x[\"decoder_target\"])\n",
        "                std_tensor = (0.1 * batch_std).expand_as(x[\"decoder_target\"])\n",
        "                noise_fill = torch.normal(mean_tensor, std_tensor)\n",
        "                del batch_mean, std_tensor, mean_tensor\n",
        "            else:\n",
        "                dec_tgt = x[\"decoder_target\"]\n",
        "                device = dec_tgt.device\n",
        "                noise_fill = torch.full_like(dec_tgt, self.global_target_mean)\n",
        "                batch_size = dec_tgt.size(0)\n",
        "                batch_std = torch.full((batch_size,), self.global_target_std, device=device).unsqueeze(-1)\n",
        "\n",
        "            if torch.rand(1).item() < self.scheduled_prob:\n",
        "                decoder_fill = y_bt_temp\n",
        "            else:\n",
        "                jitter_size = noise_fill.shape\n",
        "                additional_jitter = torch.randn(jitter_size, device=noise_fill.device) * (0.05 * batch_std.expand_as(noise_fill))\n",
        "                decoder_fill = noise_fill + additional_jitter\n",
        "                del additional_jitter, noise_fill\n",
        "\n",
        "            decoder_fill = torch.clamp(decoder_fill, -1.0, 1.0)\n",
        "            x[\"decoder_target\"] = decoder_fill\n",
        "            del y_bt_temp, batch_std\n",
        "\n",
        "        out = self(x)\n",
        "        y_pred_raw = self.loss.to_prediction(out)\n",
        "        del out\n",
        "\n",
        "        y_pred_metrics = torch.clamp(y_pred_raw, -1.0, 1.0)\n",
        "        del y_pred_raw\n",
        "\n",
        "        y_actual = y if y is not None else x.get(\"decoder_target\")\n",
        "        if y_actual is None:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            y_pred_aligned, y_actual_aligned = _align_pred_target(y_pred_metrics, y_actual)\n",
        "            del y_pred_metrics, y_actual\n",
        "        except Exception:\n",
        "            return\n",
        "\n",
        "        self._val_preds.append(y_pred_aligned.detach())\n",
        "        self._val_trues.append(y_actual_aligned.detach())\n",
        "        del y_pred_aligned, y_actual_aligned\n",
        "\n",
        "        gid = x.get(\"group_ids\")\n",
        "        if gid is not None and isinstance(gid, torch.Tensor):\n",
        "            self._val_gids.append(gid.detach())\n",
        "        else:\n",
        "            self._val_gids.append(None)\n",
        "\n",
        "        if batch_idx % 50 == 0:  # Rare for speed\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if len(self._val_preds) == 0:\n",
        "            return\n",
        "\n",
        "        yp = torch.cat(self._val_preds, dim=0)\n",
        "        yt = torch.cat(self._val_trues, dim=0)\n",
        "\n",
        "        self._val_preds.clear()\n",
        "        self._val_trues.clear()\n",
        "\n",
        "        se = (yp - yt) ** 2\n",
        "        val_mse = float(se.mean().item())\n",
        "        val_mse_std = float(se.std(unbiased=False).item())\n",
        "        del yp, yt\n",
        "\n",
        "        self.log(\"val_mse\", val_mse, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log(\"val_mse_std\", val_mse_std, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        has_any_gid = any(g is not None for g in self._val_gids)\n",
        "        if has_any_gid:\n",
        "            gid_list = []\n",
        "            valid = True\n",
        "            for g in self._val_gids:\n",
        "                if g is None:\n",
        "                    valid = False\n",
        "                    break\n",
        "                gid_list.append(g)\n",
        "\n",
        "            if valid and len(gid_list) > 0:\n",
        "                gid_all = torch.cat(gid_list, dim=0).numpy().ravel()\n",
        "                se_np = se.numpy().ravel()\n",
        "                del se\n",
        "\n",
        "                if gid_all.shape[0] == se_np.shape[0]:\n",
        "                    uniq = np.unique(gid_all)\n",
        "                    g_mse = [se_np[gid_all == u].mean() for u in uniq if (gid_all == u).any()]\n",
        "                    if len(g_mse) > 0:\n",
        "                        g_mse = np.asarray(g_mse, dtype=float)\n",
        "                        val_mse_group_mean = float(g_mse.mean())\n",
        "                        val_mse_group_std = float(g_mse.std(ddof=0))\n",
        "                        self.log(\"val_mse_group_mean\", val_mse_group_mean, prog_bar=False, on_step=False, on_epoch=True)\n",
        "                        self.log(\"val_mse_group_std\", val_mse_group_std, prog_bar=True, on_step=False, on_epoch=True)\n",
        "                    del g_mse, uniq\n",
        "\n",
        "                del gid_all, se_np\n",
        "\n",
        "        self._val_gids.clear()\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def log(self, name, value, *args, **kwargs):\n",
        "        if value is None:\n",
        "            return\n",
        "        super().log(name, value, *args, **kwargs)\n",
        "\n",
        "\n",
        "def _worker_init_fn(worker_id: int, seed: int):\n",
        "    \"\"\"\n",
        "    Глобальная функция для инициализации worker-а DataLoader-а.\n",
        "    pickle её «видит» и может передать в подпроцессы.\n",
        "    \"\"\"\n",
        "    set_seeds(seed + worker_id)\n",
        "\n",
        "\n",
        "def _extract_tensor(x, role=\"pred\"):\n",
        "    \"\"\"\n",
        "    Извлекает torch.Tensor из различных контейнеров/структур.\n",
        "    - dict: сперва пробуем ключи, характерные для предсказаний/таргета\n",
        "    - tuple/list: берём первый тензор или первый элемент, приводимый к тензору\n",
        "    - tensor: возвращаем как есть\n",
        "    \"\"\"\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "\n",
        "    if isinstance(x, dict):\n",
        "        # Наиболее типичные ключи в pytorch-forecasting / lightning шагах\n",
        "        preferred_keys = [\n",
        "            \"prediction\", \"pred\", \"output\", \"y_pred\", \"yhat\", \"y\", \"target\"\n",
        "        ]\n",
        "        for k in preferred_keys:\n",
        "            if k in x and isinstance(x[k], torch.Tensor):\n",
        "                return x[k]\n",
        "        # Если значения-словари/кортежи — попробуем рекурсивно\n",
        "        for v in x.values():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                return v\n",
        "            if isinstance(v, (list, tuple, dict)):\n",
        "                try:\n",
        "                    t = _extract_tensor(v, role=role)\n",
        "                    if isinstance(t, torch.Tensor):\n",
        "                        return t\n",
        "                except Exception:\n",
        "                    pass\n",
        "        raise TypeError(f\"Cannot extract tensor from dict for role={role}. Keys={list(x.keys())}\")\n",
        "\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        for item in x:\n",
        "            if isinstance(item, torch.Tensor):\n",
        "                return item\n",
        "        # если нет прямого тензора — попробуем рекурсивно\n",
        "        for item in x:\n",
        "            if isinstance(item, (list, tuple, dict)):\n",
        "                try:\n",
        "                    t = _extract_tensor(item, role=role)\n",
        "                    if isinstance(t, torch.Tensor):\n",
        "                        return t\n",
        "                except Exception:\n",
        "                    pass\n",
        "        raise TypeError(f\"Cannot extract tensor from {type(x)} for role={role}\")\n",
        "\n",
        "    # Последняя попытка — у объектов некоторых библиотек есть .values или .tensor\n",
        "    for attr in (\"values\", \"tensor\", \"data\"):\n",
        "        if hasattr(x, attr):\n",
        "            v = getattr(x, attr)\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                return v\n",
        "\n",
        "    raise TypeError(f\"Unsupported type for tensor extraction (role={role}): {type(x)}\")\n",
        "\n",
        "\n",
        "def _maybe_squeeze_last(x):\n",
        "    \"\"\"\n",
        "    Безопасно убираем последнюю размерность, если она равна 1.\n",
        "    Если x не тензор — возвращаем как есть.\n",
        "    \"\"\"\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    if x.dim() > 0 and x.size(-1) == 1:\n",
        "        return x.squeeze(-1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def _align_pred_target(y_pred, y_actual):\n",
        "    \"\"\"\n",
        "    Приводим предсказания и таргет к совместимым формам для MSE:\n",
        "    - Извлекаем тензоры из возможных контейнеров.\n",
        "    - Сводим предсказание к (B, T_pred) с последней осью как временем.\n",
        "    - Таргет сводим к (B,) или (B, T_act).\n",
        "    - Если таргет (B,) — берём последний горизонт из предсказаний.\n",
        "    - Если таргет (B, T_act) — подгоняем по времени (обрезаем/проверяем равенство).\n",
        "    \"\"\"\n",
        "    # 1) Достаём тензоры\n",
        "    y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "    y_actual = _extract_tensor(y_actual, role=\"target\")\n",
        "\n",
        "    # 2) Сжимаем последнюю единичную ось\n",
        "    y_pred = _maybe_squeeze_last(y_pred)\n",
        "    y_actual = _maybe_squeeze_last(y_actual)\n",
        "\n",
        "    # Быстрый путь: формы совпали\n",
        "    if isinstance(y_pred, torch.Tensor) and isinstance(y_actual, torch.Tensor):\n",
        "        if y_pred.shape == y_actual.shape:\n",
        "            return y_pred, y_actual\n",
        "\n",
        "    # 3) Приводим предсказание к (B, T_pred)\n",
        "    y_pred_bt = _collapse_pred_to_bt(y_pred)  # (B, T_pred)\n",
        "\n",
        "    # 4) Приведём таргет к (B,) или (B, T_act)\n",
        "    if y_actual.dim() == 1:\n",
        "        # (B,) — ожидаем 1 шаг на таргет → берём последний горизонт из предсказаний\n",
        "        if y_pred_bt.dim() != 2 or y_pred_bt.size(0) != y_actual.size(0):\n",
        "            raise ValueError(f\"Batch mismatch: pred={tuple(y_pred_bt.shape)} vs target={tuple(y_actual.shape)}\")\n",
        "        y_pred_aligned = y_pred_bt[:, -1]  # последний шаг горизонта\n",
        "        return y_pred_aligned, y_actual\n",
        "\n",
        "    if y_actual.dim() == 2:\n",
        "        # (B, T_act)\n",
        "        if y_pred_bt.size(0) != y_actual.size(0):\n",
        "            raise ValueError(f\"Batch mismatch: pred={tuple(y_pred_bt.shape)} vs target={tuple(y_actual.shape)}\")\n",
        "        T_pred = y_pred_bt.size(1)\n",
        "        T_act = y_actual.size(1)\n",
        "        if T_pred == T_act:\n",
        "            return y_pred_bt, y_actual\n",
        "        if T_pred > T_act:\n",
        "            # Обрежем последние T_act шагов, чтобы соответствовать таргету\n",
        "            y_pred_bt = y_pred_bt[:, -T_act:]\n",
        "            return y_pred_bt, y_actual\n",
        "        # Если предсказаний по времени меньше, чем в таргете — это логическая ошибка настройки\n",
        "        raise ValueError(\n",
        "            f\"Prediction horizon shorter than target: pred T={T_pred}, target T={T_act} \"\n",
        "            f\"(pred shape={tuple(y_pred_bt.shape)}, target shape={tuple(y_actual.shape)})\"\n",
        "        )\n",
        "\n",
        "    # Случай редкий: если таргет внезапно >2D — пробуем схлопнуть по всем, кроме батча\n",
        "    if y_actual.dim() > 2:\n",
        "        # Схлопнём таргет к (B, T_act) по последней оси\n",
        "        reduce_dims = tuple(range(1, y_actual.dim() - 1))\n",
        "        if len(reduce_dims) > 0:\n",
        "            y_actual_bt = y_actual.mean(dim=reduce_dims)\n",
        "        else:\n",
        "            y_actual_bt = y_actual\n",
        "        # Рекурсивно выровняем теперь как (B, ?)\n",
        "        return _align_pred_target(y_pred_bt, y_actual_bt)\n",
        "\n",
        "    # Если таргет скалярный (редко, но вдруг), расширим до (B,) повтором\n",
        "    if y_actual.dim() == 0:\n",
        "        y_actual = y_actual.expand(y_pred_bt.size(0))\n",
        "        y_pred_aligned = y_pred_bt[:, -1]\n",
        "        return y_pred_aligned, y_actual\n",
        "\n",
        "    # Если сюда дошли — что-то совсем нетипичное\n",
        "    raise ValueError(\n",
        "        f\"Shapes still mismatch after alignment: pred={tuple(y_pred.shape)} vs target={tuple(y_actual.shape)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Фиксируем seeds для воспроизводимости и стабильности\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    pl.seed_everything(seed, verbose=False)\n",
        "\n",
        "\n",
        "class PeakFriendlyHuber(Metric):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        delta: float = 0.5,\n",
        "        peak_thr: float = 0.85,\n",
        "        peak_weight: float = 1.6,  # Увеличено до 1.6 для stronger поощрения пиков\n",
        "        contrast_weight: float = 0.02,\n",
        "        center_band: float = 0.3,\n",
        "        clip_scale: float = 1.5,  # Новый: scale для soft-clip (tanh * scale, чтобы не сжимать середину сильно)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.delta = float(delta)\n",
        "        self.peak_thr = float(peak_thr)\n",
        "        self.peak_weight = float(peak_weight)\n",
        "        self.contrast_weight = float(contrast_weight)\n",
        "        self.center_band = float(center_band)\n",
        "        self.clip_scale = float(clip_scale)  # Новый параметр\n",
        "        self.mse = MeanSquaredError()\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth_l1(diff, delta):\n",
        "        absd = diff.abs()\n",
        "        return torch.where(absd < delta, 0.5 * (diff ** 2) / delta, absd - 0.5 * delta)\n",
        "\n",
        "    def to_prediction(self, y_pred):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        return super().to_prediction(y_pred)\n",
        "\n",
        "    def to_quantiles(self, y_pred, quantiles=None, **kwargs):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        return super().to_quantiles(y_pred, quantiles=quantiles, **kwargs)\n",
        "\n",
        "    def loss(self, y_pred, y_actual, **kwargs):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        y_actual = _extract_tensor(y_actual, role=\"target\")\n",
        "        y_pred, y_actual = _align_pred_target(y_pred, y_actual)\n",
        "\n",
        "        # Soft-clip (без изменений)\n",
        "        y_pred = torch.tanh(y_pred * self.clip_scale) / self.clip_scale\n",
        "\n",
        "        diff = y_pred - y_actual\n",
        "        base = self._smooth_l1(diff, self.delta)\n",
        "\n",
        "        # Адаптивный peak_weight: средний по батчу, scale от доли пиков\n",
        "        if self.peak_weight > 1.0:\n",
        "            with torch.no_grad():\n",
        "                peak_mag = torch.relu(y_actual.abs() - self.peak_thr)\n",
        "                peak_frac = (peak_mag > 0).float().mean()  # Доля пиков в батче\n",
        "                adaptive_weight = 1.0 + (self.peak_weight - 1.0) * peak_frac  # Больше веса, если много пиков\n",
        "                w = adaptive_weight * torch.clamp(peak_mag / (1.0 - self.peak_thr + 1e-8), 0.0, 1.0) + 1.0\n",
        "            huber_term = (base * w).mean()\n",
        "        else:\n",
        "            huber_term = base.mean()\n",
        "\n",
        "        # Лёгкий «anti-flatness» у центра: штрафим чрезмерно малую амплитуду,\n",
        "        # но только там, где таргет далеко от 0.\n",
        "        if self.contrast_weight > 0.0:\n",
        "            with torch.no_grad():\n",
        "                far_mask = (y_actual.abs() >= self.center_band).float()\n",
        "                near_mask = (y_actual.abs() < self.center_band).float()\n",
        "\n",
        "            # Прямая амплитуда предсказания\n",
        "            far_amp = (y_pred.abs() * far_mask).sum() / (far_mask.sum() + 1e-8)\n",
        "            near_amp = (y_pred.abs() * near_mask).sum() / (near_mask.sum() + 1e-8)\n",
        "\n",
        "            # Хотим far_amp >= near_amp + margin; введём небольшой margin\n",
        "            margin = 0.05\n",
        "            contrast = torch.relu((near_amp + margin) - far_amp)\n",
        "            loss = huber_term + self.contrast_weight * contrast\n",
        "        else:\n",
        "            loss = huber_term\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __call__(self, y_pred, y_actual, **kwargs):\n",
        "        return self.loss(y_pred, y_actual, **kwargs)\n",
        "\n",
        "    def update(self, y_pred, y_actual, **kwargs):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        y_actual = _extract_tensor(y_actual, role=\"target\")\n",
        "        y_pred, y_actual = _align_pred_target(y_pred, y_actual)\n",
        "        self.mse.update(y_pred, y_actual)\n",
        "\n",
        "    def compute(self):\n",
        "        return self.mse.compute()\n",
        "\n",
        "    def reset(self):\n",
        "        self.mse.reset()\n",
        "\n",
        "    def name(self):\n",
        "        return \"PeakFriendlyHuber\"\n",
        "\n",
        "class TFTAdapter(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Обёртка над TemporalFusionTransformer,\n",
        "    чтобы Trainer воспринимал модель нужного типа\n",
        "    и наш tft.training_step видел непустой self.trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tft: TemporalFusionTransformer):\n",
        "        super().__init__()\n",
        "        self.tft = tft\n",
        "\n",
        "    def on_fit_start(self) -> None:\n",
        "        # вызовется перед стартом Trainer.fit\n",
        "        # прикрепляем Trainer к внутреннему tft\n",
        "        self.tft.trainer = self.trainer\n",
        "        # и логгеры\n",
        "        self.tft.log = self.log\n",
        "        self.tft.log_dict = self.log_dict\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.tft(*args, **kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.tft.trainer = self.trainer\n",
        "        result = self.tft.training_step(batch, batch_idx)\n",
        "        if isinstance(result, dict):\n",
        "            return result.get(\"loss\")\n",
        "        elif isinstance(result, tuple) and len(result) >= 2:\n",
        "            log_dict, out = result[:2]  # Берем первые два элемента\n",
        "            if isinstance(log_dict, dict):\n",
        "                return log_dict.get(\"loss\")\n",
        "        # Если ни один вариант не подошел\n",
        "        raise ValueError(f\"Unexpected return type from tft.training_step: {type(result)}\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # обеспечим корректную ссылку на тренер внутри tft (если нужно)\n",
        "        self.tft.trainer = self.trainer\n",
        "        self.tft.validation_step(batch, batch_idx)\n",
        "        return\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        # Просто передаём вызов внутреннему TFT, без dataloader_idx (не нужен для TFT)\n",
        "        return self.tft.predict_step(batch, batch_idx)\n",
        "\n",
        "    def predict(self, *args, **kwargs):\n",
        "        return self.tft.predict(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.tft.configure_optimizers()\n",
        "\n",
        "\n",
        "def to_dense(X):\n",
        "    \"\"\"Преобразование sparse matrix в dense numpy array\"\"\"\n",
        "    if hasattr(X, 'toarray'):\n",
        "        return X.toarray()\n",
        "    return np.asarray(X)\n",
        "\n",
        "def prepare_data_transformer(df, target_col):\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    if 'time' in df.columns and 'time_idx' not in df.columns:\n",
        "        df = df.rename(columns={'time': 'time_idx'})\n",
        "    X = df.drop(columns=target_col)\n",
        "    y = df[target_col].copy()\n",
        "\n",
        "    exclude = ['time_idx', 'batch']\n",
        "    numeric_features = [\n",
        "        c for c in X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
        "        if c not in exclude\n",
        "    ]\n",
        "    categorical_features = [\n",
        "        c for c in X.select_dtypes(include=['object', 'category']).columns\n",
        "        if c not in exclude\n",
        "    ]\n",
        "\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('yeo', PowerTransformer(method='yeo-johnson'))\n",
        "            ]), numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "        ],\n",
        "        remainder='passthrough'  # passthrough → тут окажутся сначала все num→cat, а потом time_idx и batch\n",
        "    )\n",
        "    # Возвращаем дополнительные списки для передачи в модель\n",
        "    return X, y, preprocessing, numeric_features, categorical_features\n",
        "\n",
        "def split_features_batch_time(X_array, n_transformed):\n",
        "    \"\"\"\n",
        "    X_array: np.ndarray после преобразований shape=(N, n_transformed + 2)\n",
        "    n_transformed: сколько колонок ушло на num+cat\n",
        "    возвращает (features, time_raw, batch_raw)\n",
        "    \"\"\"\n",
        "    features = X_array[:, :n_transformed]\n",
        "    batch_raw = X_array[:, n_transformed].ravel()\n",
        "    time_raw = X_array[:, n_transformed + 1].ravel()\n",
        "    return features, batch_raw, time_raw\n",
        "\n",
        "\n",
        "def tft_output_transformer(x):\n",
        "    # Больше НЕ клипуем внутри графа. Пусть модель учится выходить за [-1,1],\n",
        "    # а мы ограничим при расчёте метрик и при возврате пользователю.\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_early_stopping_callback(patience=10, min_delta=0.001):\n",
        "    return EarlyStopping(\n",
        "        monitor=\"train_loss\",\n",
        "        patience=patience,\n",
        "        min_delta=min_delta,\n",
        "        mode=\"min\",\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "\n",
        "class SequenceTransformerRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    Temporal Fusion Transformer для последовательностей.\n",
        "    Интегрируется в Pipeline аналогично LSTM.\n",
        "    Адаптировано для стабильного обучения и алготрейдинга (реального времени).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 seq_len: int = 10,\n",
        "                 pred_len: int = 1,\n",
        "                 hidden_size: int = 64,  # Увеличено для лучшей емкости\n",
        "                 hidden_continuous_size: int = 24,\n",
        "                 epochs: int = 100,  # Увеличено для более долгого обучения\n",
        "                 batch_size: int = 128,  # Увеличено для стабильности\n",
        "                 learning_rate: float = 1e-3,  # Увеличено для более быстрого старта\n",
        "                 patience: int = 15,  # Увеличено для терпимости\n",
        "                 seed: int = 42,\n",
        "                 dropout: float = 0.25,  # Уменьшено для меньшей регуляризации\n",
        "                 weight_decay: float = 1e-4,  # Новый: для регуляризации\n",
        "                 verbose: int = 2,\n",
        "                 mask_prob: float = 0.1,  # Уменьшено, чтобы меньше шумить\n",
        "                 infer_stride: int = 2,\n",
        "                 ckpt_path=None,\n",
        "                 preprocessing=None,\n",
        "                 numeric_features=None,\n",
        "                 categorical_features=None,\n",
        "                 remainder_columns=None,\n",
        "                 min_encoder_length: int = 1):  # Больше логов\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_continuous_size = hidden_continuous_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.patience = patience\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "        self._model = None\n",
        "        self._trainer = None\n",
        "        self._n_transformed = None\n",
        "        self._n_feat = None\n",
        "        self._train_dataset = None\n",
        "        self._feature_columns = None\n",
        "        self._dropout = dropout\n",
        "        self.norm_eps = 1e-6\n",
        "        self.norm_window = max(10, self.seq_len // 2)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.weight_decay = weight_decay  # Новый\n",
        "        self.mask_prob = mask_prob\n",
        "        self.infer_stride = infer_stride\n",
        "        self.global_target_mean = 0.0  # Будем вычислять в fit\n",
        "        self.global_target_std = 1.0  # Fallback global scale\n",
        "        self.global_target_min = None\n",
        "        self.global_target_max = None\n",
        "        self.global_range = None\n",
        "        self.soft_clip_scale =  None\n",
        "        self.preprocessing = preprocessing\n",
        "        self.numeric_features = numeric_features or []\n",
        "        self.categorical_features = categorical_features or []\n",
        "        self.remainder_columns = remainder_columns or ['batch', 'time']\n",
        "        self.use_tanh_post = False\n",
        "        self.train_q_lo = None\n",
        "        self.train_q_hi = None\n",
        "        self.clip_scale = 1.5\n",
        "        self.future_fill_mode = \"repeat\"\n",
        "        self.smooth_window = max(5, self.seq_len // 5)  # For savgol\n",
        "        self.smooth_poly = 2\n",
        "        self.ema_alpha = 0.1\n",
        "        self.infer_batch_size = 512  # Новый: большой батч для inference\n",
        "        self.infer_stride = infer_stride if infer_stride is not None else 4\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'# Добавьте это, если нужно tanh в предикте\n",
        "        self.min_encoder_length = max(1, min_encoder_length)  # Не меньше 1\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        set_seeds(self.seed)\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        if self._n_transformed is None:\n",
        "            self._n_transformed = X.shape[1] - 2\n",
        "            self._n_feat = X.shape[1]\n",
        "\n",
        "        feat, batch_raw, time_raw = split_features_batch_time(X, self._n_transformed)\n",
        "\n",
        "        df = pd.DataFrame(feat, columns=[f\"f{i}\" for i in range(feat.shape[1])])\n",
        "        self._feature_columns = df.columns.tolist()\n",
        "\n",
        "        df[\"batch\"] = pd.Series(batch_raw).astype(\"int64\")\n",
        "        df[\"time_raw\"] = pd.to_datetime(time_raw, utc=True, errors=\"coerce\")\n",
        "\n",
        "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
        "            y = y.reset_index(drop=True)\n",
        "        df[\"target\"] = pd.Series(y, index=df.index).astype(float)\n",
        "\n",
        "        before = len(df)\n",
        "        df = df.dropna(subset=[\"time_raw\", \"target\"]).reset_index(drop=True)\n",
        "        if self.verbose and len(df) < before:\n",
        "            print(f\"Dropped {before - len(df)} rows with invalid time/target\")\n",
        "\n",
        "        df = df.sort_values([\"batch\", \"time_raw\"]).reset_index(drop=True)\n",
        "        df[\"time_idx\"] = df.groupby(\"batch\").cumcount()\n",
        "\n",
        "        min_len = max(1, int(self.pred_len))  # Изменено: позволяем короткие батчи (encoder может быть < seq_len)\n",
        "        gsize = df.groupby(\"batch\").size()\n",
        "        valid_batches = gsize[gsize >= min_len].index\n",
        "        if len(valid_batches) == 0:\n",
        "            raise ValueError(f\"No batches with length >= {min_len}. Reduce pred_len.\")\n",
        "        if self.verbose and len(valid_batches) < gsize.index.nunique():\n",
        "            dropped = sorted(list(set(gsize.index) - set(valid_batches)))\n",
        "            print(f\"Warning: dropped {len(dropped)} short batches: {dropped[:8]}{' ...' if len(dropped) > 8 else ''}\")\n",
        "        df = df[df[\"batch\"].isin(valid_batches)].reset_index(drop=True)\n",
        "\n",
        "        batch_starts = df.groupby(\"batch\")[\"time_raw\"].min().sort_values()\n",
        "        uniq_batches = batch_starts.index.to_numpy()\n",
        "        n_total = len(uniq_batches)\n",
        "        val_frac = 0.3 if n_total >= 10 else 0.1  # Увеличено для лучшего обобщения\n",
        "        n_val = max(1, int(round(n_total * val_frac)))\n",
        "\n",
        "        embargo = 1 if n_total >= 8 else 0\n",
        "\n",
        "        train_end = max(0, n_total - n_val - embargo)\n",
        "        train_batches = uniq_batches[:train_end]\n",
        "        val_batches = uniq_batches[-n_val:]\n",
        "\n",
        "        if len(train_batches) == 0 and n_total > 1:\n",
        "            train_batches = uniq_batches[:-1]\n",
        "            val_batches = uniq_batches[-1:]\n",
        "\n",
        "        train_df = df[df[\"batch\"].isin(train_batches)].copy()\n",
        "        val_df = df[df[\"batch\"].isin(val_batches)].copy()\n",
        "\n",
        "        self.global_target_mean = train_df[\"target\"].mean()\n",
        "        self.global_target_std = train_df[\"target\"].std() + 1e-8\n",
        "        self.global_target_min = train_df[\"target\"].min()\n",
        "        self.global_target_max = train_df[\"target\"].max()\n",
        "        self.global_range = self.global_target_max - self.global_target_min + 1e-8\n",
        "        self.soft_clip_scale = max(1.0, self.global_target_std * 1.5)  # Adaptive soft clip for stable [-1,1] without compression\n",
        "\n",
        "        #self.clip_scale = max(1.0, self.global_target_std * 1.2)   # Adaptive to train variance\n",
        "\n",
        "        train_targets = train_df[\"target\"].values\n",
        "        #if len(train_targets) > 0:\n",
        "        #   self.train_q_lo, self.train_q_hi = np.quantile(train_targets, [0.01, 0.99])\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Train batches: {len(np.unique(train_batches))}\")\n",
        "            print(f\"Val batches: {len(np.unique(val_batches))}\")\n",
        "            print(f\"Train rows: {len(train_df)}, Val rows: {len(val_df)}\")\n",
        "\n",
        "        full_dataset = TimeSeriesDataSet(\n",
        "            df,\n",
        "            time_idx=\"time_idx\",\n",
        "            target=\"target\",\n",
        "            group_ids=[\"batch\"],\n",
        "            max_encoder_length=int(self.seq_len),\n",
        "            min_encoder_length=0,#int(self.min_encoder_length),  # Новый: позволяем короткие encoder\n",
        "            max_prediction_length=int(self.pred_len),\n",
        "            time_varying_unknown_reals=self._feature_columns,\n",
        "            target_normalizer=None,\n",
        "            allow_missing_timesteps=True,\n",
        "            add_relative_time_idx=True,\n",
        "            add_target_scales=False,\n",
        "            add_encoder_length=True,\n",
        "            min_prediction_length=1,\n",
        "        )\n",
        "\n",
        "        train_dataset = TimeSeriesDataSet.from_dataset(\n",
        "            full_dataset, train_df, predict=False, stop_randomization=True\n",
        "        )\n",
        "        val_dataset = TimeSeriesDataSet.from_dataset(\n",
        "            full_dataset, val_df, predict=False, stop_randomization=True\n",
        "        ) if len(val_df) > 0 else None\n",
        "\n",
        "        train_dl = train_dataset.to_dataloader(\n",
        "            train=True,\n",
        "            batch_size=int(self.batch_size),\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            worker_init_fn=None,\n",
        "            drop_last=False,\n",
        "            persistent_workers=False,\n",
        "        )\n",
        "        val_dl = None\n",
        "        if val_dataset is not None and len(val_dataset) > 0:\n",
        "            val_dl = val_dataset.to_dataloader(\n",
        "                train=False,\n",
        "                batch_size=int(self.batch_size),\n",
        "                shuffle=False,\n",
        "                num_workers=0,\n",
        "                worker_init_fn=None,\n",
        "                drop_last=False,\n",
        "                persistent_workers=False,\n",
        "            )\n",
        "\n",
        "        tft = CustomTFT.from_dataset(\n",
        "            train_dataset,\n",
        "            hidden_size=int(self.hidden_size),\n",
        "            output_size=1,\n",
        "            loss=PeakFriendlyHuber(\n",
        "                delta=0.5,\n",
        "                peak_thr=0.85,  # можно затем подвинуть 0.8..0.9\n",
        "                peak_weight=1.3,  # аккуратно: 1.3..1.6\n",
        "                contrast_weight=0.03,  # очень маленькая добавка\n",
        "                center_band=0.3,  # что считать «центром»\n",
        "                clip_scale=1.5\n",
        "            ),\n",
        "            optimizer=\"adam\",\n",
        "            learning_rate=float(self.learning_rate),  # оставьте тот, на котором MSE был лучше (у вас 1e-4 давал ~0.186)\n",
        "            lstm_layers=3,\n",
        "            hidden_continuous_size=self.hidden_continuous_size,\n",
        "            attention_head_size=4,\n",
        "            dropout=float(self._dropout),\n",
        "            reduce_on_plateau_patience=5,\n",
        "            reduce_on_plateau_min_lr=1e-6,\n",
        "            weight_decay=float(self.weight_decay),\n",
        "            mask_prob=float(self.mask_prob),\n",
        "            output_transformer=tft_output_transformer,\n",
        "        )\n",
        "\n",
        "        class GCCallback(pl.Callback):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.last_mem = psutil.Process().memory_info().rss / 1e6\n",
        "\n",
        "            def _check_gc(self):\n",
        "                current_mem = psutil.Process().memory_info().rss / 1e6\n",
        "                if current_mem - self.last_mem > 50:\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                self.last_mem = current_mem\n",
        "\n",
        "            def on_train_epoch_end(self, trainer, pl_module):\n",
        "                self._check_gc()\n",
        "\n",
        "            def on_validation_epoch_end(self, trainer, pl_module):\n",
        "                self._check_gc()\n",
        "\n",
        "            def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "                if batch_idx % 10 == 0:\n",
        "                    self._check_gc()\n",
        "\n",
        "            def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "                if batch_idx % 10 == 0:\n",
        "                    self._check_gc()\n",
        "\n",
        "        callbacks = []\n",
        "        if val_dl is not None:\n",
        "            callbacks.append(\n",
        "                get_early_stopping_callback(patience=self.patience, min_delta=1e-3))  # Новый: больше patience\n",
        "            checkpoint_callback = ModelCheckpoint(monitor=\"train_loss\", mode=\"min\", save_top_k=1, verbose=True)\n",
        "            callbacks.append(checkpoint_callback)\n",
        "        else:\n",
        "            checkpoint_callback = None\n",
        "\n",
        "        logger = TensorBoardLogger(save_dir=\"lightning_logs/\", name=\"my_model\") if self.verbose > 0 else False\n",
        "        if self.verbose > 0:\n",
        "            callbacks.append(LearningRateMonitor(logging_interval=\"step\"))\n",
        "            callbacks.append(NoValidationBar(refresh_rate=20))\n",
        "        callbacks.append(GCCallback())\n",
        "\n",
        "        self._model = TFTAdapter(tft)\n",
        "        self._trainer = pl.Trainer(\n",
        "            max_epochs=int(self.epochs),\n",
        "            enable_checkpointing=(checkpoint_callback is not None),\n",
        "            callbacks=callbacks,\n",
        "            logger=logger,\n",
        "            enable_model_summary=True,\n",
        "            gradient_clip_val=1.0,\n",
        "            gradient_clip_algorithm=\"norm\",\n",
        "            deterministic=True,\n",
        "            benchmark=False,\n",
        "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "            precision=32,\n",
        "            limit_val_batches=1.0 if val_dl is not None else 0.0,\n",
        "            enable_progress_bar=self.verbose > 0,\n",
        "            log_every_n_steps=50,\n",
        "            num_sanity_val_steps=0,\n",
        "        )\n",
        "\n",
        "        self._trainer.fit(\n",
        "            self._model,\n",
        "            train_dataloaders=train_dl,\n",
        "            val_dataloaders=val_dl if val_dl is not None else None,\n",
        "            ckpt_path=self.ckpt_path if self.ckpt_path and self.epochs > 0 else None,\n",
        "        )\n",
        "\n",
        "        if checkpoint_callback is not None and checkpoint_callback.best_model_path:\n",
        "            best_path = checkpoint_callback.best_model_path\n",
        "            if self.verbose:\n",
        "                print(f\"Loaded best model from {best_path} with val_loss={checkpoint_callback.best_model_score}\")\n",
        "            self._model = TFTAdapter.load_from_checkpoint(best_path, tft=tft)\n",
        "\n",
        "        self._train_dataset = full_dataset\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        #return self\n",
        "        try:\n",
        "            y_train = train_df[\"target\"].to_numpy(dtype=float)\n",
        "            # robust percentiles — перестрахуемся от выносов: 1% и 99%\n",
        "            self._cal_p_low = float(np.nanpercentile(y_train, 1))\n",
        "            self._cal_p_high = float(np.nanpercentile(y_train, 99))\n",
        "            # амплитуды «типичных пиков»\n",
        "            top_mask = y_train >= self._cal_p_high\n",
        "            bot_mask = y_train <= self._cal_p_low\n",
        "            self._cal_mean_top = float(np.nanmean(y_train[top_mask])) if np.any(top_mask) else float(self.global_target_max)\n",
        "            self._cal_mean_bot = float(np.nanmean(y_train[bot_mask])) if np.any(bot_mask) else float(self.global_target_min)\n",
        "            # защита от вырождения\n",
        "            if not np.isfinite(self._cal_mean_top): self._cal_mean_top = float(self.global_target_max)\n",
        "            if not np.isfinite(self._cal_mean_bot): self._cal_mean_bot = float(self.global_target_min)\n",
        "        except Exception:\n",
        "            # безопасные фолбэки\n",
        "            self._cal_p_low, self._cal_p_high = self.global_target_min, self.global_target_max\n",
        "            self._cal_mean_top, self._cal_mean_bot = self.global_target_max, self.global_target_min\n",
        "        # ----------------------------------------------\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self._train_dataset is None or self._model is None:\n",
        "            raise RuntimeError(\"Model is not fitted yet\")\n",
        "\n",
        "        # Full suppress context (no logs/output, including PL/Torch/PF/Seed/GPU/TPU/HPU messages)\n",
        "        @contextlib.contextmanager\n",
        "        def suppress_all():\n",
        "            with open(os.devnull, \"w\") as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n",
        "                # Подавление всех возможных логгеров\n",
        "                root_logger = logging.getLogger()\n",
        "                old_level = root_logger.level\n",
        "                root_logger.setLevel(logging.CRITICAL + 1)  # Выше CRITICAL, чтобы ничего не логировалось\n",
        "\n",
        "                # Специфические логгеры (расширенный список)\n",
        "                for logger_name in [\n",
        "                    \"pytorch_lightning\", \"lightning.pytorch\", \"lightning\",\n",
        "                    \"torch\", \"pytorch_forecasting\", \"optuna\",\n",
        "                    \"sklearn\", \"joblib\", \"numpy\", \"pandas\",\n",
        "                    \"scipy\", \"matplotlib\", \"seaborn\", \"plotly\",\n",
        "                    \"shap\", \"statsmodels\", \"torchmetrics\",\n",
        "                    \"lightning.pytorch.utilities.migration.utils\",  # Для Attribute 'loss' warnings\n",
        "                    \"lightning.pytorch.utilities.migration\",\n",
        "                    \"lightning.pytorch.utilities\",\n",
        "                    \"lightning.pytorch\"\n",
        "                ]:\n",
        "                    logging.getLogger(logger_name).setLevel(logging.CRITICAL + 1)\n",
        "\n",
        "                # Подавление всех предупреждений (расширенный список)\n",
        "                warnings.filterwarnings(\"ignore\")\n",
        "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*Attribute 'loss' is an instance of nn.Module.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*Attribute 'logging_metrics' is an instance of nn.Module.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*GPU available.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*TPU available.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*HPU available.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*This Pipeline instance is not fitted yet.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*Using an existing study with name.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*A value is trying to be set on a copy of a slice.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*The behavior of DataFrame concatenation.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: the use_reentrant parameter.*\")\n",
        "\n",
        "                # Окружение (расширенное)\n",
        "                os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "                os.environ['LITMODELS_DISABLE_TIP'] = '1'\n",
        "                os.environ['HYDRA_FULL_ERROR'] = '1'\n",
        "                os.environ['TQDM_DISABLE'] = '0'  # Не подавлять tqdm (если вызван снаружи)\n",
        "                os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Для синхронизации CUDA, но без логов\n",
        "                os.environ['TORCH_USE_DETERMINISTIC_ALGORITHMS'] = '1'  # Без логов\n",
        "\n",
        "                try:\n",
        "                    yield\n",
        "                finally:\n",
        "                    root_logger.setLevel(old_level)\n",
        "                    warnings.resetwarnings()\n",
        "\n",
        "        with suppress_all():\n",
        "            set_seeds(self.seed)\n",
        "            pl.seed_everything(self.seed, verbose=False, workers=True)\n",
        "\n",
        "            if self.preprocessing is not None and not isinstance(X, np.ndarray):\n",
        "                X = self.preprocessing.transform(X)\n",
        "\n",
        "            X = np.asarray(X)\n",
        "            N = X.shape[0]\n",
        "            if N < self.min_encoder_length:\n",
        "                return [float(np.nan)] * N\n",
        "\n",
        "            feat, _, time_raw = split_features_batch_time(X, self._n_transformed)\n",
        "            df = pd.DataFrame(feat, columns=self._feature_columns)\n",
        "            df[\"__row_id\"] = np.arange(N)\n",
        "            df[\"time_raw\"] = pd.to_datetime(time_raw, utc=True, errors=\"coerce\")\n",
        "            df[\"batch\"] = np.int64(0)\n",
        "            df[\"target\"] = self.global_target_mean\n",
        "\n",
        "            df = df.dropna(subset=[\"time_raw\"]).reset_index(drop=True)\n",
        "            df_sorted = df.sort_values(\"time_raw\").reset_index(drop=True)\n",
        "            df_sorted[\"time_idx\"] = np.arange(len(df_sorted), dtype=np.int64)\n",
        "\n",
        "            eff_N = len(df_sorted)\n",
        "            if eff_N < self.min_encoder_length:\n",
        "                out = np.full(N, np.nan, dtype=float)\n",
        "                return out.tolist()\n",
        "\n",
        "            step_ns = int(60 * 1e9)\n",
        "            if eff_N >= 2:\n",
        "                diffs = df_sorted[\"time_raw\"].view(\"int64\").astype(\"int64\").to_numpy()\n",
        "                diffs = np.diff(diffs)\n",
        "                step_ns = int(np.nan_to_num(np.median(diffs), nan=step_ns))\n",
        "                if step_ns <= 0:\n",
        "                    step_ns = int(60 * 1e9)\n",
        "\n",
        "            out_raw = np.full(eff_N, np.nan, dtype=float)\n",
        "\n",
        "            tft = self._model.tft\n",
        "            orig_log = tft.log\n",
        "            tft.log = lambda *args, **kwargs: None\n",
        "\n",
        "            try:\n",
        "                eff_encoder_len = min(self.seq_len, eff_N)\n",
        "                M = max(0, eff_N - eff_encoder_len + 1)\n",
        "                stride = self.pred_len  # Keep original stride to preserve logic\n",
        "                window_starts = np.arange(0, M, stride)\n",
        "                K = len(window_starts)\n",
        "\n",
        "                if K == 0:\n",
        "                    K = 1\n",
        "                    window_starts = np.array([0])\n",
        "                    eff_encoder_len = eff_N\n",
        "\n",
        "                num_feat_cols = len(self._feature_columns)\n",
        "\n",
        "                # Vectorized construction of all_feats\n",
        "                enc_indices = window_starts[:, np.newaxis] + np.arange(eff_encoder_len)\n",
        "                all_enc_feats = feat[enc_indices]  # (K, eff_encoder_len, num_feat_cols)\n",
        "                last_enc_feats = all_enc_feats[:, -1, :]\n",
        "                all_fut_feats = np.repeat(last_enc_feats[:, np.newaxis, :], self.pred_len, axis=1)  # (K, pred_len, num_feat_cols)\n",
        "                all_feats = np.concatenate([all_enc_feats, all_fut_feats], axis=1).reshape(-1, num_feat_cols)\n",
        "\n",
        "                # Vectorized all_time_idx\n",
        "                orig_time_idx = df_sorted[\"time_idx\"].values\n",
        "                all_enc_time_idx = orig_time_idx[enc_indices]  # (K, eff_encoder_len)\n",
        "                last_time_idx = all_enc_time_idx[:, -1]\n",
        "                fut_offsets = np.arange(1, self.pred_len + 1)\n",
        "                all_fut_time_idx = last_time_idx[:, np.newaxis] + fut_offsets  # (K, pred_len)\n",
        "                all_time_idx = np.concatenate([all_enc_time_idx, all_fut_time_idx], axis=1).reshape(-1)\n",
        "\n",
        "                # Vectorized all_batch\n",
        "                batch_per_window = np.arange(K)[:, np.newaxis]\n",
        "                all_enc_batch = np.repeat(batch_per_window, eff_encoder_len, axis=1)  # (K, eff_encoder_len)\n",
        "                all_fut_batch = np.repeat(batch_per_window, self.pred_len, axis=1)  # (K, pred_len)\n",
        "                all_batch = np.concatenate([all_enc_batch, all_fut_batch], axis=1).reshape(-1)\n",
        "\n",
        "                # all_target (constant)\n",
        "                all_target = np.full(K * (eff_encoder_len + self.pred_len), self.global_target_mean, dtype=np.float32)\n",
        "\n",
        "                # Vectorized all_time_raw using int64 ns\n",
        "                orig_time_raw_int = df_sorted[\"time_raw\"].view(\"int64\").values\n",
        "                all_enc_time_int = orig_time_raw_int[enc_indices]  # (K, eff_encoder_len)\n",
        "                last_time_int = all_enc_time_int[:, -1]\n",
        "                fut_offsets_ns = fut_offsets * step_ns\n",
        "                all_fut_time_int = last_time_int[:, np.newaxis] + fut_offsets_ns  # (K, pred_len)\n",
        "                all_time_int_flat = np.concatenate([all_enc_time_int.reshape(-1), all_fut_time_int.reshape(-1)])\n",
        "                all_time_raw = pd.to_datetime(all_time_int_flat, unit='ns', utc=True).values  # object array of Timestamp\n",
        "\n",
        "                pred_df = pd.DataFrame(all_feats, columns=self._feature_columns)\n",
        "                pred_df[\"time_idx\"] = all_time_idx\n",
        "                pred_df[\"batch\"] = all_batch\n",
        "                pred_df[\"target\"] = all_target\n",
        "                pred_df[\"time_raw\"] = all_time_raw\n",
        "\n",
        "                pred_df[\"batch\"] = pred_df[\"batch\"].astype(\"int64\")\n",
        "                pred_df[\"time_idx\"] = pred_df[\"time_idx\"].astype(\"int64\")\n",
        "                pred_df[\"target\"] = pred_df[\"target\"].astype(\"float32\")\n",
        "\n",
        "                sliding_dataset = TimeSeriesDataSet.from_dataset(\n",
        "                    self._train_dataset,\n",
        "                    pred_df,\n",
        "                    predict=True,\n",
        "                    stop_randomization=True,\n",
        "                    min_prediction_length=self.pred_len,\n",
        "                    max_prediction_length=self.pred_len,\n",
        "                )\n",
        "\n",
        "                test_dl = sliding_dataset.to_dataloader(\n",
        "                    train=False,\n",
        "                    batch_size=K,  # Full batch for max speed (K small due to stride=pred_len)\n",
        "                    num_workers=0,\n",
        "                    persistent_workers=False,\n",
        "                    pin_memory=False,\n",
        "                )\n",
        "\n",
        "                preds = tft.predict(\n",
        "                    test_dl,\n",
        "                    mode=\"prediction\",\n",
        "                    return_x=False,\n",
        "                    trainer_kwargs={\n",
        "                        \"logger\": False,\n",
        "                        \"enable_progress_bar\": False,\n",
        "                        \"enable_model_summary\": False,\n",
        "                        \"enable_checkpointing\": False,\n",
        "                        \"accelerator\": \"gpu\" if self.device == \"cuda\" else \"cpu\"\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                if isinstance(preds, torch.Tensor):\n",
        "                    preds = preds.detach().cpu()\n",
        "                    if preds.dim() == 3 and preds.size(-1) == 1:\n",
        "                        preds = preds.squeeze(-1)\n",
        "                    if preds.dim() == 2:\n",
        "                        yhat_stride = preds[:, 0].numpy()  # First step per window\n",
        "                    elif preds.dim() == 1:\n",
        "                        yhat_stride = preds.numpy()\n",
        "                    elif preds.dim() == 0:\n",
        "                        yhat_stride = np.array([preds.item()])\n",
        "                    else:\n",
        "                        yhat_stride = np.full(K, self.global_target_mean)\n",
        "                else:\n",
        "                    yhat_stride = np.full(K, self.global_target_mean)\n",
        "\n",
        "                # Fill out_raw at window ends\n",
        "                for j, i in enumerate(window_starts):\n",
        "                    out_raw[i + eff_encoder_len - 1] = yhat_stride[j]\n",
        "\n",
        "                # Causal interpolation for missed points (linear from past, no future look)\n",
        "                s = pd.Series(out_raw)\n",
        "                s = s.interpolate(method='linear', limit_direction='forward')  # Only forward for causality\n",
        "                s = s.ffill()  # Fill initial with first pred\n",
        "                out_raw = s.values\n",
        "\n",
        "                # Global map to [-1,1] (consistent scale, preserves relative peaks)\n",
        "                out = (out_raw - self.global_target_min) / self.global_range * 2 - 1\n",
        "                out = np.tanh(out * self.soft_clip_scale) / np.tanh(self.soft_clip_scale)\n",
        "                out = np.clip(out, -1.0, 1.0)\n",
        "\n",
        "                # Fill any remaining nans (unlikely) with mapped mean\n",
        "                nan_mask = np.isnan(out)\n",
        "                mapped_mean = (self.global_target_mean - self.global_target_min) / self.global_range * 2 - 1\n",
        "                out[nan_mask] = mapped_mean\n",
        "\n",
        "            finally:\n",
        "                tft.log = orig_log\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            original_pos = df_sorted[\"__row_id\"].to_numpy()\n",
        "            out_final = np.full(N, np.nan, dtype=float)\n",
        "            out_final[original_pos] = out\n",
        "\n",
        "            return out_final.tolist()\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Чтобы sklearn корректно передавал параметры в Pipeline\n",
        "        return {\n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pred_len\": self.pred_len,\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"epochs\": self.epochs,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"patience\": self.patience,\n",
        "            \"seed\": self.seed,\n",
        "            \"dropout\": self._dropout,\n",
        "            \"weight_decay\": self.weight_decay,\n",
        "            \"verbose\": self.verbose,\n",
        "            \"ckpt_path\": self.ckpt_path\n",
        "        }\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "def _tensor_state_dict_to(dtype: torch.dtype, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    out = {}\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            # важно: сохраняем в CPU и ровно в dtype (float32 для идентичности)\n",
        "            out[k] = v.detach().to('cpu', dtype=dtype)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def _save_bytes_compressed_zip(file_path: str, bytes_map: Dict[str, bytes], compression=zipfile.ZIP_DEFLATED, compresslevel=9):\n",
        "    with zipfile.ZipFile(file_path, mode='w', compression=compression, compresslevel=compresslevel) as zf:\n",
        "        for name, b in bytes_map.items():\n",
        "            zf.writestr(name, b)\n",
        "\n",
        "\n",
        "def _load_bytes_from_zip(file_path: str) -> Dict[str, bytes]:\n",
        "    out = {}\n",
        "    with zipfile.ZipFile(file_path, mode='r') as zf:\n",
        "        for name in zf.namelist():\n",
        "            out[name] = zf.read(name)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _safe_json_dump(obj: Dict[str, Any]) -> bytes:\n",
        "    def to_builtin(x):\n",
        "        import numpy as _np\n",
        "        import pandas as _pd\n",
        "        if isinstance(x, (_np.integer,)):\n",
        "            return int(x)\n",
        "        if isinstance(x, (_np.floating,)):\n",
        "            return float(x)\n",
        "        if isinstance(x, (_np.ndarray,)):\n",
        "            return x.tolist()\n",
        "        if isinstance(x, (_pd.Timestamp,)):\n",
        "            return x.isoformat()\n",
        "        return x\n",
        "\n",
        "    def convert(v):\n",
        "        if isinstance(v, dict):\n",
        "            return {k: convert(val) for k, val in v.items()}\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            return [convert(i) for i in v]\n",
        "        return to_builtin(v)\n",
        "\n",
        "    return json.dumps(convert(obj), ensure_ascii=False, separators=(\",\", \":\")).encode(\"utf-8\")\n",
        "\n",
        "\n",
        "def _safe_json_load(b: bytes) -> Dict[str, Any]:\n",
        "    return json.loads(b.decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "def _extract_tsd_feature_lists(tds) -> Dict[str, Any]:\n",
        "    def g(name, default=None):\n",
        "        return getattr(tds, name, default)\n",
        "\n",
        "    lists = {}\n",
        "    for name in [\n",
        "        \"time_varying_known_reals\",\n",
        "        \"time_varying_unknown_reals\",\n",
        "        \"static_reals\",\n",
        "        \"time_varying_known_categoricals\",\n",
        "        \"time_varying_unknown_categoricals\",\n",
        "        \"static_categoricals\",\n",
        "        \"target_categoricals\",\n",
        "        \"known_reals\",\n",
        "        \"unknown_reals\",\n",
        "        \"known_categoricals\",\n",
        "        \"unknown_categoricals\",\n",
        "        \"reals\",\n",
        "        \"categoricals\",\n",
        "    ]:\n",
        "        val = g(name, None)\n",
        "        if val is not None:\n",
        "            try:\n",
        "                lists[name] = list(val)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return lists\n",
        "\n",
        "\n",
        "def save_transformer(\n",
        "    pipeline: Pipeline,\n",
        "    path: str,\n",
        "    *,\n",
        "    # ЖЁСТКО: float32 по умолчанию для идентичности. Не выставляйте True, пока не сравните предикты.\n",
        "    float16_weights: bool = False,\n",
        "    preprocessing_filename: str = \"preprocessing.joblib.lzma\",\n",
        "    model_zip_filename: str = \"model_weights.zip\",\n",
        "    meta_json_filename: str = \"meta.json\"\n",
        "):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    if not isinstance(pipeline, Pipeline):\n",
        "        raise TypeError(\"Expected sklearn Pipeline\")\n",
        "\n",
        "    steps_dict = dict(pipeline.named_steps)\n",
        "    if \"preprocessing\" not in steps_dict or \"model\" not in steps_dict:\n",
        "        raise ValueError(\"Pipeline must have 'preprocessing' and 'model' steps\")\n",
        "\n",
        "    preprocessing = steps_dict[\"preprocessing\"]\n",
        "    model: SequenceTransformerRegressor = steps_dict[\"model\"]\n",
        "\n",
        "    if model._model is None or model._train_dataset is None:\n",
        "        raise RuntimeError(\"Model must be fitted before saving\")\n",
        "\n",
        "    # 1) preprocessing\n",
        "    prep_path = os.path.join(path, preprocessing_filename)\n",
        "    with lzma.open(prep_path, \"wb\", preset=9) as f:\n",
        "        joblib.dump(preprocessing, f)\n",
        "\n",
        "    # 2) параметры TDS\n",
        "    tds = model._train_dataset\n",
        "    def g(name, default=None):\n",
        "        return getattr(tds, name, default)\n",
        "\n",
        "    train_dataset_params = {\n",
        "        \"time_idx\": g(\"time_idx\", \"time_idx\"),\n",
        "        \"target\": g(\"target\", \"target\"),\n",
        "        \"group_ids\": list(g(\"group_ids\", [\"batch\"])),\n",
        "        \"max_encoder_length\": int(g(\"max_encoder_length\", model.seq_len)),\n",
        "        \"max_prediction_length\": int(g(\"max_prediction_length\", model.pred_len)),\n",
        "        \"target_normalizer\": None,\n",
        "        \"allow_missing_timesteps\": bool(g(\"allow_missing_timesteps\", True)),\n",
        "        \"add_relative_time_idx\": bool(g(\"add_relative_time_idx\", True)),\n",
        "        \"add_target_scales\": bool(g(\"add_target_scales\", False)),\n",
        "        \"add_encoder_length\": bool(g(\"add_encoder_length\", True)),\n",
        "        \"min_prediction_length\": int(g(\"min_prediction_length\", 1)),\n",
        "        \"min_encoder_length\": int(g(\"min_encoder_length\", 0)),\n",
        "    }\n",
        "    feature_lists = _extract_tsd_feature_lists(tds)\n",
        "\n",
        "    # 3) веса TFT — строго в float32 (если float16_weights=False)\n",
        "    tft = model._model.tft\n",
        "    state = tft.state_dict()\n",
        "    dtype = torch.float16 if float16_weights else torch.float32\n",
        "    state = _tensor_state_dict_to(dtype, state)\n",
        "    buf = io.BytesIO()\n",
        "    torch.save(state, buf, _use_new_zipfile_serialization=True)\n",
        "    buf.seek(0)\n",
        "    weights_zip_path = os.path.join(path, model_zip_filename)\n",
        "    _save_bytes_compressed_zip(weights_zip_path, {\"tft_state_dict.pt\": buf.getvalue()})\n",
        "\n",
        "    # 4) метаданные\n",
        "    meta = {\n",
        "        \"class\": \"SequenceTransformerRegressor\",\n",
        "        \"params\": {\n",
        "            \"seq_len\": model.seq_len,\n",
        "            \"pred_len\": model.pred_len,\n",
        "            \"hidden_size\": model.hidden_size,\n",
        "            \"hidden_continuous_size\": model.hidden_continuous_size,\n",
        "            \"epochs\": model.epochs,\n",
        "            \"batch_size\": model.batch_size,\n",
        "            \"learning_rate\": model.learning_rate,\n",
        "            \"patience\": model.patience,\n",
        "            \"seed\": model.seed,\n",
        "            \"dropout\": model._dropout,\n",
        "            \"weight_decay\": model.weight_decay,\n",
        "            \"verbose\": model.verbose,\n",
        "            \"mask_prob\": model.mask_prob,\n",
        "            \"infer_stride\": model.infer_stride,\n",
        "            \"ckpt_path\": None,\n",
        "            \"min_encoder_length\": model.min_encoder_length,\n",
        "            \"lstm_layers\": 3,\n",
        "            \"attention_head_size\": 4,\n",
        "            \"output_size\": 1,\n",
        "        },\n",
        "        \"feature_columns\": model._feature_columns,\n",
        "        \"n_transformed\": model._n_transformed,\n",
        "        \"n_feat\": model._n_feat,\n",
        "        \"global_stats\": {\n",
        "            \"global_target_mean\": model.global_target_mean,\n",
        "            \"global_target_std\": model.global_target_std,\n",
        "            \"global_target_min\": model.global_target_min,\n",
        "            \"global_target_max\": model.global_target_max,\n",
        "            \"global_range\": model.global_range,\n",
        "            \"soft_clip_scale\": model.soft_clip_scale,\n",
        "            \"clip_scale\": model.clip_scale,\n",
        "        },\n",
        "        \"train_dataset_params\": train_dataset_params,\n",
        "        \"feature_lists\": feature_lists,\n",
        "        \"torch_dtype\": \"float16\" if float16_weights else \"float32\",\n",
        "        \"preproc_feature_names_out\": list(getattr(preprocessing, \"get_feature_names_out\", lambda: [])()),\n",
        "    }\n",
        "\n",
        "    meta_path = os.path.join(path, meta_json_filename)\n",
        "    with open(meta_path, \"wb\") as f:\n",
        "        f.write(_safe_json_dump(meta))\n",
        "\n",
        "    weights_size_mb = os.path.getsize(weights_zip_path) / (1024 * 1024)\n",
        "    prep_size_mb = os.path.getsize(prep_path) / (1024 * 1024)\n",
        "    meta_size_kb = os.path.getsize(meta_path) / 1024.0\n",
        "    print(f\"Saved: weights={weights_size_mb:.2f} MB, preprocessing={prep_size_mb:.2f} MB, meta={meta_size_kb:.1f} KB → dir={path}\")\n",
        "\n",
        "# Требуется наличие в окружении:\n",
        "# - SequenceTransformerRegressor\n",
        "# - TimeSeriesDataSet\n",
        "# - CustomTFT, TFTAdapter\n",
        "# - tft_output_transformer\n",
        "# - PeakFriendlyHuber\n",
        "# - to_dense\n",
        "# - вспомогательные функции _load_bytes_from_zip, _safe_json_load из предыдущей версии\n",
        "\n",
        "\n",
        "def load_transformer_exact(\n",
        "    path: str,\n",
        "    *,\n",
        "    preprocessing_filename: str = \"preprocessing.joblib.lzma\",\n",
        "    model_zip_filename: str = \"model_weights.zip\",\n",
        "    meta_json_filename: str = \"meta.json\",\n",
        "    device: Optional[str] = None,\n",
        ") -> Pipeline:\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", message=\".*Attribute 'loss' is an instance of `nn.Module`.*\")\n",
        "    warnings.filterwarnings(\"ignore\", message=\".*Attribute 'logging_metrics' is an instance of `nn.Module`.*\")\n",
        "\n",
        "    # 1) preprocessing\n",
        "    prep_path = os.path.join(path, preprocessing_filename)\n",
        "    with lzma.open(prep_path, \"rb\") as f:\n",
        "        preprocessing = joblib.load(f)\n",
        "\n",
        "    # 2) meta\n",
        "    meta_path = os.path.join(path, meta_json_filename)\n",
        "    if not os.path.isfile(meta_path):\n",
        "        raise FileNotFoundError(meta_path)\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = json.loads(f.read().decode(\"utf-8\"))\n",
        "\n",
        "    params = meta[\"params\"]\n",
        "    feature_columns = meta[\"feature_columns\"]\n",
        "    n_transformed = meta[\"n_transformed\"]\n",
        "    n_feat = meta[\"n_feat\"]\n",
        "    global_stats = meta[\"global_stats\"]\n",
        "    tds_params = meta[\"train_dataset_params\"]\n",
        "    feature_lists = meta.get(\"feature_lists\", {})\n",
        "    saved_torch_dtype = meta.get(\"torch_dtype\", \"float32\")\n",
        "\n",
        "    # 3) регрессор\n",
        "    model = SequenceTransformerRegressor(\n",
        "        seq_len=params[\"seq_len\"],\n",
        "        pred_len=params[\"pred_len\"],\n",
        "        hidden_size=params[\"hidden_size\"],\n",
        "        hidden_continuous_size=params[\"hidden_continuous_size\"],\n",
        "        epochs=params[\"epochs\"],\n",
        "        batch_size=params[\"batch_size\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        patience=params[\"patience\"],\n",
        "        seed=params[\"seed\"],\n",
        "        dropout=params[\"dropout\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        verbose=params[\"verbose\"],\n",
        "        mask_prob=params[\"mask_prob\"],\n",
        "        infer_stride=params[\"infer_stride\"],\n",
        "        ckpt_path=None,\n",
        "        preprocessing=preprocessing,\n",
        "        min_encoder_length=params.get(\"min_encoder_length\", 1),\n",
        "    )\n",
        "\n",
        "    model._feature_columns = feature_columns\n",
        "    model._n_transformed = n_transformed\n",
        "    model._n_feat = n_feat\n",
        "    model.global_target_mean = float(global_stats[\"global_target_mean\"])\n",
        "    model.global_target_std = float(global_stats[\"global_target_std\"])\n",
        "    model.global_target_min = float(global_stats[\"global_target_min\"])\n",
        "    model.global_target_max = float(global_stats[\"global_target_max\"])\n",
        "    model.global_range = float(global_stats[\"global_range\"])\n",
        "    model.soft_clip_scale = float(global_stats[\"soft_clip_scale\"])\n",
        "    model.clip_scale = float(global_stats.get(\"clip_scale\", 1.5))\n",
        "    model.device = device\n",
        "\n",
        "    # 4) синтетический df\n",
        "    max_enc = int(tds_params[\"max_encoder_length\"])\n",
        "    max_pred = int(tds_params[\"max_prediction_length\"])\n",
        "    num_feat_cols = len(feature_columns)\n",
        "    rows = max_enc + max_pred\n",
        "\n",
        "    synth_df_base = pd.DataFrame(\n",
        "        np.zeros((rows, num_feat_cols), dtype=np.float32),\n",
        "        columns=feature_columns,\n",
        "    )\n",
        "    synth_df_base[\"time_idx\"] = np.arange(rows, dtype=np.int64)\n",
        "    synth_df_base[\"target\"] = np.zeros(rows, dtype=np.float32)\n",
        "    synth_df_base[\"batch\"] = 0\n",
        "\n",
        "    # Попытка №1: «обычная» конструкция TDS как при обучении\n",
        "    tds_kwargs = dict(\n",
        "        time_idx=tds_params[\"time_idx\"],\n",
        "        target=tds_params[\"target\"],\n",
        "        group_ids=tds_params[\"group_ids\"],\n",
        "        max_encoder_length=tds_params[\"max_encoder_length\"],\n",
        "        max_prediction_length=tds_params[\"max_prediction_length\"],\n",
        "        target_normalizer=None,\n",
        "        allow_missing_timesteps=tds_params[\"allow_missing_timesteps\"],\n",
        "        add_relative_time_idx=tds_params[\"add_relative_time_idx\"],\n",
        "        add_target_scales=tds_params[\"add_target_scales\"],\n",
        "        add_encoder_length=tds_params[\"add_encoder_length\"],\n",
        "        min_prediction_length=tds_params[\"min_prediction_length\"],\n",
        "        min_encoder_length=tds_params.get(\"min_encoder_length\", 0),\n",
        "    )\n",
        "\n",
        "    if \"time_varying_unknown_reals\" in feature_lists:\n",
        "        tds_kwargs[\"time_varying_unknown_reals\"] = list(feature_lists[\"time_varying_unknown_reals\"])\n",
        "    else:\n",
        "        tds_kwargs[\"time_varying_unknown_reals\"] = list(feature_columns)\n",
        "\n",
        "    for key in [\n",
        "        \"time_varying_known_reals\",\n",
        "        \"static_reals\",\n",
        "        \"time_varying_known_categoricals\",\n",
        "        \"time_varying_unknown_categoricals\",\n",
        "        \"static_categoricals\",\n",
        "        \"known_reals\",\n",
        "        \"unknown_reals\",\n",
        "        \"known_categoricals\",\n",
        "        \"unknown_categoricals\",\n",
        "    ]:\n",
        "        if key in feature_lists:\n",
        "            tds_kwargs[key] = list(feature_lists[key])\n",
        "\n",
        "    dataset = TimeSeriesDataSet(synth_df_base.copy(), **tds_kwargs)\n",
        "\n",
        "    # Проверяем порядок reals\n",
        "    saved_reals = feature_lists.get(\"reals\")\n",
        "    rebuild_forced = False\n",
        "    if saved_reals is not None:\n",
        "        # В PF список dataset.reals может быть кортежами/объектами -- приводим к строкам\n",
        "        ds_reals = list(getattr(dataset, \"reals\", []))\n",
        "        ds_reals = [str(x) for x in ds_reals]\n",
        "        saved_reals_str = [str(x) for x in saved_reals]\n",
        "        if ds_reals != saved_reals_str:\n",
        "            rebuild_forced = True\n",
        "\n",
        "    if rebuild_forced:\n",
        "        # Попытка №2: Форсируем порядок каналов reals.\n",
        "        # Для этого отключим автоматические добавления и сами сгенерируем служебные колонки\n",
        "        # encoder_length и relative_time_idx в synth_df.\n",
        "        synth_df = synth_df_base.copy()\n",
        "        # relative_time_idx: от 0 до rows-1\n",
        "        synth_df[\"relative_time_idx\"] = np.arange(rows, dtype=np.int64)\n",
        "        # encoder_length: длина encoder для каждой позиции; для синтетики можно установить константу max_enc\n",
        "        # PF ожидает целочисленную encoder_length, соответствующую длине энкодера на каждом шаге.\n",
        "        # Для инициализации архитектуры достаточно положить валидные числа.\n",
        "        enc_len = np.zeros(rows, dtype=np.int64)\n",
        "        enc_len[:max_enc] = np.arange(1, max_enc + 1, dtype=np.int64)\n",
        "        enc_len[max_enc:] = max_enc\n",
        "        synth_df[\"encoder_length\"] = enc_len\n",
        "\n",
        "        # Теперь задаём TDS без add_* фичей, и передаём time_varying_unknown_reals в порядке saved_reals.\n",
        "        # saved_reals начинается с [\"encoder_length\",\"relative_time_idx\", ... f0..fN]\n",
        "        tds_kwargs_forced = dict(\n",
        "            time_idx=tds_params[\"time_idx\"],\n",
        "            target=tds_params[\"target\"],\n",
        "            group_ids=tds_params[\"group_ids\"],\n",
        "            max_encoder_length=tds_params[\"max_encoder_length\"],\n",
        "            max_prediction_length=tds_params[\"max_prediction_length\"],\n",
        "            target_normalizer=None,\n",
        "            allow_missing_timesteps=tds_params[\"allow_missing_timesteps\"],\n",
        "            add_relative_time_idx=False,\n",
        "            add_target_scales=tds_params[\"add_target_scales\"],\n",
        "            add_encoder_length=False,\n",
        "            min_prediction_length=tds_params[\"min_prediction_length\"],\n",
        "            min_encoder_length=tds_params.get(\"min_encoder_length\", 0),\n",
        "            time_varying_unknown_reals=list(saved_reals),  # порядок каналов фиксируем здесь\n",
        "            # Не задаём known/unknown cats/reals дополнительно, чтобы не нарушить порядок\n",
        "        )\n",
        "        dataset = TimeSeriesDataSet(synth_df, **tds_kwargs_forced)\n",
        "\n",
        "        # Контроль: проверим снова порядок\n",
        "        ds_reals2 = [str(x) for x in list(getattr(dataset, \"reals\", []))]\n",
        "        if ds_reals2 != [str(x) for x in saved_reals]:\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to force reals order. Got {ds_reals2[:8]}..., expected {list(saved_reals)[:8]}...\"\n",
        "            )\n",
        "\n",
        "    # 5) TFT 1-в-1\n",
        "    tft = CustomTFT.from_dataset(\n",
        "        dataset,\n",
        "        hidden_size=int(model.hidden_size),\n",
        "        output_size=int(params.get(\"output_size\", 1)),\n",
        "        loss=PeakFriendlyHuber(\n",
        "            delta=0.5,\n",
        "            peak_thr=0.85,\n",
        "            peak_weight=1.3,\n",
        "            contrast_weight=0.03,\n",
        "            center_band=0.3,\n",
        "            clip_scale=1.5,\n",
        "        ),\n",
        "        optimizer=\"adam\",\n",
        "        learning_rate=float(model.learning_rate),\n",
        "        lstm_layers=int(params.get(\"lstm_layers\", 3)),\n",
        "        hidden_continuous_size=int(model.hidden_continuous_size),\n",
        "        attention_head_size=int(params.get(\"attention_head_size\", 4)),\n",
        "        dropout=float(model._dropout),\n",
        "        reduce_on_plateau_patience=5,\n",
        "        reduce_on_plateau_min_lr=1e-6,\n",
        "        weight_decay=float(model.weight_decay),\n",
        "        mask_prob=float(model.mask_prob),\n",
        "        output_transformer=tft_output_transformer,\n",
        "    )\n",
        "\n",
        "    model._model = TFTAdapter(tft)\n",
        "    model._train_dataset = dataset\n",
        "\n",
        "    # 6) загрузка весов\n",
        "    weights_zip_path = os.path.join(path, model_zip_filename)\n",
        "    with zipfile.ZipFile(weights_zip_path, mode='r') as zf:\n",
        "        if \"tft_state_dict.pt\" not in zf.namelist():\n",
        "            raise RuntimeError(\"tft_state_dict.pt not found in weights zip\")\n",
        "        state = torch.load(io.BytesIO(zf.read(\"tft_state_dict.pt\")), map_location=\"cpu\")\n",
        "\n",
        "    missing, unexpected = tft.load_state_dict(state, strict=False)\n",
        "    if missing or unexpected:\n",
        "        warnings.warn(f\"load_state_dict: missing={missing}, unexpected={unexpected}\")\n",
        "\n",
        "    # 7) eval + dropout off\n",
        "    tft.eval()\n",
        "    for m in tft.modules():\n",
        "        if isinstance(m, torch.nn.Dropout):\n",
        "            m.p = 0.0\n",
        "\n",
        "    model._model.to(device)\n",
        "\n",
        "    restored = Pipeline([\n",
        "        (\"preprocessing\", preprocessing),\n",
        "        (\"to_dense\", FunctionTransformer(to_dense)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "    return restored\n",
        "\n",
        "def apply_linear_calibration(y_pred: np.ndarray, calib: dict) -> np.ndarray:\n",
        "    a, b = calib.get('a', 1.0), calib.get('b', 0.0)\n",
        "    return a * np.ravel(y_pred) + b\n",
        "\n",
        "def ranker_postprocess_minus1_1(y_pred_ranker: np.ndarray) -> np.ndarray:\n",
        "    yp = np.ravel(y_pred_ranker)\n",
        "    return 2.0 * ((yp + 15.0) / (5.0 + 15.0)) - 1.0\n",
        "\n",
        "def load_optimized_pipeline(out_dir, model_type='regressor'):\n",
        "    \"\"\"\n",
        "    Загружает оптимизированный pipeline.\n",
        "    Возвращает полный pipe.\n",
        "    \"\"\"\n",
        "    # Загружаем preproc\n",
        "    preproc_file = f'preproc_{model_type}.pkl' if model_type == 'ranker' else 'preproc.pkl'\n",
        "    preproc = joblib.load(os.path.join(out_dir, preproc_file))\n",
        "\n",
        "    # Booster из gz\n",
        "    gz_file = os.path.join(out_dir, f'{model_type}.txt.gz')\n",
        "    temp_file = os.path.join(out_dir, f'temp_{model_type}.txt')\n",
        "    with gzip.open(gz_file, 'rb') as f_in:\n",
        "        with open(temp_file, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    booster = lgb.Booster(model_file=temp_file)\n",
        "    os.remove(temp_file)\n",
        "\n",
        "    # Реконструируем модель (LGBM wrapper)\n",
        "    step_name = 'model' if model_type == 'regressor' else 'ranker'\n",
        "    if model_type == 'regressor':\n",
        "        model = lgb.LGBMRegressor()\n",
        "    elif model_type == 'ranker':\n",
        "        model = lgb.LGBMRanker()\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model_type\")\n",
        "\n",
        "    model._Booster = booster\n",
        "    model.fitted_ = True  # Отметить как обученную\n",
        "    # Устанавливаем n_features_in_ из booster\n",
        "    model.n_features_in_ = booster.num_feature()\n",
        "\n",
        "    # Полный pipe\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessing', preproc),\n",
        "        ('to_dense', FunctionTransformer(to_dense, feature_names_out=\"one-to-one\")),\n",
        "        (step_name, model),\n",
        "    ])\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def load_model_for_ticker(ticker, models_dir=r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\models'):\n",
        "    \"\"\"\n",
        "    Загружает модель(и) для тикера из директории models//.\n",
        "    Возвращает dict с: pipe_reg, pipe_rank (если combined), meta, threshold.\n",
        "    \"\"\"\n",
        "    out_dir = os.path.join(models_dir, ticker)\n",
        "    if not os.path.exists(out_dir):\n",
        "        raise FileNotFoundError(f\"Directory for {ticker} not found: {out_dir}\")\n",
        "\n",
        "    # Загружаем meta\n",
        "    with open(os.path.join(out_dir, 'meta.json'), 'r') as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    best_method = meta['best_method']\n",
        "    thresh = meta.get('sell_threshold', 0.0)  # Fallback\n",
        "\n",
        "    pipe_reg = None\n",
        "    pipe_rank = None\n",
        "\n",
        "    if best_method == 'regressor':\n",
        "        pipe_reg = load_optimized_pipeline(out_dir, 'regressor')\n",
        "\n",
        "    elif best_method == 'ranker':\n",
        "        pipe_rank = load_optimized_pipeline(out_dir, 'ranker')\n",
        "\n",
        "    elif best_method == 'combined':\n",
        "        pipe_reg = load_optimized_pipeline(out_dir, 'regressor')\n",
        "        pipe_rank = load_optimized_pipeline(out_dir, 'ranker')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown best_method: {best_method}\")\n",
        "\n",
        "    return {\n",
        "        'best_method': best_method,\n",
        "        'pipe_reg': pipe_reg,\n",
        "        'pipe_rank': pipe_rank,\n",
        "        'meta': meta,\n",
        "        'threshold': thresh,\n",
        "        'calib_reg': meta.get('reg_calibration', {'a': 1.0, 'b': 0.0}),\n",
        "        'w_reg': meta.get('best_w_reg', 0.5) if best_method == 'combined' else None\n",
        "    }\n",
        "\n",
        "def extract_features(df: pd.DataFrame, window: int = 126):\n",
        "    \"\"\"\n",
        "    Вычисляет устойчивые признаки для кластеризации рыночных режимов.\n",
        "    \"\"\"\n",
        "\n",
        "    def calculate_macd(df, macd_fast_periods=[12], macd_slow_periods=[26], macd_signal_periods=[9]):\n",
        "        \"\"\"\n",
        "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
        "        \"\"\"\n",
        "        close = df['close']\n",
        "\n",
        "        # Создаем множества для уникальных периодов\n",
        "        unique_fast = set(macd_fast_periods)\n",
        "        unique_slow = set(macd_slow_periods)\n",
        "\n",
        "\n",
        "        # Предварительно вычисляем все необходимые EMA и скользящие средние\n",
        "        ema_cache = {}\n",
        "        rolling_cache = {}\n",
        "\n",
        "        # Кешируем быстрые EMA\n",
        "        for fp in unique_fast:\n",
        "            ema_cache[f'ema_{fp}'] = close.ewm(span=fp, adjust=False).mean()\n",
        "\n",
        "        # Кешируем медленные EMA и скользящие средние\n",
        "        for sp in unique_slow:\n",
        "            ema_cache[f'ema_{sp}'] = close.ewm(span=sp, adjust=False).mean()\n",
        "            rolling_cache[f'rolling_{sp}'] = close.rolling(window=sp).mean()\n",
        "\n",
        "        # Основной цикл вычислений\n",
        "        for fp in macd_fast_periods:\n",
        "            ema_fast = ema_cache[f'ema_{fp}']\n",
        "            for sp in macd_slow_periods:\n",
        "                ema_slow = ema_cache[f'ema_{sp}']\n",
        "                rolling_mean = rolling_cache[f'rolling_{sp}']\n",
        "\n",
        "                # Вычисляем MACD и нормализацию\n",
        "                macd = ema_fast - ema_slow\n",
        "                macd_norm = macd / rolling_mean\n",
        "\n",
        "                # Сохраняем MACD только один раз для комбинации fp/sp\n",
        "\n",
        "                # Обрабатываем сигнальные периоды\n",
        "                for sig in macd_signal_periods:\n",
        "                    # Вычисляем сигнальную линию\n",
        "                    signal = macd.ewm(span=sig, adjust=False).mean()\n",
        "                    signal_norm = signal / rolling_mean\n",
        "\n",
        "        return pd.DataFrame([macd_norm, signal_norm, macd_norm - signal_norm]).T.fillna(0)\n",
        "\n",
        "    def calculate_atr(df, atr_window=14):\n",
        "        \"\"\"\n",
        "        Расчет ATR и его сдвигов.\n",
        "        \"\"\"\n",
        "        high = df['high']\n",
        "        low = df['low']\n",
        "        close = df['close']\n",
        "\n",
        "        tr1 = high - low\n",
        "        tr2 = np.abs(high - close.shift(1))\n",
        "        tr3 = np.abs(low - close.shift(1))\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_window).mean()\n",
        "\n",
        "        return pd.Series(atr).fillna(0)\n",
        "\n",
        "    def calculate_rsi(df, rsi_period=14):\n",
        "        \"\"\"\n",
        "        Расчет RSI и его сдвиги.\n",
        "        \"\"\"\n",
        "        close = df['close']\n",
        "        delta = close.diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(rsi_period).mean()\n",
        "        avg_loss = loss.rolling(rsi_period).mean()\n",
        "        rs = avg_gain / (avg_loss + 1e-10)\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        return pd.Series(rsi).fillna(0)\n",
        "\n",
        "    def calculate_bollinger_bands(df, bollinger_window=20):\n",
        "        \"\"\"\n",
        "        Расчет Bollinger Bands (ширины полос) и сдвигов.\n",
        "        \"\"\"\n",
        "        close = df['close']\n",
        "        ma = close.rolling(bollinger_window).mean()\n",
        "        std = close.rolling(bollinger_window).std()\n",
        "        bb_width = (2 * std) / ma\n",
        "\n",
        "        return pd.Series(bb_width).fillna(0)\n",
        "\n",
        "    macd_trend = calculate_macd(df, macd_slow_periods=[window], macd_fast_periods=[window//3],\n",
        "                                 macd_signal_periods=[window//6])\n",
        "    atr = calculate_atr(df, atr_window=window)\n",
        "    rel_volatility = atr / df[\"close\"]\n",
        "    rsi_ind = calculate_rsi(df, rsi_period=window//2)\n",
        "    volume_ratio = df['volume'].rolling(window).apply(\n",
        "        lambda x: x[-1]/x.mean(), raw=True\n",
        "    ).fillna(1).values\n",
        "\n",
        "    features = np.column_stack([\n",
        "        macd_trend,\n",
        "        rel_volatility,\n",
        "        rsi_ind,\n",
        "        volume_ratio\n",
        "    ])\n",
        "\n",
        "    return features\n",
        "\n",
        "def prepare_regime_params(optuna_params):\n",
        "    \"\"\"\n",
        "    Преобразует параметры из формата Optuna в два словаря: базовые параметры режимов и параметры расчета.\n",
        "\n",
        "    Args:\n",
        "        optuna_params (dict): Словарь с параметрами из Optuna\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь с двумя ключами: 'base_params' (параметры режимов) и 'calc_params' (остальные параметры)\n",
        "    \"\"\"\n",
        "    # Инициализируем словари для базовых параметров и параметров расчета\n",
        "    start_params = {}\n",
        "    base_params = {}\n",
        "    calc_params = {}\n",
        "\n",
        "    # Сначала обрабатываем параметры режимов (0-4)\n",
        "\n",
        "    start_params['moving_average_length'] = optuna_params.get('moving_average_length', 14)\n",
        "    start_params['atr_period'] = optuna_params.get('atr_period', 10)\n",
        "    for regime in range(5):\n",
        "        regime_key = f'regime_{regime}_'\n",
        "        regime_params = {}\n",
        "\n",
        "        # Основные параметры режима\n",
        "        regime_params['average_type'] = optuna_params.get(f'{regime_key}average_type', 'SMA')\n",
        "        regime_params['moving_average_length'] = optuna_params.get(f'{regime_key}ma_length', 50)\n",
        "        regime_params['atr_period'] = optuna_params.get(f'{regime_key}atr_period', 14)\n",
        "        regime_params['atr_multiplier'] = optuna_params.get(f'{regime_key}atr_multiplier', 3.0)\n",
        "\n",
        "        # Параметры AMA, если они есть\n",
        "        ama_atr_period = optuna_params.get(f'{regime_key}ama_atr_period')\n",
        "        ama_min_period = optuna_params.get(f'{regime_key}ama_min_period')\n",
        "        ama_max_period = optuna_params.get(f'{regime_key}ama_max_period')\n",
        "\n",
        "        if regime_params['average_type'] == 'AMA' and all(p is not None for p in [ama_atr_period, ama_min_period, ama_max_period]):\n",
        "            regime_params['ama_params'] = {\n",
        "                'atr_period': int(ama_atr_period),\n",
        "                'min_period': int(ama_min_period),\n",
        "                'max_period': int(ama_max_period)\n",
        "            }\n",
        "\n",
        "        base_params[regime] = regime_params\n",
        "\n",
        "    # Теперь собираем все остальные параметры в calc_params\n",
        "    other_params = [\n",
        "        'rsi_length', 'use_smoothing', 'smoothing_length', 'smoothing_type',\n",
        "        'alma_sigma', 'rsi_overbought', 'rsi_oversold', 'use_knn',\n",
        "        'knn_neighbors', 'knn_lookback', 'knn_weight', 'feature_count',\n",
        "        'use_filter', 'filter_method', 'filter_strength', 'sma_length',\n",
        "        'ema_length', 'rsi_helbuth'\n",
        "    ]\n",
        "\n",
        "    for param in other_params:\n",
        "        if param in optuna_params:\n",
        "            calc_params[param] = optuna_params[param]\n",
        "\n",
        "    return {\n",
        "        'start_params': start_params,\n",
        "        'base_params': base_params,\n",
        "        'calc_params': calc_params\n",
        "    }\n",
        "\n",
        "class FastRollingMode:\n",
        "    def __init__(self, window_size):\n",
        "        self.window = deque(maxlen=window_size)\n",
        "        self.counts = {}\n",
        "\n",
        "    def update(self, new_val):\n",
        "        if len(self.window) == self.window.maxlen:\n",
        "            old_val = self.window.popleft()\n",
        "            self.counts[old_val] -= 1\n",
        "            if self.counts[old_val] == 0:\n",
        "                del self.counts[old_val]\n",
        "\n",
        "        self.window.append(new_val)\n",
        "        self.counts[new_val] = self.counts.get(new_val, 0) + 1\n",
        "        return max(self.counts.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "MODELS_CACHE = ModelsRuntimeCache()\n",
        "\n",
        "class TradingBot:\n",
        "    def __init__(self, token, tickers, data_path, interval, timeframe_minutes, data_points):\n",
        "        self.token = token\n",
        "        self.tickers = tickers\n",
        "        self.data_path = data_path\n",
        "        self.interval = interval\n",
        "        self.timeframe_minutes = timeframe_minutes\n",
        "        self.data_points = data_points\n",
        "\n",
        "        # Маппинг тикеров и FIGI\n",
        "        self.ticker_to_figi = {}\n",
        "        self.figi_to_ticker = {}\n",
        "\n",
        "        # Исторические данные\n",
        "        self.open_price = open_price\n",
        "        self.close_price = close_price\n",
        "        self.high_price = high_price\n",
        "        self.low_price = low_price\n",
        "        self.volume = volume\n",
        "        self.time_last_kline_start = time_last_kline_start\n",
        "        self.time_last_kline_end = time_last_kline_end\n",
        "        self.ma = ma\n",
        "        self.pmax = pmax\n",
        "        self.signals = signals\n",
        "        self.close_preds = close_preds\n",
        "\n",
        "        self.active_tickers = {}\n",
        "\n",
        "        self._smoothers       = {}    # ticker → FastRollingMode\n",
        "        self._regime_history  = {}    # ticker → deque[int]\n",
        "        self._adaptive_params = {}\n",
        "        self._regime_hist    = {}\n",
        "\n",
        "\n",
        "        self.api_limits = {\n",
        "            'remaining': 600,\n",
        "            'reset_time': None,\n",
        "            'last_request': None\n",
        "        }\n",
        "\n",
        "        # Создать папку для сохранения данных, если её нет\n",
        "        if not os.path.exists(self.data_path):\n",
        "            os.makedirs(self.data_path)\n",
        "\n",
        "    async def calculate_indicators_and_signals(self, df, ticker):\n",
        "        # 0) Читаем цену\n",
        "        high_arr = df['high'].copy()\n",
        "        low_arr = df['low'].copy()\n",
        "        close_arr = df['close'].copy()\n",
        "        vol_arr = df['volume'].copy()\n",
        "\n",
        "        logging.debug(f\"high_data: {high_arr}\")\n",
        "        logging.debug(f\"low_data: {low_arr}\")\n",
        "        logging.debug(f\"close_data: {close_arr}\")\n",
        "\n",
        "        if len(high_arr) < 2 or len(low_arr) < 2 or len(close_arr) < 2:\n",
        "            return\n",
        "\n",
        "        n = high_arr.shape[0]\n",
        "\n",
        "        # 1) Режимная часть: кластерим + сглаживаем\n",
        "        params       = ticker_params[ticker]['params']\n",
        "        # размер окна для признаков\n",
        "        window_feat  = int(params['moving_average_length'] * 9.5)\n",
        "        # формируем DataFrame-мини-окно для extract_features\n",
        "        import pandas as pd\n",
        "        hist_df = pd.DataFrame({\n",
        "            'high':   high_arr,\n",
        "            'low':    low_arr,\n",
        "            'close':  close_arr,\n",
        "            'volume': vol_arr\n",
        "        })\n",
        "        features = extract_features(hist_df, window=window_feat)  # ваша функция\n",
        "        # scale + predict\n",
        "        scaled = scaler_global.transform(features)\n",
        "        labels = kmeans_global.predict(scaled)\n",
        "\n",
        "        # сглаживаем\n",
        "        window_smooth = int(params['atr_period'] * 5.5)\n",
        "        smoother = FastRollingMode(window_size=window_smooth)\n",
        "        regimes = np.array([smoother.update(l) for l in labels], dtype=int)\n",
        "\n",
        "        # 2) Готовим базовые параметры по режиму\n",
        "        regime_params = prepare_regime_params(params)['base_params']\n",
        "\n",
        "        # 3) PRE-COMPUTE MA & ATR для каждого режима\n",
        "        ma_cache  = {}\n",
        "        atr_cache = {}\n",
        "        for regime, p in regime_params.items():\n",
        "            atype = p['average_type']\n",
        "            L     = p['moving_average_length']\n",
        "            P     = p['atr_period']\n",
        "\n",
        "            if atype == 'SMA':\n",
        "                ma = self.generateSma(high_arr, low_arr, window=L)\n",
        "            elif atype == 'VAR':\n",
        "                ma = self.generateVar(high_arr, low_arr, moving_average_length=L)\n",
        "            elif atype == 'EMA':\n",
        "                ma = self.generateEma(high_arr, low_arr, moving_average_length=L)\n",
        "            elif atype == 'AMA':\n",
        "                ama_p = p.get('ama_params', {'atr_period':14,'min_period':5,'max_period':50})\n",
        "                ma = self.generateAma(\n",
        "                    high_arr, low_arr, close_arr,\n",
        "                    atr_period=ama_p['atr_period'],\n",
        "                    min_period=ama_p['min_period'],\n",
        "                    max_period=ama_p['max_period']\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown MA type {atype!r}\")\n",
        "\n",
        "            ma_cache[regime]  = ma\n",
        "            atr_cache[regime] = self.generateAtr(high_arr, low_arr, close_arr, length=P)\n",
        "\n",
        "        # 4) MERGE по маске режимов\n",
        "        var_all = np.empty(n, dtype=float)\n",
        "        atr_all = np.empty(n, dtype=float)\n",
        "        mul_all = np.empty(n, dtype=float)\n",
        "\n",
        "        for regime, p in regime_params.items():\n",
        "            mask = (regimes == regime)\n",
        "            var_all[mask] = ma_cache[regime][mask]\n",
        "            atr_all[mask] = atr_cache[regime][mask]\n",
        "            mul_all[mask] = p['atr_multiplier']\n",
        "\n",
        "        # Заполняем NAN в начале var_all\n",
        "        if np.isnan(var_all[0]):\n",
        "            first = var_all[~np.isnan(var_all)][0]\n",
        "            var_all[np.isnan(var_all)] = first\n",
        "\n",
        "        # 5) PMax state machine\n",
        "        atr_all = np.nan_to_num(atr_all, nan=0.0)\n",
        "        first_var = var_all[~np.isnan(var_all)][0]\n",
        "        var_all = np.nan_to_num(var_all, nan=first_var)\n",
        "        pmax_all = np.empty(n, dtype=float)\n",
        "        prev_v   = var_all[0]\n",
        "        prev_a   = atr_all[0]\n",
        "        prev_m   = mul_all[0]\n",
        "        prev_fu  = prev_v + prev_m * prev_a\n",
        "        prev_fl  = prev_v - prev_m * prev_a\n",
        "        prev_p   = prev_fl\n",
        "        pmax_all[0] = prev_p\n",
        "\n",
        "        for i in range(1, n):\n",
        "            v = var_all[i]; a = atr_all[i]; m = mul_all[i]\n",
        "            bu = v + m * a\n",
        "            bl = v - m * a\n",
        "\n",
        "            fu = bu if (bu < prev_fu or prev_v > prev_fu) else prev_fu\n",
        "            fl = bl if (bl > prev_fl or prev_v < prev_fl) else prev_fl\n",
        "\n",
        "            if prev_p == prev_fu:\n",
        "                p = fu if v <= fu else fl\n",
        "            else:\n",
        "                p = fl if v >= fl else fu\n",
        "\n",
        "            pmax_all[i] = p\n",
        "            prev_v, prev_fu, prev_fl, prev_p = v, fu, fl, p\n",
        "\n",
        "        # 7. Сигналы\n",
        "        v_prev = np.concatenate(([var_all[0]], var_all[:-1]))\n",
        "        p_prev = np.concatenate(([pmax_all[0]], pmax_all[:-1]))\n",
        "        buy = (v_prev < p_prev) & (var_all > pmax_all)\n",
        "        sell = (v_prev > p_prev) & (var_all < pmax_all)\n",
        "\n",
        "\n",
        "        self.ma[ticker] = var_all\n",
        "        self.pmax[ticker] = pmax_all\n",
        "\n",
        "\n",
        "        try:\n",
        "            signal = self.generate_signal(var_all[-2:], pmax_all[-2:])\n",
        "            logging.debug(f\"Сигнал для {ticker}: {signal}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при генерации сигнала для тикера {ticker}: {e}\")\n",
        "            return\n",
        "\n",
        "        # Сохраняем сигнал\n",
        "        self.signals[ticker] = signal\n",
        "        logging.debug(f\"Сигнал сохранен для {ticker}: {signal}\")\n",
        "\n",
        "    async def calculate_indicators_and_signals1(self, df, ticker):\n",
        "        \"\"\"\n",
        "        Вычисление индикаторов и сигналов для каждого тикера.\n",
        "\n",
        "        \"\"\"\n",
        "        # Получаем последние данные для тикера\n",
        "\n",
        "        #Paramsticker_params\n",
        "        params = ticker_params[ticker]['params']\n",
        "        MOVING_AVERAGE_LENGHT = params['moving_average_length']\n",
        "        ATR_PERIOD = params['atr_period']\n",
        "        ATR_MULTIPLIER = params['atr_multiplier']\n",
        "        average_type = params['average_type']\n",
        "        if average_type == 'AMA':\n",
        "          ama_params = {\n",
        "          'atr_period': int(params['ama_atr_period']),\n",
        "          'min_period': int(params['ama_min_period']),\n",
        "          'max_period': int(params['ama_max_period'])\n",
        "          }\n",
        "\n",
        "\n",
        "        high_data = df['high'].copy()\n",
        "        low_data = df['low'].copy()\n",
        "        close_data = df['close'].copy()\n",
        "\n",
        "        logging.debug(f\"high_data: {high_data}\")\n",
        "        logging.debug(f\"low_data: {low_data}\")\n",
        "        logging.debug(f\"close_data: {close_data}\")\n",
        "\n",
        "        # Проверяем, есть ли данные\n",
        "        if high_data.empty or low_data.empty or close_data.empty:\n",
        "            logging.warning(f\"Пустые данные для тикера {ticker}\")\n",
        "            return\n",
        "\n",
        "        # Генерация индикаторов\n",
        "\n",
        "        try:\n",
        "          if average_type == 'SMA':\n",
        "              ma_data = self.generateSma(high_data, low_data, MOVING_AVERAGE_LENGHT)\n",
        "          elif average_type == 'VAR':\n",
        "              ma_data = self.generateVar(high_data, low_data, MOVING_AVERAGE_LENGHT)\n",
        "          elif average_type == 'AMA':\n",
        "              if ama_params is None:\n",
        "                  raise ValueError(\"Для AMA необходимо указать параметры ama_params.\")\n",
        "              ma_data = self.generateAma(high_data, low_data, close_data, **ama_params)\n",
        "          else:\n",
        "              raise ValueError(\"Неподдерживаемый тип скользящего среднего.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при вычислении линий средних для тикера {ticker}: {e}\")\n",
        "            return\n",
        "        try:\n",
        "            logging.debug(f\"ma_data: {ma_data}\")\n",
        "\n",
        "            pmax_data = self.generatePMax(ma_data, close_data, high_data, low_data, atr_period=ATR_PERIOD,\n",
        "                                          atr_multiplier=ATR_MULTIPLIER)\n",
        "            logging.debug(f\"pmax_data: {pmax_data}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при вычислении индикаторов для тикера {ticker}: {e}\")\n",
        "            return\n",
        "\n",
        "        # Обновляем словари с индикаторами\n",
        "        self.ma[ticker] = ma_data\n",
        "        self.pmax[ticker] = pmax_data\n",
        "\n",
        "        # Вычисляем сигналы\n",
        "        try:\n",
        "            signal = self.generate_signal(ma_data[-2:], pmax_data[-2:])\n",
        "            logging.debug(f\"Сигнал для {ticker}: {signal}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Ошибка при генерации сигнала для тикера {ticker}: {e}\")\n",
        "            return\n",
        "\n",
        "        # Сохраняем сигнал\n",
        "        self.signals[ticker] = signal\n",
        "        logging.debug(f\"Сигнал сохранен для {ticker}: {signal}\")\n",
        "\n",
        "        '''# Отправляем сигнал в Telegram если необходимо\n",
        "        if signal == 'buy':\n",
        "            send_buy_signal_to_telegram(ticker, close_data[-1])\n",
        "        elif signal == 'sell':\n",
        "            send_sell_signal_to_telegram(ticker, close_data[-1])'''\n",
        "\n",
        "    def generateVar(self, high_array, low_array, moving_average_length=14):\n",
        "        \"\"\"\n",
        "        Генерация VAR (Volatility Adjusted Ratio).\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param moving_average_length: Период для расчета VAR.\n",
        "        :return: Массив значений VAR.\n",
        "        \"\"\"\n",
        "        # Константа alpha\n",
        "        valpha = 2 / (moving_average_length + 1)\n",
        "\n",
        "        # Вычисляем среднее значение между high и low\n",
        "        hl2 = (high_array + low_array) / 2\n",
        "\n",
        "        # Вычисляем разницы для vud1 и vdd1\n",
        "        diff = np.diff(hl2, prepend=hl2[0])\n",
        "        vud1 = np.where(diff > 0, diff, 0)\n",
        "        vdd1 = np.where(diff < 0, -diff, 0)\n",
        "\n",
        "        # Функция для расчета скользящих сумм\n",
        "        def calculate_window_sums(arr, window_size=9):\n",
        "            cumsum = np.cumsum(arr)\n",
        "            return cumsum - np.concatenate((np.zeros(window_size), cumsum[:-window_size]))\n",
        "\n",
        "        # Вычисляем vUD и vDD\n",
        "        vUD = calculate_window_sums(vud1, 9)\n",
        "        vDD = calculate_window_sums(vdd1, 9)\n",
        "\n",
        "        # Вычисляем vCMO\n",
        "        epsilon = 1e-10\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            vCMO = np.divide(vUD - vDD, vUD + vDD + epsilon)\n",
        "        vCMO = np.nan_to_num(vCMO, nan=0.0)\n",
        "\n",
        "        # Вычисляем VAR\n",
        "        cmo_abs = np.abs(vCMO)\n",
        "        var = np.zeros_like(hl2)\n",
        "        var_before = 0.0\n",
        "        for i in range(len(hl2)):\n",
        "            if i < len(cmo_abs):\n",
        "                var[i] = (valpha * cmo_abs[i] * hl2[i]) + (1 - valpha * cmo_abs[i]) * var_before\n",
        "            else:\n",
        "                var[i] = var_before\n",
        "            var_before = var[i]\n",
        "        del valpha, hl2, vud1, vdd1, var_before, vUD, vDD, vCMO\n",
        "        return var\n",
        "\n",
        "    def generateAma(self, high_array, low_array, close_array, atr_period=14, min_period=5, max_period=50):\n",
        "        \"\"\"\n",
        "        Генерация адаптивного скользящего среднего на основе волатильности.\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param close_array: Массив значений close.\n",
        "        :param atr_period: Период для расчета ATR.\n",
        "        :param min_period: Минимальный период скользящего среднего.\n",
        "        :param max_period: Максимальный период скользящего среднего.\n",
        "        :return: Массив значений адаптивного скользящего среднего.\n",
        "        \"\"\"\n",
        "        # Рассчитываем ATR\n",
        "        atr = self._calculate_atr(high_array, low_array, close_array, atr_period)\n",
        "\n",
        "        # Нормализуем ATR для использования в качестве коэффициента\n",
        "        normalized_atr = (atr - np.min(atr)) / (np.max(atr) - np.min(atr) + 1e-10)\n",
        "\n",
        "        # Рассчитываем динамический период\n",
        "        dynamic_period = min_period + (max_period - min_period) * normalized_atr\n",
        "\n",
        "        # Рассчитываем адаптивное скользящее среднее (гибрид SMA и EMA)\n",
        "        adaptive_ma = np.zeros_like(close_array)\n",
        "        for i in range(len(close_array)):\n",
        "            if i < int(dynamic_period[i]):\n",
        "                adaptive_ma[i] = np.mean(close_array[:i+1])  # SMA для начальных значений\n",
        "            else:\n",
        "                period = int(dynamic_period[i])\n",
        "                alpha = 2 / (period + 1)\n",
        "                adaptive_ma[i] = alpha * close_array[i] + (1 - alpha) * adaptive_ma[i-1]  # EMA\n",
        "\n",
        "        return adaptive_ma\n",
        "\n",
        "    def _calculate_atr(self, high_array, low_array, close_array, period=14):\n",
        "        \"\"\"\n",
        "        Рассчитывает Average True Range (ATR).\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param close_array: Массив значений close.\n",
        "        :param period: Период для расчета ATR.\n",
        "        :return: Массив значений ATR.\n",
        "        \"\"\"\n",
        "        tr = np.zeros_like(high_array)\n",
        "        tr[0] = high_array[0] - low_array[0]\n",
        "\n",
        "        for i in range(1, len(high_array)):\n",
        "            hl = high_array[i] - low_array[i]\n",
        "            hc = abs(high_array[i] - close_array[i-1])\n",
        "            lc = abs(low_array[i] - close_array[i-1])\n",
        "            tr[i] = max(hl, hc, lc)\n",
        "\n",
        "        atr = np.zeros_like(tr)\n",
        "        atr[period-1] = np.mean(tr[:period])\n",
        "\n",
        "        for i in range(period, len(tr)):\n",
        "            atr[i] = (atr[i-1] * (period-1) + tr[i]) / period\n",
        "\n",
        "        return atr\n",
        "\n",
        "    def generateEma(self, high_array, low_array, moving_average_length=14):\n",
        "        \"\"\"Вычисление EMA.\"\"\"\n",
        "        if high_array.empty or low_array.empty:\n",
        "            return []\n",
        "\n",
        "        hl2 = [(high + low) / 2 for high, low in zip(high_array, low_array)]\n",
        "        ema = np.full_like(hl2, np.nan)\n",
        "        alpha = 2 / (moving_average_length + 1)\n",
        "\n",
        "        if moving_average_length <= 1:\n",
        "            return hl2\n",
        "\n",
        "        start_idx = moving_average_length - 1\n",
        "        sma = np.mean(hl2[:moving_average_length])\n",
        "        ema[start_idx] = sma\n",
        "\n",
        "        for i in range(start_idx + 1, len(hl2)):\n",
        "            ema[i] = alpha * hl2[i] + (1 - alpha) * ema[i - 1]\n",
        "\n",
        "        del hl2, alpha, start_idx, sma\n",
        "        return ema\n",
        "\n",
        "    def generateAtr(self, high_array, low_array, close_array, length=14):\n",
        "        \"\"\"Вычисление ATR.\"\"\"\n",
        "        if high_array.empty or low_array.empty or close_array.empty:\n",
        "            return []\n",
        "\n",
        "        n = len(high_array)\n",
        "        if n == 0 or length > n:\n",
        "            return []\n",
        "\n",
        "        tr = np.zeros(n)\n",
        "        atr = np.full(n, np.nan)\n",
        "\n",
        "        prev_close = np.roll(close_array, 1)\n",
        "        prev_close[0] = np.nan\n",
        "\n",
        "        tr[0] = high_array[0] - low_array[0]\n",
        "\n",
        "        for i in range(1, n):\n",
        "            hl = high_array[i] - low_array[i]\n",
        "            hc = abs(high_array[i] - prev_close[i])\n",
        "            lc = abs(low_array[i] - prev_close[i])\n",
        "            tr[i] = max(hl, hc, lc)\n",
        "\n",
        "        if n >= length:\n",
        "            atr[length - 1] = np.mean(tr[:length])\n",
        "            alpha = 1.0 / length\n",
        "            for i in range(length, n):\n",
        "                atr[i] = alpha * tr[i] + (1 - alpha) * atr[i - 1]\n",
        "\n",
        "        del n, tr, prev_close, hl, hc, lc, alpha\n",
        "        return atr\n",
        "\n",
        "    def generateSma(self, high_array, low_array, window=14):\n",
        "        if len(high_array) < window or len(low_array) < window:\n",
        "            return np.full(len(high_array), np.nan)\n",
        "\n",
        "        hl2 = (high_array + low_array) * 0.5\n",
        "        sma = np.full_like(hl2, np.nan)\n",
        "        cumsum = np.cumsum(hl2)\n",
        "        shifted_cumsum = np.zeros_like(cumsum)\n",
        "        shifted_cumsum[window:] = cumsum[:-window]\n",
        "        valid = slice(window - 1, None)\n",
        "        sma[valid] = (cumsum[valid] - shifted_cumsum[valid]) / window\n",
        "\n",
        "        return sma\n",
        "\n",
        "    def generatePMax(self, var_array, close_array, high_array, low_array, atr_period=14, atr_multiplier=3):\n",
        "        \"\"\"Вычисление PMAX.\"\"\"\n",
        "        if high_array.size==0 or low_array.size==0 or close_array.size==0 or len(var_array) == 0:\n",
        "            return []\n",
        "\n",
        "        atr = self.generateAtr(high_array, low_array, close_array, length=atr_period)\n",
        "        pmax = []\n",
        "        previous_final_upperband = 0\n",
        "        previous_final_lowerband = 0\n",
        "        previous_var = 0\n",
        "        previous_pmax = 0\n",
        "\n",
        "        for i in range(len(close_array)):\n",
        "            atrc = atr[i] if i < len(atr) and not np.isnan(atr[i]) else 0\n",
        "            varc = var_array[i] if i < len(var_array) else 0\n",
        "\n",
        "            basic_upperband = varc + atr_multiplier * atrc\n",
        "            basic_lowerband = varc - atr_multiplier * atrc\n",
        "\n",
        "            final_upperband = basic_upperband if (\n",
        "                    basic_upperband < previous_final_upperband or previous_var > previous_final_upperband) else previous_final_upperband\n",
        "            final_lowerband = basic_lowerband if (\n",
        "                    basic_lowerband > previous_final_lowerband or previous_var < previous_final_lowerband) else previous_final_lowerband\n",
        "\n",
        "            if previous_pmax == previous_final_upperband:\n",
        "                pmaxc = final_upperband if varc <= final_upperband else final_lowerband\n",
        "            else:\n",
        "                pmaxc = final_lowerband if varc >= final_lowerband else final_upperband\n",
        "\n",
        "            pmax.append(pmaxc)\n",
        "            previous_var = varc\n",
        "            previous_final_upperband = final_upperband\n",
        "            previous_final_lowerband = final_lowerband\n",
        "            previous_pmax = pmaxc\n",
        "\n",
        "        del atr, previous_final_upperband, previous_final_lowerband, previous_var, previous_pmax, pmaxc\n",
        "        return pmax\n",
        "\n",
        "    def generate_signal(self, var, pmax):\n",
        "        \"\"\"Генерация сигнала на основе VAR и PMAX.\"\"\"\n",
        "        if var.size!= 0 or not pmax:\n",
        "            return None\n",
        "\n",
        "        last_var = var[-1]\n",
        "\n",
        "        previous_var = var[-2] if len(var) >= 2 else last_var\n",
        "        last_pmax = pmax[-1]\n",
        "        previous_pmax = pmax[-2] if len(pmax) >= 2 else last_pmax\n",
        "\n",
        "        if last_var > last_pmax and previous_var < previous_pmax:\n",
        "            del last_var, previous_var, last_pmax, previous_pmax\n",
        "            return 'buy'\n",
        "        elif last_var < last_pmax and previous_var > previous_pmax:\n",
        "            del last_var, previous_var, last_pmax, previous_pmax\n",
        "            return 'sell'\n",
        "        else:\n",
        "            del last_var, previous_var, last_pmax, previous_pmax\n",
        "            return 'hold'\n",
        "\n",
        "    async def initialize_tickers(self):\n",
        "        \"\"\"Инициализация тикеров и проверка их торгового статуса.\"\"\"\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            instruments = await client.instruments.shares()\n",
        "            for share in instruments.instruments:\n",
        "                if share.ticker in self.tickers:\n",
        "                    status = await client.market_data.get_trading_status(figi=share.figi)\n",
        "                    if status.api_trade_available_flag and share.ticker in ticker_params.keys():\n",
        "                        self.ticker_to_figi[share.ticker] = share.figi\n",
        "                        self.figi_to_ticker[share.figi] = share.ticker\n",
        "                        self.active_tickers[share.ticker] = {\n",
        "                            'figi': share.figi,\n",
        "                            'name': share.name,\n",
        "                            'status': status.market_order_available_flag\n",
        "                        }\n",
        "            print(f\"[INIT] Активных тикеров: {len(self.ticker_to_figi)}\")\n",
        "\n",
        "    '''async def fetch_historical_data(self, days=365):\n",
        "        \"\"\"Получить исторические данные и обработать каждый тикер последовательно\"\"\"\n",
        "        now = dt.utcnow()\n",
        "\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            tickers_list = list(self.active_tickers.items())\n",
        "            total_tickers = len(tickers_list)\n",
        "\n",
        "            for idx, (ticker, figi) in enumerate(tickers_list, 1):\n",
        "                try:\n",
        "                    print(f\"\\nОбработка тикера {ticker} ({idx}/{total_tickers})\")\n",
        "\n",
        "                    # Загрузка данных для текущего тикера\n",
        "                    df = await self._load_ticker_data(client, figi['figi'], now, days)\n",
        "                    if df.empty:\n",
        "                        continue\n",
        "\n",
        "                    await self.calculate_indicators_and_signals(df, ticker)\n",
        "\n",
        "                    # Инициализируем списки данных\n",
        "                    self.time_last_kline_start[ticker] = [ts.isoformat() for ts in df['time'].tail(self.data_points).tolist()]\n",
        "                    self.time_last_kline_end[ticker] = [ts.isoformat() for ts in df['time_close'].tail(self.data_points).tolist()]\n",
        "                    self.open_price[ticker] = df['open'].tail(self.data_points).tolist()\n",
        "                    self.close_price[ticker] = df['close'].tail(self.data_points).tolist()\n",
        "                    self.high_price[ticker] = df['high'].tail(self.data_points).tolist()\n",
        "                    self.low_price[ticker] = df['low'].tail(self.data_points).tolist()\n",
        "                    self.volume[ticker] = df['volume'].tail(self.data_points).tolist()\n",
        "\n",
        "                    self.ma[ticker] = self.ma[ticker][-self.data_points:].tolist()\n",
        "                    self.pmax[ticker] = self.pmax[ticker][-self.data_points:]\n",
        "\n",
        "                    await self.save_data()\n",
        "\n",
        "\n",
        "\n",
        "                    # Явная очистка памяти\n",
        "                    del df\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Ошибка при обработке {ticker}: {str(e)}\")\n",
        "                finally:\n",
        "                    # Задержка для соблюдения лимитов API\n",
        "                    await asyncio.sleep(0.5)'''\n",
        "\n",
        "    async def compute_predictions_full(self, df, ticker: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Глобальный расчёт предиктов для стартового бота:\n",
        "          - не пересчитывает MA/PMax;\n",
        "          - формирует buy/sell интервалы через event_time/event_sell_time,\n",
        "            trade_bars_counter и batch, как в FeatureCalculatorForRegression;\n",
        "          - предсказания только внутри батчей (интервалы buy→sell, либо buy→конец);\n",
        "          - вне батчей — np.nan;\n",
        "          - обрезает хвост до self.data_points и сохраняет в self.predictions[ticker].\n",
        "        \"\"\"\n",
        "        try:\n",
        "            df['ma'] = self.ma[ticker]\n",
        "            df['pmax'] = self.pmax[ticker]\n",
        "\n",
        "            n_total = len(df)\n",
        "\n",
        "            if len(df) == 0:\n",
        "               self.close_preds[ticker] = np.array([], dtype=float)\n",
        "               return self.close_preds[ticker]\n",
        "\n",
        "            # 2) Режимы (кластеризация + сглаживание)\n",
        "            params_phase = phase_ticker_params[ticker]['params']\n",
        "            window_feat = int(params_phase['moving_average_length'] * 9.5)\n",
        "            scaler_global = MODELS_CACHE.scaler_global\n",
        "            kmeans_global = MODELS_CACHE.kmeans_global\n",
        "\n",
        "            # extract_features ожидает high/low/close/volume\n",
        "            feat_for_cluster = extract_features(\n",
        "                df[['high', 'low', 'close', 'volume']].copy(),\n",
        "                window=window_feat\n",
        "            )\n",
        "            if len(feat_for_cluster) == 0:\n",
        "                out = np.full(n_total, np.nan, dtype=float)\n",
        "                out = out[-self.data_points:] if hasattr(self, 'data_points') and self.data_points > 0 else out\n",
        "                if not hasattr(self, 'predictions'):\n",
        "                    self.close_preds = {}\n",
        "                self.close_preds[ticker] = out\n",
        "                return out\n",
        "\n",
        "            labels = kmeans_global.predict(scaler_global.transform(feat_for_cluster))\n",
        "            window_smooth = int(params_phase['atr_period'] * 5.5)\n",
        "            smoother = FastRollingMode(window_size=window_smooth)\n",
        "            regimes_smoothed = np.array([smoother.update(int(x)) for x in labels], dtype=int)\n",
        "            df['regime'] = regimes_smoothed\n",
        "\n",
        "            # 3) Восстановление buy/sell и событий\n",
        "            # 3.0: первичные buy/sell по ma/pmax, как в calculate_indicators_and_signals\n",
        "            v = df['ma'].to_numpy()\n",
        "            p = df['pmax'].to_numpy()\n",
        "            v_prev = np.concatenate(([v[0]], v[:-1]))\n",
        "            p_prev = np.concatenate(([p[0]], p[:-1]))\n",
        "            df['buy_signal'] = (v_prev < p_prev) & (v > p)\n",
        "            df['sell_signal'] = (v_prev > p_prev) & (v < p)\n",
        "\n",
        "            # 3.1: Проставляем event_time/event_sell_time (если ещё не готовы в вашем объекте)\n",
        "            # Если у вас уже есть эти колонки — закомментируйте этот блок и присвойте из ваших структур.\n",
        "            df['event_time'] = pd.NaT\n",
        "            df['event_price'] = np.nan\n",
        "            df['event_sell_time'] = pd.NaT\n",
        "            df['event_sell_price'] = np.nan\n",
        "\n",
        "            buy_rows = df.index[df['buy_signal'] == True]\n",
        "            sell_rows = df.index[df['sell_signal'] == True]\n",
        "            sell_times = df.loc[sell_rows, 'time']\n",
        "\n",
        "            for st in buy_rows:\n",
        "                # находим первый sell после buy\n",
        "                later_sell = sell_times[sell_times > df.at[st, 'time']]\n",
        "                if len(later_sell) > 0:\n",
        "                    end_time = later_sell.iloc[0]\n",
        "                    end_idx = int(df.index[df['time'] == end_time][0])\n",
        "                    df.at[st, 'event_time'] = df.at[st, 'time']\n",
        "                    df.at[st, 'event_price'] = df.at[st, 'close']\n",
        "                    df.at[st, 'event_sell_time'] = df.at[end_idx, 'time']\n",
        "                    df.at[st, 'event_sell_price'] = df.at[end_idx, 'close']\n",
        "                else:\n",
        "                    # активная сделка до конца — sell_time = NaT (это важно для _feat_trade_duration)\n",
        "                    df.at[st, 'event_time'] = df.at[st, 'time']\n",
        "                    df.at[st, 'event_price'] = df.at[st, 'close']\n",
        "                    # event_sell_time останется NaT\n",
        "\n",
        "            # 3.2: PnL чисто для совместимости с вашими функциями (не обязателен для прогнозов)\n",
        "            # Если есть пара buy→sell — расчёт как у вас:\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                df['pnl'] = ((df['event_sell_price'] * (1 - 0.003)) /\n",
        "                            (df['event_price'] * (1 + 0.003)) - 1) * 100\n",
        "\n",
        "            # 3.4: Нормализованный таргет + batch, как в calculate_smoothed_target_qnorm\n",
        "            # Эта функция назначает batch для каждого интервала buy→sell и создаёт normalized_target.\n",
        "            df_q = calculate_smoothed_target_qnorm(\n",
        "                df.copy(),\n",
        "                smooth_method='whittaker',\n",
        "                whittaker_lambda=10,\n",
        "                savgol_window=15,\n",
        "                savgol_poly=3,\n",
        "                per_batch_equalize=True,\n",
        "                per_batch_q=0.01\n",
        "            )\n",
        "            # переносим batch и normalized_target обратно\n",
        "            df['batch'] = df_q['batch']\n",
        "            df['normalized_target'] = df_q['normalized_target']\n",
        "\n",
        "            # 4) Готовим финальные признаки через calculate_indicators (строго как у вас)\n",
        "            final_cols = MODELS_CACHE.final_cols\n",
        "            final_params = MODELS_CACHE.final_params\n",
        "            feats_list = final_cols[ticker]\n",
        "            params = build_feature_params(final_params[ticker])\n",
        "\n",
        "            # calculate_indicators внутри себя применит:\n",
        "            # - FeatureCalculatorForRegression\n",
        "            # - отфильтрует строки по trade_bars_counter >= 0\n",
        "            # - приведет типы и удалит служебные колонки\n",
        "            gh, _ = calculate_indicators_pred(\n",
        "                df.copy(),\n",
        "                features=feats_list,\n",
        "                params=params,\n",
        "                multy=False\n",
        "            )\n",
        "\n",
        "            if len(gh) == 0:\n",
        "                out = np.full(n_total, np.nan, dtype=float)\n",
        "                out = out[-self.data_points:] if hasattr(self, 'data_points') and self.data_points > 0 else out\n",
        "                if not hasattr(self, 'predictions'):\n",
        "                    self.close_preds = {}\n",
        "                self.close_preds[ticker] = out\n",
        "                return out\n",
        "\n",
        "            # 5) Прогон через transformer\n",
        "            pipeline_trans = MODELS_CACHE.load_transformer_for(ticker, device='cpu')\n",
        "\n",
        "            # Собираем вход для transformer: feats_list + ['batch','regime','time_idx']\n",
        "            # В gh уже есть 'regime' (object) и 'batch' (int), но time мы удалили раньше — вернём time в gh\n",
        "            gh = gh.rename(columns={'time': 'time_idx'})\n",
        "\n",
        "            indexes = gh.index\n",
        "\n",
        "            columns_for_neuro = feats_list + ['batch', 'regime', 'time_idx']\n",
        "            gh_test = gh[columns_for_neuro].copy()\n",
        "\n",
        "            with warnings.catch_warnings(record=True):\n",
        "                warnings.simplefilter(\"always\")\n",
        "                predd = pipeline_trans.predict(gh_test)\n",
        "\n",
        "            gh['predd'] = np.asarray(predd, dtype=float)\n",
        "            gh['predd_shift_5'] = gh['predd'].shift(5).fillna(0.0)\n",
        "            gh['predd_pct'] = gh['predd'].pct_change(3).fillna(0.0)\n",
        "            gh['predd_var'] = gh['predd'].rolling(10).var().fillna(0.0)\n",
        "\n",
        "            # 6) Итоговая модель (LightGBM/Ranker/Combined)\n",
        "            model_data = MODELS_CACHE.load_lgb_for(ticker)\n",
        "            columns_for_model = feats_list + ['regime', 'predd', 'predd_var', 'predd_shift_5', 'predd_pct']\n",
        "            X_pred_all = gh[columns_for_model].copy()\n",
        "\n",
        "            best_method = getattr(model_data, 'best_method', 'combined')\n",
        "            if best_method == 'regressor':\n",
        "                pred_all = model_data.pipe_reg.predict(X_pred_all)\n",
        "                pred_all = apply_linear_calibration(pred_all, model_data.calib_reg)\n",
        "            elif best_method == 'ranker':\n",
        "                pred_all = model_data.pipe_rank.predict(X_pred_all)\n",
        "                pred_all = ranker_postprocess_minus1_1(pred_all)\n",
        "            elif best_method == 'combined':\n",
        "                pred_reg = model_data.pipe_reg.predict(X_pred_all)\n",
        "                pred_reg = apply_linear_calibration(pred_reg, model_data.calib_reg)\n",
        "                pred_rank = model_data.pipe_rank.predict(X_pred_all)\n",
        "                pred_rank = ranker_postprocess_minus1_1(pred_rank)\n",
        "                w = model_data.w_reg if getattr(model_data, 'w_reg', None) is not None else 0.5\n",
        "                pred_all = pred_reg * w + pred_rank * (1 - w)\n",
        "            else:\n",
        "                pred_all = np.full(len(X_pred_all), np.nan, dtype=float)\n",
        "\n",
        "            pred_all = np.asarray(pred_all, dtype=float).ravel()\n",
        "            # 7) Собираем выход по всей длине ряда и маскируем вне батчей\n",
        "            # Индексы gh соответствуют только строкам trade_bars_counter >= 0 (внутри сделок)\n",
        "            out_full = np.full(n_total, np.nan, dtype=float)\n",
        "\n",
        "            # Нам нужно сопоставить gh-строки к исходным индексам df.\n",
        "            # В FeatureCalculatorForRegression строки отбрасывались, но порядок сохранялся, поэтому\n",
        "            # построим позиционное соответствие через time_idx.\n",
        "            gh_times = pd.to_datetime(gh['time_idx'], utc=True)\n",
        "            df_times = df['time']\n",
        "            # Быстрый маппинг времени -> индекс df\n",
        "            pos_in_df = pd.Index(df_times).get_indexer(gh_times)\n",
        "            valid = pos_in_df >= 0\n",
        "            out_full[pos_in_df[valid]] = pred_all[valid]\n",
        "\n",
        "            self.close_preds[ticker] = out_full\n",
        "\n",
        "        except Exception as e:\n",
        "            # Лог и безопасный возврат\n",
        "            logging.exception(f\"[{ticker}] Ошибка compute_predictions_full: {e}\")\n",
        "            if not hasattr(self, 'predictions'):\n",
        "                self.close_preds = {}\n",
        "            self.close_preds[ticker] = np.array([], dtype=float)\n",
        "            return self.close_preds[ticker]\n",
        "\n",
        "\n",
        "    def _compute_buy_sell_ranges_from_ma_pmax(self, ma_arr: Sequence[float], pmax_arr: Sequence[float]) -> list[tuple[int, typing.Optional[int]]]:\n",
        "        \"\"\"\n",
        "        Восстанавливает интервалы сделок по логике пересечений var=pmax:\n",
        "        - buy, когда var пересекает pmax снизу вверх (v_prev < p_prev) & (v_now > p_now)\n",
        "        - sell, когда var пересекает pmax сверху вниз (v_prev > p_prev) & (v_now < p_now)\n",
        "        Возвращает список кортежей (buy_idx, sell_idx_or_None), где sell_idx None если сделка не закрыта до конца.\n",
        "        \"\"\"\n",
        "        v = np.asarray(ma_arr, dtype=float)\n",
        "        p = np.asarray(pmax_arr, dtype=float)\n",
        "        n = len(v)\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        v_prev = np.concatenate(([v[0]], v[:-1]))\n",
        "        p_prev = np.concatenate(([p[0]], p[:-1]))\n",
        "\n",
        "        buy_mask = (v_prev < p_prev) & (v > p)\n",
        "        sell_mask = (v_prev > p_prev) & (v < p)\n",
        "\n",
        "        buy_idx = np.where(buy_mask)[0].tolist()\n",
        "        sell_idx = np.where(sell_mask)[0].tolist()\n",
        "\n",
        "        ranges = []\n",
        "        si = 0\n",
        "        for b in buy_idx:\n",
        "            # ищем первый sell строго после b\n",
        "            s = None\n",
        "            while si < len(sell_idx) and sell_idx[si] <= b:\n",
        "                si += 1\n",
        "            if si < len(sell_idx):\n",
        "                s = sell_idx[si]\n",
        "                si += 1\n",
        "            ranges.append((b, s))\n",
        "        # если первых событий sell больше, чем buy, они игнорируются (нет открытой сделки до них)\n",
        "        return ranges\n",
        "\n",
        "    async def fetch_historical_data(self, days=800):\n",
        "          \"\"\"Получить исторические данные и обработать каждый тикер с контролем длительности.\"\"\"\n",
        "          now = dt.utcnow()\n",
        "\n",
        "          async with AsyncClient(self.token) as client:\n",
        "              tickers_list = list(self.active_tickers.items())\n",
        "              total_tickers = len(tickers_list)\n",
        "\n",
        "              for idx, (ticker, figi) in enumerate(tickers_list, 1):\n",
        "                  start_time = time.monotonic()  # Зафиксируем старт времени\n",
        "\n",
        "                  try:\n",
        "                      print(f\"\\nОбработка тикера {ticker} ({idx}/{total_tickers})\")\n",
        "\n",
        "                      # Загрузка данных\n",
        "                      df = await self._load_ticker_data(client, figi['figi'], now, days)\n",
        "                      if df.empty:\n",
        "                          continue\n",
        "\n",
        "                      await self.calculate_indicators_and_signals(df, ticker)\n",
        "\n",
        "                      await self.compute_predictions_full(df, ticker)\n",
        "\n",
        "                      # Инициализация списков\n",
        "                      self.time_last_kline_start[ticker] = [ts.isoformat() for ts in df['time'].tail(self.data_points).tolist()]\n",
        "                      self.time_last_kline_end[ticker] = [ts.isoformat() for ts in df['time_close'].tail(self.data_points).tolist()]\n",
        "                      self.open_price[ticker] = df['open'].tail(self.data_points).tolist()\n",
        "                      self.close_price[ticker] = df['close'].tail(self.data_points).tolist()\n",
        "                      self.high_price[ticker] = df['high'].tail(self.data_points).tolist()\n",
        "                      self.low_price[ticker] = df['low'].tail(self.data_points).tolist()\n",
        "                      self.volume[ticker] = df['volume'].tail(self.data_points).tolist()\n",
        "\n",
        "                      self.ma[ticker] = self.ma[ticker][-self.data_points:].tolist()\n",
        "                      self.pmax[ticker] = self.pmax[ticker][-self.data_points:].tolist()\n",
        "\n",
        "                      self.close_preds[ticker] = self.close_preds[ticker][-self.data_points:].tolist()\n",
        "\n",
        "                      await self.save_data()\n",
        "\n",
        "                      del df  # Очистка памяти\n",
        "\n",
        "                  except Exception as e:\n",
        "                      print(f\"Ошибка при обработке {ticker}: {str(e)}\")\n",
        "\n",
        "                  finally:\n",
        "                      elapsed = time.monotonic() - start_time\n",
        "                      remaining = 60 - elapsed\n",
        "                      if remaining > 0:\n",
        "                          # Обеспечим, чтобы обработка одного тикера занимала не менее 60 секунд\n",
        "                          await asyncio.sleep(remaining)\n",
        "\n",
        "\n",
        "\n",
        "    async def _load_ticker_data(self, client, figi, now, days, retries=3):\n",
        "        candles = []\n",
        "        total_blocks = (days // 90) + (1 if days % 90 else 0)\n",
        "\n",
        "        for block in range(total_blocks):\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    from_time = now - timedelta(days=(block+1)*90)\n",
        "                    to_time = now - timedelta(days=block*90)\n",
        "\n",
        "                    async for candle in client.get_all_candles(\n",
        "                        figi=figi,\n",
        "                        from_=from_time,\n",
        "                        to=to_time,\n",
        "                        interval=self.interval\n",
        "                    ):\n",
        "                        candles.append({\n",
        "                            \"time\": candle.time + timedelta(hours=3),\n",
        "                            \"time_close\": candle.time + timedelta(hours=3)+timedelta(minutes=self.timeframe_minutes),\n",
        "                            \"open\": float(candle.open.units + candle.open.nano * 1e-9),\n",
        "                            \"close\": float(candle.close.units + candle.close.nano * 1e-9),\n",
        "                            \"high\": float(candle.high.units + candle.high.nano * 1e-9),\n",
        "                            \"low\": float(candle.low.units + candle.low.nano * 1e-9),\n",
        "                            \"volume\": candle.volume,\n",
        "                        })\n",
        "\n",
        "                    break  # успешная попытка — выходим из цикла retry\n",
        "                except RequestError as e:\n",
        "                    if e.status == StatusCode.INTERNAL and attempt < retries - 1:\n",
        "                        logging.warning(f\"[{figi}] INTERNAL ERROR. Повтор {attempt+1}/{retries}\")\n",
        "                        await asyncio.sleep(1 + attempt)\n",
        "                    else:\n",
        "                        raise\n",
        "        return self._clean_data(candles)\n",
        "\n",
        "    async def _load_ticker_data_back(self, client, figi, now, days):\n",
        "        \"\"\"Загрузка данных для одного тикера\"\"\"\n",
        "        candles = []\n",
        "        total_blocks = (days // 90) + (1 if days % 90 else 0)\n",
        "\n",
        "        try:\n",
        "            for block in range(total_blocks):\n",
        "                from_time = now - timedelta(days=(block+1)*90)\n",
        "                to_time = now - timedelta(days=block*90)\n",
        "\n",
        "                async for candle in client.get_all_candles(\n",
        "                    figi=figi,\n",
        "                    from_=from_time,\n",
        "                    to=to_time,\n",
        "                    interval=self.interval\n",
        "                ):\n",
        "                    candles.append({\n",
        "                        \"time\": candle.time + timedelta(hours=3),\n",
        "                        \"time_close\": candle.time + timedelta(hours=3)+timedelta(minutes=self.timeframe_minutes),\n",
        "                        \"open\": float(candle.open.units + candle.open.nano * 1e-9),\n",
        "                        \"close\": float(candle.close.units + candle.close.nano * 1e-9),\n",
        "                        \"high\": float(candle.high.units + candle.high.nano * 1e-9),\n",
        "                        \"low\": float(candle.low.units + candle.low.nano * 1e-9),\n",
        "                        \"volume\": candle.volume,\n",
        "                    })\n",
        "\n",
        "                # Обновление прогресса\n",
        "                print(f\"Загружено блоков: {block+1}/{total_blocks}\", end='\\r')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка загрузки данных: {str(e)}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        return self._clean_data(candles)\n",
        "\n",
        "    def _clean_data(self, candles):\n",
        "        \"\"\"Очистка и преобразование данных\"\"\"\n",
        "        if not candles:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame(candles)\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        return df.sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "    async def process_candle_stream(self, candle: Candle):\n",
        "        \"\"\"Обработка новых свечей в реальном времени.\"\"\"\n",
        "        ticker = self.figi_to_ticker.get(candle.figi)\n",
        "        if not ticker:\n",
        "            return\n",
        "\n",
        "\n",
        "        ts = int(candle.time.timestamp() * 1000)\n",
        "        self.open_price[ticker] = self.open_price[ticker][1:] + [float(candle.open.units + candle.open.nano * 1e-9)]\n",
        "        self.close_price[ticker] = self.close_price[ticker][1:] + [float(candle.close.units + candle.close.nano * 1e-9)]\n",
        "        self.high_price[ticker] = self.high_price[ticker][1:] + [float(candle.high.units + candle.high.nano * 1e-9)]\n",
        "        self.low_price[ticker] = self.low_price[ticker][1:] + [float(candle.low.units + candle.low.nano * 1e-9)]\n",
        "        self.volume[ticker] = self.volume[ticker][1:] + [candle.volume]\n",
        "        self.time_last_kline_start[ticker] = time_last_kline_start[ticker][1:] + [ts]\n",
        "        self.time_last_kline_end[ticker] = self.time_last_kline_end[ticker][1:] + [ts + 60000 * self.timeframe_minutes]\n",
        "\n",
        "        print(f\"[{ticker}] Time: {candle.time:%Y-%m-%d %H:%M} \"\n",
        "              f\"O: {self.open_price[ticker][-1]:.2f} H: {self.high_price[ticker][-1]:.2f} \"\n",
        "              f\"L: {self.low_price[ticker][-1]:.2f} C: {self.close_price[ticker][-1]:.2f}\")\n",
        "\n",
        "    async def candle_stream_handler(self):\n",
        "        \"\"\"Обработка реального потока данных.\"\"\"\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            figi_list = list(self.figi_to_ticker.keys())\n",
        "\n",
        "            async def request_iterator():\n",
        "                yield MarketDataRequest(\n",
        "                    subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                        subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_SUBSCRIBE,\n",
        "                        instruments=[\n",
        "                            CandleInstrument(\n",
        "                                figi=figi,\n",
        "                                interval=self.interval\n",
        "                            ) for figi in figi_list\n",
        "                        ],\n",
        "                        waiting_close=True\n",
        "                    )\n",
        "                )\n",
        "                while True:\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            try:\n",
        "                stream = client.market_data_stream.market_data_stream(request_iterator())\n",
        "                async for response in stream:\n",
        "                    if response.candle:\n",
        "                        await self.process_candle_stream(response.candle)\n",
        "            except Exception as e:\n",
        "                print(f\"[Stream Error] {e}\")\n",
        "                await asyncio.sleep(10)\n",
        "\n",
        "    async def save_data(self):\n",
        "        \"\"\"Save data to files periodically.\"\"\"\n",
        "        try:\n",
        "            with open(f'{self.data_path}open_price.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.open_price))\n",
        "\n",
        "            with open(f'{self.data_path}close_price.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.close_price))\n",
        "\n",
        "            with open(f'{self.data_path}high_price.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.high_price))\n",
        "\n",
        "            with open(f'{self.data_path}low_price.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.low_price))\n",
        "\n",
        "            with open(f'{self.data_path}volume.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.volume))\n",
        "\n",
        "            with open(f'{self.data_path}ma.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.ma))\n",
        "\n",
        "            with open(f'{self.data_path}pmax.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.pmax))\n",
        "\n",
        "            with open(f'{self.data_path}close_preds.txt', 'w') as f:\n",
        "                f.write(json.dumps(self.close_preds))\n",
        "\n",
        "            with open(f'{self.data_path}time_last_kline_start.txt', 'w') as f:\n",
        "                json.dump(self.time_last_kline_start, f)\n",
        "\n",
        "            with open(f'{self.data_path}time_last_kline_end.txt', 'w') as f:\n",
        "                json.dump(self.time_last_kline_end, f)\n",
        "\n",
        "            print(\"Данные успешно сохранены.\")\n",
        "        except Exception as e:\n",
        "            print(f'Ошибка сохранения в {e}')\n",
        "            await asyncio.sleep(0.5)\n",
        "\n",
        "    async def run(self):\n",
        "        \"\"\"Запуск бота.\"\"\"\n",
        "\n",
        "        await self.initialize_tickers()\n",
        "        MODELS_CACHE.init_metadata(path_to_save=self.data_path)\n",
        "        await self.fetch_historical_data()\n",
        "\n",
        "\n",
        "# Запуск бота\n",
        "if __name__ == \"__main__\":\n",
        "    bot = TradingBot(\n",
        "        token=TOKEN,\n",
        "        tickers=TICKERS,\n",
        "        data_path=DATA_PATH,\n",
        "        interval=CANDLE_INTERVAL,\n",
        "        timeframe_minutes=TIMEFRAME_MINUTES,\n",
        "        data_points=HISTORY_DATA_POINTS\n",
        "    )\n",
        "    asyncio.run(bot.run())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FhlkyZXiq94"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dmu2HvYTI77CkgCzZ9gSxz4FHKD08DcI",
      "authorship_tag": "ABX9TyM8xsM4KZ4IUuBXtuS9fuP6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}