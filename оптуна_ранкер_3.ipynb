{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6112e23-5d8a-45e6-978a-2d4acd10a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "import gc\n",
    "import inspect\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import lzma\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import psutil\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgm\n",
    "import matplotlib.pyplot as plt\n",
    "import os, gc, warnings, typing as tp\n",
    "import mplfinance as mpf\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pytorch_lightning as pl\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import statsmodels.api as sm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from collections import defaultdict, deque\n",
    "from collections.abc import Mapping, Sequence\n",
    "from typing import Optional, Literal\n",
    "from contextlib import redirect_stderr, redirect_stdout\n",
    "from io import StringIO\n",
    "from joblib import Parallel, delayed\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from lightgbm import LGBMRanker\n",
    "from numba import jit, njit\n",
    "from optuna.trial import FrozenTrial\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import EncoderNormalizer\n",
    "from pytorch_forecasting.metrics import Metric, QuantileLoss\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.progress import RichProgressBar, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from scipy import stats\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm, pearsonr\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score, get_scorer, ndcg_score\n",
    "from sklearn.model_selection import BaseCrossValidator, GroupKFold, KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torchmetrics.functional import mean_squared_error as torchmetrics_mse\n",
    "from tqdm.auto import tqdm as tqdm_class\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Dict, Iterable, Iterator, List, Literal, Mapping, Optional, Tuple\n",
    "from xgboost import XGBRegressor\n",
    "os.environ['OMP_NUM_THREADS'] = '3'\n",
    "os.environ['MKL_NUM_THREADS'] = '8'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '8'\n",
    "#torch.set_num_threads(8)\n",
    "#torch.set_num_interop_threads(8)\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "def to_dense(X):\n",
    "    \"\"\"Преобразует разреженную матрицу в плотную.\"\"\"\n",
    "    if issparse(X):\n",
    "        return X.toarray()\n",
    "    return X\n",
    "    \n",
    "class ToDenseTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Преобразует разреженную матрицу в плотную numpy-массив.\"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        return X\n",
    "    def get_feature_names_out(self, input_features: tp.Sequence[str] | None = None):\n",
    "        return np.asarray(input_features) if input_features is not None else np.array([])\n",
    "\n",
    "\n",
    "PARALLEL_FILES = 1                         # ← меняйте при желании\n",
    "N_JOBS  = max(os.cpu_count() // PARALLEL_FILES-1, 1)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1.  RareCategoryGrouper – вместо FunctionTransformer(\"rare\")\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Объединяет редкие категории (freq < threshold) в '__RARE__'.\n",
    "    Приводит все категории к str, чтобы не было смешения типов.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold: float = .01):\n",
    "        self.threshold = threshold\n",
    "        self._levels_: list[pd.Index] = []\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        # Обязательно приводим к строкам\n",
    "        X_str = X.astype(str)\n",
    "        self._levels_ = [\n",
    "            X_str[col].value_counts(normalize=True)[lambda s: s >= self.threshold].index\n",
    "            for col in X_str.columns\n",
    "        ]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        X_new = X.astype(str).copy()          # ← всё превращаем в str\n",
    "        for col, keep in zip(X_new.columns, self._levels_):\n",
    "            X_new[col] = np.where(\n",
    "                X_new[col].isin(keep), X_new[col], \"__RARE__\"\n",
    "            )\n",
    "        return X_new\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.asarray(input_features) if input_features is not None else np.array([])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2.  FrequencyEncoder с методом get_feature_names_out\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class FrequencyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.maps_ = [X[c].value_counts(normalize=True).to_dict() for c in X.columns]\n",
    "        self.feature_names_in_ = list(X.columns)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        cols = [X[c].map(m).fillna(0.).to_numpy(float) for c, m in zip(X.columns, self.maps_)]\n",
    "        return np.vstack(cols).T\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return np.array(self.feature_names_in_)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3.  Конструктор препроцессора\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "\n",
    "to_dense_tr = FunctionTransformer(to_dense, feature_names_out=\"one-to-one\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "def build_preprocessor(\n",
    "    X: pd.DataFrame,\n",
    "    rare_thr: float = .01,\n",
    "    high_card_thr: int = 20\n",
    ") -> ColumnTransformer:\n",
    "\n",
    "    num_cols = [c for c in make_column_selector(dtype_include=np.number)(X) if c != \"batch\"]\n",
    "    cat_cols = [c for c in make_column_selector(dtype_include=object)(X)  if c != \"batch\"]\n",
    "\n",
    "    low_card, high_card = [], []\n",
    "    for c in cat_cols:\n",
    "        (high_card if X[c].nunique() >= high_card_thr else low_card).append(c)\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"sc\",  StandardScaler())\n",
    "    ])\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "\n",
    "    low_cat_pipe = Pipeline([\n",
    "        (\"to_str\", FunctionTransformer(lambda df: df.astype(str), feature_names_out=\"one-to-one\")),\n",
    "        #(\"rare\",  RareCategoryGrouper(threshold=rare_thr)),\n",
    "        (\"imp\",   SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\",   ohe)\n",
    "    ])\n",
    "\n",
    "    high_cat_pipe = Pipeline([\n",
    "        (\"to_str\", FunctionTransformer(lambda df: df.astype(str), feature_names_out=\"one-to-one\")),\n",
    "        #(\"rare\",  RareCategoryGrouper(threshold=rare_thr)),\n",
    "        (\"imp\",   SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"freq\",  FrequencyEncoder())          # → dense вектор размера 1\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        [(\"num\",   num_pipe,     num_cols),\n",
    "         (\"lowc\",  low_cat_pipe, low_card),\n",
    "         (\"highc\", high_cat_pipe, high_card)],\n",
    "        remainder=\"drop\",\n",
    "        n_jobs=1,\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "\n",
    "def _to_dense(x):\n",
    "    if issparse(x):\n",
    "        return x.toarray()\n",
    "    return np.asarray(x)\n",
    "\n",
    "# Modernized feature_ranking for multioutput (optimized for memory)\n",
    "def feature_ranking_multioutput(\n",
    "    df: pd.DataFrame,\n",
    "    target_cols: Sequence[str],\n",
    "    group_col: str = \"batch\",\n",
    "    top_k: int = 50,\n",
    "    *,\n",
    "    top_pi_feats: Optional[int] = 300,  # Reduced default to save memory/compute\n",
    "    val_frac: float = .10,  # Reduced to save memory in SHAP\n",
    "    max_shap_samples: int = 1000,  # Limit SHAP computation to this many samples\n",
    "    parallel_fit: bool = False,\n",
    "    n_estimators: int = 100  # Reduced for faster fitting and less memory\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ранжирование признаков для multi-output регрессии.\n",
    "    Возвращает DataFrame с важностью и подробными Δ-метриками.\n",
    "    Оптимизировано для снижения потребления памяти.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- отделяем таргеты\n",
    "    df = df.copy().dropna(subset=target_cols)\n",
    "    y = df[target_cols].reset_index(drop=True).to_numpy(float)   # (n, K)\n",
    "    df.drop(columns=target_cols, inplace=True)\n",
    "    groups = df[group_col] if group_col in df.columns else None\n",
    "\n",
    "    # --- препроцессинг (keep sparse if possible, convert to dense only for folds)\n",
    "    pre = build_preprocessor(df)\n",
    "    Xall = pre.fit_transform(df)\n",
    "    feat_names = pre.get_feature_names_out()\n",
    "\n",
    "    # --- модели (only necessary ones: LGBM for perm/SHAP, Ridge for coef)\n",
    "    MODELS: dict[str, object] = {\n",
    "        \"LGBM\": MultiOutputRegressor(\n",
    "                    LGBMRegressor(\n",
    "                        n_estimators=n_estimators, learning_rate=.05,\n",
    "                        subsample=.8, colsample_bytree=.8,\n",
    "                        random_state=RANDOM_STATE, n_jobs=1,\n",
    "                        verbosity=-1\n",
    "                    ), n_jobs=1),\n",
    "        \"Ridge\": Ridge(alpha=1.0)\n",
    "    }\n",
    "\n",
    "    # --- контейнеры\n",
    "    pi_mix, pi_mse, pi_r2, pi_corr = [], [], [], []\n",
    "    shap_all, coef_all = [], []\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "\n",
    "    # функция корреляции по каждому таргету\n",
    "    def _corr_multi(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        corrs = []\n",
    "        for j in range(y_true.shape[1]):\n",
    "            c = np.corrcoef(y_true[:, j], y_pred[:, j])[0, 1]\n",
    "            corrs.append(-1. if np.isnan(c) else c)\n",
    "        return np.mean(corrs)\n",
    "\n",
    "    # --- кросс-валидация\n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(Xall, y, groups=groups)):\n",
    "        # Convert to dense per fold to manage memory\n",
    "        Xtr = _to_dense(Xall[tr_idx])\n",
    "        Xval = _to_dense(Xall[val_idx])\n",
    "        ytr, yval = y[tr_idx], y[val_idx]\n",
    "\n",
    "        # ---------- ФИТ МОДЕЛЕЙ --------------\n",
    "        if parallel_fit:\n",
    "            trained = dict(joblib.Parallel(\n",
    "                              n_jobs=min(3, len(MODELS)), backend=\"threading\")(\n",
    "                joblib.delayed(lambda nm, mdl: (nm, clone(mdl).fit(Xtr, ytr)))(\n",
    "                    nm, mdl) for nm, mdl in MODELS.items()\n",
    "            ))\n",
    "        else:\n",
    "            trained = {nm: clone(mdl).fit(Xtr, ytr) for nm, mdl in MODELS.items()}\n",
    "\n",
    "        # ------------- SHAP -----------------\n",
    "        # усредняем |SHAP| по таргетам и объектам, limit sample size\n",
    "        shap_fold = np.zeros(Xval.shape[1])\n",
    "        m = min(max_shap_samples, math.ceil(len(Xval) * val_frac))\n",
    "        X_shap = Xval[:m]\n",
    "        for est_idx, est in enumerate(trained[\"LGBM\"].estimators_):\n",
    "            expl = shap.TreeExplainer(est, feature_perturbation=\"tree_path_dependent\")\n",
    "            sv = expl.shap_values(X_shap, check_additivity=False)  # (m, F)\n",
    "            shap_fold += np.abs(sv).mean(axis=0)\n",
    "            del expl, sv  # Free memory immediately\n",
    "            gc.collect()\n",
    "        shap_all.append(shap_fold / len(trained[\"LGBM\"].estimators_))\n",
    "\n",
    "        # ------------- Ridge коэффициенты ----\n",
    "        coef = np.abs(trained[\"Ridge\"].coef_)\n",
    "        if coef.ndim == 2:\n",
    "            coef = coef.mean(axis=0)\n",
    "        coef_all.append(coef)\n",
    "\n",
    "        # ------------- baseline метрики -------\n",
    "        y_pred_base = trained[\"LGBM\"].predict(Xval)                    # (n, K)\n",
    "        base_mse = mean_squared_error(yval, y_pred_base, multioutput=\"raw_values\").mean()\n",
    "        base_r2 = r2_score(yval, y_pred_base, multioutput=\"raw_values\").mean()\n",
    "        base_corr = _corr_multi(yval, y_pred_base)\n",
    "        del y_pred_base\n",
    "        gc.collect()\n",
    "\n",
    "        # ------------- permutation importance -\n",
    "        variances = np.array(Xtr.var(axis=0)).ravel()\n",
    "        order = np.argsort(variances)[::-1][: (top_pi_feats or len(variances))]\n",
    "\n",
    "        fold_mix = np.zeros(Xval.shape[1])\n",
    "        fold_mse = np.zeros_like(fold_mix)\n",
    "        fold_r2 = np.zeros_like(fold_mix)\n",
    "        fold_corr = np.zeros_like(fold_mix)\n",
    "\n",
    "        rng = np.random.default_rng(RANDOM_STATE)\n",
    "        for pi_idx, idx in enumerate(order):\n",
    "            original_col = Xval[:, idx].copy()\n",
    "            rng.shuffle(Xval[:, idx])  # Shuffle in place\n",
    "            y_perm = trained[\"LGBM\"].predict(Xval)\n",
    "\n",
    "            new_mse = mean_squared_error(yval, y_perm, multioutput=\"raw_values\").mean()\n",
    "            new_r2 = r2_score(yval, y_perm, multioutput=\"raw_values\").mean()\n",
    "            new_corr = _corr_multi(yval, y_perm)\n",
    "\n",
    "            d_mse = new_mse - base_mse\n",
    "            d_r2 = base_r2 - new_r2\n",
    "            d_corr = base_corr - new_corr\n",
    "\n",
    "            fold_mse[idx] = d_mse / (abs(base_mse) + 1e-12)\n",
    "            fold_r2[idx] = d_r2\n",
    "            fold_corr[idx] = d_corr\n",
    "            fold_mix[idx] = (fold_mse[idx] + d_r2 + d_corr) / 3\n",
    "\n",
    "            # Restore original column\n",
    "            Xval[:, idx] = original_col\n",
    "\n",
    "            # Periodic garbage collection to manage memory\n",
    "            if (pi_idx + 1) % 50 == 0:\n",
    "                del y_perm\n",
    "                gc.collect()\n",
    "\n",
    "        pi_mix.append(old_mix)\n",
    "        pi_mse.append(fold_mse)\n",
    "        pi_r2.append(fold_r2)\n",
    "        pi_corr.append(fold_corr)\n",
    "\n",
    "        del trained, Xtr, Xval, ytr, yval\n",
    "        gc.collect()\n",
    "\n",
    "    # --- агрегируем фолды\n",
    "    mean_ = lambda lst: np.stack(lst).mean(axis=0)\n",
    "    mix_m, mse_m, r2_m, corr_m = map(mean_, (pi_mix, pi_mse, pi_r2, pi_corr))\n",
    "    shap_m, coef_m = map(mean_, (shap_all, coef_all))\n",
    "\n",
    "    # --- финальный комбинированный скор\n",
    "    z = lambda x: (x - x.mean()) / (x.std() + 1e-9)\n",
    "    final_score = np.nanmean(np.vstack([z(mix_m), z(shap_m), z(coef_m)]), axis=0)\n",
    "\n",
    "    res = (pd.DataFrame(dict(\n",
    "             feature    = feat_names,\n",
    "             Δmse       = mse_m,\n",
    "             Δr2        = r2_m,\n",
    "             Δcorr      = corr_m,\n",
    "             perm_mix   = mix_m,\n",
    "             shap       = shap_m,\n",
    "             ridge_coef = coef_m,\n",
    "             importance = final_score))\n",
    "           .sort_values(\"importance\", ascending=False)\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    return res\n",
    "\n",
    "def feature_ranking(\n",
    "    df: pd.DataFrame,\n",
    "    target_col : str = \"target\",\n",
    "    group_col  : str = \"batch\",\n",
    "    top_k      : int = 50,\n",
    "    *,\n",
    "    top_pi_feats : Optional[int] = 800,\n",
    "    val_frac     : float = .30\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Важность = усреднение четырёх каналов:\n",
    "        1. perm_mix  – средний ущерб трёх метрик (MSE, R², Corr)\n",
    "        2. SHAP      – |SHAP|\n",
    "        3. Ridge      |coef|\n",
    "        4. (опционально) можно добавить любой другой канал\n",
    "    Возвращает top_k фичей + подробные Δ-метрики.\n",
    "    \"\"\"\n",
    "    df = df.copy().dropna(subset=[target_col])\n",
    "    y  = df.pop(target_col).reset_index(drop=True)\n",
    "    g  = df[group_col].reset_index(drop=True) if group_col in df.columns else None\n",
    "\n",
    "    pre = build_preprocessor(df)\n",
    "    Xt  = pre.fit_transform(df, y)\n",
    "    f_names = pre.get_feature_names_out()\n",
    "\n",
    "    MODELS: Dict[str, object] = {\n",
    "        \"LGBM\": LGBMRegressor(\n",
    "                    n_estimators=250, learning_rate=.05,\n",
    "                    subsample=.8, colsample_bytree=.8,\n",
    "                    random_state=RANDOM_STATE, n_jobs=1, verbosity=-1),\n",
    "        \"HGBR\": HistGradientBoostingRegressor(\n",
    "                    max_depth=None, learning_rate=.06, l2_regularization=1.,\n",
    "                    max_iter=250, random_state=RANDOM_STATE,\n",
    "                    categorical_features=None),       # ← фиксация бага\n",
    "        \"RF\"  : RandomForestRegressor(\n",
    "                    n_estimators=350, max_features=\"sqrt\",\n",
    "                    random_state=RANDOM_STATE, n_jobs=1),\n",
    "        \"Ridge\": Ridge(alpha=1.0)\n",
    "    }\n",
    "\n",
    "    # контейнеры\n",
    "    pi_mix, pi_mse, pi_r2, pi_corr = [], [], [], []\n",
    "    shap_all, coef_all = [], []\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "\n",
    "    # вспом. функция корреляции\n",
    "    def _corr(a, b):\n",
    "        c = np.corrcoef(a, b)[0, 1]\n",
    "        return -1. if np.isnan(c) else c\n",
    "\n",
    "    # параллельный фит одного алгоритма\n",
    "    def _fit(name, mdl, Xtr, ytr):\n",
    "        return name, mdl.fit(Xtr, ytr)\n",
    "\n",
    "    for tr_idx, val_idx in gkf.split(Xt, y, groups=g):\n",
    "        Xtr, Xval = Xt[tr_idx], Xt[val_idx]\n",
    "        ytr, yval = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "        trained = dict(joblib.Parallel(n_jobs=len(MODELS))(\n",
    "            joblib.delayed(_fit)(nm, mdl, Xtr, ytr)\n",
    "            for nm, mdl in MODELS.items()\n",
    "        ))\n",
    "\n",
    "        # ─── SHAP\n",
    "        expl = shap.TreeExplainer(trained[\"LGBM\"], feature_perturbation=\"tree_path_dependent\")\n",
    "        m = math.ceil(len(Xval) * val_frac)\n",
    "        shap_vals = expl.shap_values(Xval[:m], check_additivity=False)\n",
    "        shap_all.append(np.abs(shap_vals).mean(axis=0))\n",
    "\n",
    "        # ─── Ridge\n",
    "        coef_all.append(np.abs(trained[\"Ridge\"].coef_))\n",
    "\n",
    "        # ─── Baseline метрики\n",
    "        y_pred_base = trained[\"LGBM\"].predict(Xval)\n",
    "        base_mse  = mean_squared_error(yval, y_pred_base)\n",
    "        base_r2   = r2_score(yval, y_pred_base)\n",
    "        base_corr = _corr(yval, y_pred_base)\n",
    "\n",
    "        # ─── Permutation importance по top-N\n",
    "        variances = np.array(Xtr.var(axis=0)).ravel()\n",
    "        order = np.argsort(variances)[::-1][: (top_pi_feats or len(variances))]\n",
    "\n",
    "        fold_mix  = np.zeros(Xval.shape[1])\n",
    "        fold_mse  = np.zeros_like(fold_mix)\n",
    "        fold_r2   = np.zeros_like(fold_mix)\n",
    "        fold_corr = np.zeros_like(fold_mix)\n",
    "\n",
    "        rng = np.random.default_rng(RANDOM_STATE)\n",
    "        for idx in order:\n",
    "            col = Xval[:, idx].copy()\n",
    "            rng.shuffle(col)\n",
    "            Xperm = Xval.copy()\n",
    "            Xperm[:, idx] = col\n",
    "            y_perm = trained[\"LGBM\"].predict(Xperm)\n",
    "\n",
    "            new_mse  = mean_squared_error(yval, y_perm)\n",
    "            new_r2   = r2_score(yval, y_perm)\n",
    "            new_corr = _corr(yval, y_perm)\n",
    "\n",
    "            d_mse  = new_mse  - base_mse          # ↑ плохое\n",
    "            d_r2   = base_r2  - new_r2            # ↑ плохое\n",
    "            d_corr = base_corr - new_corr         # ↑ плохое\n",
    "\n",
    "            fold_mse[idx]  = d_mse  / (abs(base_mse) + 1e-12)   # нормируем\n",
    "            fold_r2[idx]   = d_r2\n",
    "            fold_corr[idx] = d_corr\n",
    "            fold_mix[idx]  = (fold_mse[idx] + d_r2 + d_corr) / 3\n",
    "\n",
    "        pi_mix.append(fold_mix)\n",
    "        pi_mse.append(fold_mse)\n",
    "        pi_r2.append(fold_r2)\n",
    "        pi_corr.append(fold_corr)\n",
    "\n",
    "        del trained, expl, shap_vals\n",
    "        gc.collect()\n",
    "\n",
    "    # ─── агрегируем по фолдам\n",
    "    mean_ = lambda lst: np.stack(lst).mean(axis=0)\n",
    "    mix_m, mse_m, r2_m, corr_m = map(mean_, (pi_mix, pi_mse, pi_r2, pi_corr))\n",
    "    shap_m, coef_m             = map(mean_, (shap_all, coef_all))\n",
    "\n",
    "    # ─── финальный комбинированный скор\n",
    "    z = lambda x: (x - x.mean()) / (x.std() + 1e-9)\n",
    "    final_score = np.nanmean(np.vstack([z(mix_m), z(shap_m), z(coef_m)]), axis=0)\n",
    "\n",
    "    res = (pd.DataFrame(dict(\n",
    "             feature=f_names,\n",
    "             Δmse = mse_m,\n",
    "             Δr2  = r2_m,\n",
    "             Δcorr= corr_m,\n",
    "             perm_mix = mix_m,\n",
    "             shap = shap_m,\n",
    "             ridge = coef_m,\n",
    "             importance = final_score))\n",
    "           .sort_values(\"importance\", ascending=False)\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "    return res\n",
    "\n",
    "def find_best_trial_by_weighted_three_score(\n",
    "    trials: List[FrozenTrial],\n",
    "    pnl_score = 0.45,\n",
    "    diff_score = 0.45,\n",
    "    weight_score = 0.10\n",
    ") -> Tuple[float, int, float, float, int, Dict]:\n",
    "    \"\"\"\n",
    "    Ищет лучший трейл по взвешенной сумме двух метрик:\n",
    "        - pnl (trial.values[0]), вес 0.65, диапазон 0..600\n",
    "        - diff (trial.values[1]), вес 0.35, диапазон 0..1\n",
    "\n",
    "    Возвращает:\n",
    "        - score: float — итоговый взвешенный скор\n",
    "        - trial_number: int — номер трейла\n",
    "        - pnl: float — значение pnl\n",
    "        - diff: float — значение diff\n",
    "        - params: dict — параметры трейла\n",
    "    \"\"\"\n",
    "    WEIGHT_PNL = pnl_score\n",
    "    WEIGHT_DIFF = diff_score\n",
    "    WEIGHT_SCORE = weight_score\n",
    "    MAX_PNL = np.max([i.values[0] for i in [trial for trial in trials if trial.values is not None]])*2.5  # для нормализации\n",
    "    MAX_SCORE = np.max([i.values[2] for i in [trial for trial in study.trials if trial.values is not None]])  # для нормализации\n",
    "    MAX_DIFF = 1   # для нормализации\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    best_trial_number = -1\n",
    "    best_pnl = None\n",
    "    best_diff = None\n",
    "    best_score_n = None\n",
    "    best_params = None\n",
    "\n",
    "    for trial in trials:\n",
    "        # Проверяем что трейл валидный и содержит обе метрики\n",
    "        if not trial.values or len(trial.values) < 3:\n",
    "            continue\n",
    "\n",
    "        pnl = trial.values[0]\n",
    "        diff = trial.values[1]\n",
    "        score_n = trial.values[2]\n",
    "\n",
    "        # Нормализуем значения\n",
    "        norm_pnl = pnl / MAX_PNL if MAX_PNL else 0\n",
    "        norm_diff = diff / MAX_DIFF if MAX_DIFF else 0\n",
    "        norm_score = score_n / MAX_SCORE if WEIGHT_SCORE else 0\n",
    "\n",
    "        # Взвешенная сумма\n",
    "        score = WEIGHT_PNL * norm_pnl + WEIGHT_DIFF * norm_diff + WEIGHT_SCORE * norm_score\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_trial_number = trial.number\n",
    "            best_pnl = pnl\n",
    "            best_diff = diff\n",
    "            best_score_n = score_n\n",
    "            best_params = trial.params\n",
    "\n",
    "    if best_trial_number == -1:\n",
    "        raise ValueError(\"Нет подходящих трейлов с двумя метриками (pnl и diff).\")\n",
    "\n",
    "    return best_score, best_trial_number, best_pnl, best_diff, best_score_n, best_params\n",
    "\n",
    "def find_best_trial_by_weighted_score(\n",
    "    trials: List[FrozenTrial]\n",
    ") -> Tuple[float, int, float, float, Dict]:\n",
    "    \"\"\"\n",
    "    Ищет лучший трейл по взвешенной сумме двух метрик:\n",
    "        - pnl (trial.values[0]), вес 0.65, диапазон 0..600\n",
    "        - diff (trial.values[1]), вес 0.35, диапазон 0..1\n",
    "\n",
    "    Возвращает:\n",
    "        - score: float — итоговый взвешенный скор\n",
    "        - trial_number: int — номер трейла\n",
    "        - pnl: float — значение pnl\n",
    "        - diff: float — значение diff\n",
    "        - params: dict — параметры трейла\n",
    "    \"\"\"\n",
    "    WEIGHT_PNL = 0.60\n",
    "    WEIGHT_DIFF = 0.40\n",
    "    MAX_PNL = np.max([i.values for i in [trial for trial in trials if trial.values is not None]])*2.5  # для нормализации\n",
    "    MAX_DIFF = 1   # для нормализации\n",
    "\n",
    "    best_score = float('-inf')\n",
    "    best_trial_number = -1\n",
    "    best_pnl = None\n",
    "    best_diff = None\n",
    "    best_params = None\n",
    "\n",
    "    for trial in trials:\n",
    "        # Проверяем что трейл валидный и содержит обе метрики\n",
    "        if not trial.values or len(trial.values) < 2:\n",
    "            continue\n",
    "\n",
    "        pnl = trial.values[0]\n",
    "        diff = trial.values[1]\n",
    "\n",
    "        # Нормализуем значения\n",
    "        norm_pnl = pnl / MAX_PNL if MAX_PNL else 0\n",
    "        norm_diff = diff / MAX_DIFF if MAX_DIFF else 0\n",
    "\n",
    "        # Взвешенная сумма\n",
    "        score = WEIGHT_PNL * norm_pnl + WEIGHT_DIFF * norm_diff\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_trial_number = trial.number\n",
    "            best_pnl = pnl\n",
    "            best_diff = diff\n",
    "            best_params = trial.params\n",
    "\n",
    "    if best_trial_number == -1:\n",
    "        raise ValueError(\"Нет подходящих трейлов с двумя метриками (pnl и diff).\")\n",
    "\n",
    "    return best_score, best_trial_number, best_pnl, best_diff, best_params\n",
    "\n",
    "\n",
    "def to_dense(X):\n",
    "    # LightGBM «любит» плотные матрицы\n",
    "    return X.toarray() if hasattr(X, 'toarray') else X\n",
    "\n",
    "def corrcoef(y_true, y_pred):\n",
    "    return np.corrcoef(y_true, y_pred)[0, 1]\n",
    "\n",
    "def find_best_trial_by_weighted_score_extended(\n",
    "    trials: List[FrozenTrial]\n",
    ") -> Tuple[float, int, Dict, str, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Ищет лучший трейл по взвешенной сумме метрик для двух стратегий:\n",
    "        - Стратегия RSI: pnl_rsi и diff_rsi\n",
    "        - Стратегия RSI+SMA/EMA: pnl_full и diff_full\n",
    "    \n",
    "    Возвращает:\n",
    "        - best_score: float - наивысший взвешенный скор среди всех стратегий\n",
    "        - best_trial_number: int - номер лучшего трейла\n",
    "        - best_params: dict - параметры лучшего трейла\n",
    "        - best_strategy: str - название лучшей стратегии ('RSI' или 'RSI_SMA_EMA')\n",
    "        - best_pnl: float - лучшее значение PnL (из выбранной стратегии)\n",
    "        - best_diff: float - лучшее значение diff (из выбранной стратегии)\n",
    "        - pnl_rsi: float - значение PnL для стратегии RSI\n",
    "        - diff_rsi: float - значение diff для стратегии RSI\n",
    "        - pnl_full: float - значение PnL для стратегии RSI+SMA/EMA\n",
    "        - diff_full: float - значение diff для стратегии RSI+SMA/EMA\n",
    "    \"\"\"\n",
    "    # Веса для метрик\n",
    "    WEIGHT_PNL = 0.80\n",
    "    WEIGHT_DIFF = 0.20\n",
    "    \n",
    "    # Максимальные значения для нормализации (можно настроить)\n",
    "    MAX_PNL = np.max([i.values for i in [trial for trial in study.trials if trial.values is not None]])  # предполагаемый максимум PnL\n",
    "    MAX_DIFF = 1    # максимум для diff (уже нормализован)\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    best_trial_number = -1\n",
    "    best_params = None\n",
    "    best_strategy = None\n",
    "    best_pnl = None\n",
    "    best_diff = None\n",
    "    \n",
    "    # Для хранения всех метрик (для отладки/анализа)\n",
    "    full_results = []\n",
    "\n",
    "    for trial in trials:\n",
    "        # Проверяем что трейл валидный и содержит все 4 метрики\n",
    "        if not trial.values or len(trial.values) < 4 and trial.values:\n",
    "            continue\n",
    "            \n",
    "        pnl_rsi, pnl_full, diff_rsi, diff_full = trial.values\n",
    "        \n",
    "        # Нормализация значений (защита от деления на 0)\n",
    "        norm_pnl_rsi = pnl_rsi / MAX_PNL if MAX_PNL != 0 else 0\n",
    "        norm_pnl_full = pnl_full / MAX_PNL if MAX_PNL != 0 else 0\n",
    "        norm_diff_rsi = diff_rsi / MAX_DIFF if MAX_DIFF != 0 else 0\n",
    "        norm_diff_full = diff_full / MAX_DIFF if MAX_DIFF != 0 else 0\n",
    "        \n",
    "        # Вычисляем скоринг для обеих стратегий\n",
    "        score_rsi = WEIGHT_PNL * norm_pnl_rsi + WEIGHT_DIFF * norm_diff_rsi\n",
    "        score_full = WEIGHT_PNL * norm_pnl_full + WEIGHT_DIFF * norm_diff_full\n",
    "        \n",
    "        # Определяем какая стратегия лучше в этом трейле\n",
    "        if score_rsi > score_full:\n",
    "            current_score = score_rsi\n",
    "            current_strategy = 'С трендовой линией'\n",
    "            current_pnl = pnl_rsi\n",
    "            current_diff = diff_rsi\n",
    "        else:\n",
    "            current_score = score_full\n",
    "            current_strategy = 'Трендовая + недельные скользящие'\n",
    "            current_pnl = pnl_full\n",
    "            current_diff = diff_full\n",
    "        \n",
    "        # Сохраняем все метрики для анализа\n",
    "        full_results.append({\n",
    "            'trial_number': trial.number,\n",
    "            'score_rsi': score_rsi,\n",
    "            'score_full': score_full,\n",
    "            'strategy': current_strategy,\n",
    "            'score': current_score,\n",
    "            'params': trial.params\n",
    "        })\n",
    "        \n",
    "        # Обновляем лучший результат\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_trial_number = trial.number\n",
    "            best_params = trial.params\n",
    "            best_strategy = current_strategy\n",
    "            best_pnl = current_pnl\n",
    "            best_diff = current_diff\n",
    "\n",
    "    if best_trial_number == -1:\n",
    "        raise ValueError(\"Нет подходящих трейлов с четырьмя метриками (pnl_rsi, pnl_full, diff_rsi, diff_full).\")\n",
    "\n",
    "    return (\n",
    "        best_score,\n",
    "        best_trial_number,\n",
    "        best_params,\n",
    "        best_strategy,\n",
    "        best_pnl,\n",
    "        best_diff,\n",
    "        # Возвращаем также все метрики для лучшего трейла\n",
    "        #next(t.values[0] for t in trials if t.number == best_trial_number),  # pnl_rsi\n",
    "        #next(t.values[2] for t in trials if t.number == best_trial_number),  # diff_rsi\n",
    "        #next(t.values[1] for t in trials if t.number == best_trial_number),  # pnl_full\n",
    "        #next(t.values[3] for t in trials if t.number == best_trial_number)   # diff_full\n",
    "    )\n",
    "\n",
    "def find_best_trial_by_weighted_score_extended1(\n",
    "    trials: List[FrozenTrial],\n",
    "    pnl_score: float = 0.45,\n",
    "    diff_score: float = 0.45,\n",
    "    weight_score: float = 0.10\n",
    ") -> Tuple[float, int, Dict, str, float, float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Ищет лучший трейл по взвешенной сумме метрик для двух стратегий:\n",
    "        - Стратегия RSI: pnl_rsi, diff_rsi, score_rsi\n",
    "        - Стратегия RSI+SMA/EMA: pnl_full, diff_full, score_full\n",
    "    \n",
    "    Параметры:\n",
    "        - pnl_score: вес для метрики PnL (по умолчанию 0.45)\n",
    "        - diff_score: вес для метрики diff (по умолчанию 0.45)\n",
    "        - weight_score: вес для дополнительной метрики score (по умолчанию 0.10)\n",
    "    \n",
    "    Возвращает:\n",
    "        - best_score: float - наивысший взвешенный скор среди всех стратегий\n",
    "        - best_trial_number: int - номер лучшего трейла\n",
    "        - best_params: dict - параметры лучшего трейла\n",
    "        - best_strategy: str - название лучшей стратегии ('RSI' или 'RSI_SMA_EMA')\n",
    "        - best_pnl: float - лучшее значение PnL (из выбранной стратегии)\n",
    "        - best_diff: float - лучшее значение diff (из выбранной стратегии)\n",
    "        - best_score_n: float - лучшее значение score (из выбранной стратегии)\n",
    "        - pnl_rsi: float - значение PnL для стратегии RSI\n",
    "        - diff_rsi: float - значение diff для стратегии RSI\n",
    "        - pnl_full: float - значение PnL для стратегии RSI+SMA/EMA\n",
    "        - diff_full: float - значение diff для стратегии RSI+SMA/EMA\n",
    "        - score_rsi: float - значение score для стратегии RSI\n",
    "        - score_full: float - значение score для стратегии RSI+SMA/EMA\n",
    "    \"\"\"\n",
    "    # Проверка корректности весов\n",
    "    total_weight = pnl_score + diff_score + weight_score\n",
    "    if not np.isclose(total_weight, 1.0):\n",
    "        raise ValueError(f\"Сумма весов должна быть равна 1.0 (получено {total_weight})\")\n",
    "    \n",
    "    # Максимальные значения для нормализации\n",
    "    MAX_PNL = np.max([max(trial.values[0], trial.values[1]) for trial in trials if trial.values is not None]) * 2.5\n",
    "    MAX_DIFF = 1.0    # максимум для diff (уже нормализован)\n",
    "    MAX_SCORE = np.max([max(trial.values[2], trial.values[3]) for trial in trials if trial.values is not None]) if weight_score > 0 else 1.0\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    best_trial_number = -1\n",
    "    best_params = None\n",
    "    best_strategy = None\n",
    "    best_pnl = None\n",
    "    best_diff = None\n",
    "    best_score_n = None\n",
    "    \n",
    "    # Для хранения всех метрик (для отладки/анализа)\n",
    "    full_results = []\n",
    "\n",
    "    for trial in trials:\n",
    "        # Проверяем что трейл валидный и содержит все 6 метрик\n",
    "        if not trial.values or len(trial.values) < 6:\n",
    "            continue\n",
    "            \n",
    "        pnl_rsi, pnl_full, score_rsi, score_full, diff_rsi, diff_full = trial.values\n",
    "        \n",
    "        # Нормализация значений (защита от деления на 0)\n",
    "        norm_pnl_rsi = pnl_rsi / MAX_PNL if MAX_PNL != 0 else 0\n",
    "        norm_pnl_full = pnl_full / MAX_PNL if MAX_PNL != 0 else 0\n",
    "        norm_diff_rsi = diff_rsi / MAX_DIFF if MAX_DIFF != 0 else 0\n",
    "        norm_diff_full = diff_full / MAX_DIFF if MAX_DIFF != 0 else 0\n",
    "        norm_score_rsi = score_rsi / MAX_SCORE if MAX_SCORE != 0 and weight_score > 0 else 0\n",
    "        norm_score_full = score_full / MAX_SCORE if MAX_SCORE != 0 and weight_score > 0 else 0\n",
    "        \n",
    "        # Вычисляем скоринг для обеих стратегий\n",
    "        score_rsi_weighted = (pnl_score * norm_pnl_rsi + \n",
    "                             diff_score * norm_diff_rsi + \n",
    "                             weight_score * norm_score_rsi)\n",
    "        \n",
    "        score_full_weighted = (pnl_score * norm_pnl_full + \n",
    "                              diff_score * norm_diff_full + \n",
    "                              weight_score * norm_score_full)\n",
    "        \n",
    "        # Определяем какая стратегия лучше в этом трейле\n",
    "        if score_rsi_weighted > score_full_weighted:\n",
    "            current_score = score_rsi_weighted\n",
    "            current_strategy = 'С трендовой линией'\n",
    "            current_pnl = pnl_rsi\n",
    "            current_diff = diff_rsi\n",
    "            current_score_n = score_rsi\n",
    "        else:\n",
    "            current_score = score_full_weighted\n",
    "            current_strategy = 'Трендовая + недельные скользящие'\n",
    "            current_pnl = pnl_full\n",
    "            current_diff = diff_full\n",
    "            current_score_n = score_full\n",
    "        \n",
    "        # Сохраняем все метрики для анализа\n",
    "        full_results.append({\n",
    "            'trial_number': trial.number,\n",
    "            'score_rsi': score_rsi_weighted,\n",
    "            'score_full': score_full_weighted,\n",
    "            'strategy': current_strategy,\n",
    "            'score': current_score,\n",
    "            'params': trial.params\n",
    "        })\n",
    "        \n",
    "        # Обновляем лучший результат\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_trial_number = trial.number\n",
    "            best_params = trial.params\n",
    "            best_strategy = current_strategy\n",
    "            best_pnl = current_pnl\n",
    "            best_diff = current_diff\n",
    "            best_score_n = current_score_n\n",
    "\n",
    "    if best_trial_number == -1:\n",
    "        raise ValueError(\"Нет подходящих трейлов с шестью метриками (pnl_rsi, pnl_full, score_rsi, score_full, diff_rsi, diff_full).\")\n",
    "\n",
    "    # Получаем все метрики для лучшего трейла\n",
    "    best_trial = next(t for t in trials if t.number == best_trial_number)\n",
    "    pnl_rsi, pnl_full, score_rsi, score_full, diff_rsi, diff_full = best_trial.values\n",
    "\n",
    "    return (\n",
    "        best_score,\n",
    "        best_trial_number,\n",
    "        best_params,\n",
    "        best_strategy,\n",
    "        best_pnl,\n",
    "        best_diff,\n",
    "        best_score_n,\n",
    "        pnl_rsi,\n",
    "        diff_rsi,\n",
    "        score_rsi,\n",
    "        pnl_full,\n",
    "        diff_full,\n",
    "        score_full\n",
    "    )\n",
    "\n",
    "def plot_price_with_indicators_mplfinance(df, ticker, regime = 'P',save_path=None):\n",
    "    \"\"\"\n",
    "    Отрисовывает график движения цены с индикаторами и сигналами покупки/продажи, используя mplfinance.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame с данными о цене и индикаторах.\n",
    "        ticker (str): Тикер акции для заголовка графика.\n",
    "        save_path (str, optional): Путь для сохранения графика. Если None, то график покажется.\n",
    "    \"\"\"\n",
    "    # Копируем DataFrame, чтобы не менять оригинал\n",
    "    df = df.copy()\n",
    "\n",
    "    # Преобразуем столбец time в datetime и делаем индексом\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    df = df.set_index(\"time\")\n",
    "\n",
    "    # Создаем список дополнительных панелей для индикаторов (без объема)\n",
    "    apds = []\n",
    "\n",
    "\n",
    "    if \"pmax\" in df.columns:\n",
    "        apds.append(mpf.make_addplot(df[\"pmax\"], color=\"red\", ylabel=\"PMAX\", panel=0))\n",
    "    if \"ma\" in df.columns:\n",
    "        apds.append(mpf.make_addplot(df[\"ma\"], color=\"blue\", ylabel=\"VAR\", panel=0))\n",
    "    if \"var\" in df.columns:\n",
    "        apds.append(mpf.make_addplot(df[\"var\"], color=\"blue\", ylabel=\"VAR\", panel=0))\n",
    "    if \"ema\" in df.columns:\n",
    "        apds.append(mpf.make_addplot(df[\"ema\"], color=\"purple\", ylabel=\"EMA\", panel=0))\n",
    "    if \"adaptive_ma\" in df.columns:\n",
    "        apds.append(mpf.make_addplot(df[\"adaptive_ma\"], color=\"blue\", ylabel=\"VAR\", panel=0))\n",
    "    if \"adaptive_pmax\" in df.columns:\n",
    "        apds.append(mpf.make_addplot(df[\"adaptive_pmax\"], color=\"purple\", ylabel=\"EMA\", panel=0))\n",
    "\n",
    "    if \"regime\" in df.columns and regime == 'R':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        unique_regimes = sorted(df['regime'].unique())\n",
    "        colors = plt.cm.get_cmap('tab10', len(unique_regimes))\n",
    "        \n",
    "        for i, reg in enumerate(unique_regimes):\n",
    "            # Создаем линию на уровне 5% от минимума\n",
    "            reg_line = (df[\"low\"] * 1.05).where(df[\"regime\"] == reg)\n",
    "            apds.append(mpf.make_addplot(\n",
    "                reg_line,\n",
    "                type='line',\n",
    "                color=colors(i),\n",
    "                panel=0\n",
    "            ))\n",
    "    if \"normalized_target\" in df.columns and regime == 'P':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        apds.append(mpf.make_addplot(df[\"normalized_target\"], color=\"blue\", ylabel=\"Regime\", panel=2))\n",
    "    if \"predicted_p\" in df.columns and regime == 'P':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        apds.append(mpf.make_addplot(df[\"predicted_p\"], color=\"black\", ylabel=\"Regime\", panel=2))\n",
    "    if \"reg_pred\" in df.columns and regime == 'P':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        apds.append(mpf.make_addplot(df[\"reg_pred\"], color=\"yellow\", ylabel=\"Regime\", panel=2))\n",
    "    if \"rank_pred\" in df.columns and regime == 'P':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        apds.append(mpf.make_addplot(df[\"rank_pred\"], color=\"green\", ylabel=\"Regime\", panel=2))\n",
    "    if \"multi_target_10\" in df.columns and regime == 'P':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        apds.append(mpf.make_addplot(df[\"multi_target_10\"], color=\"grey\", ylabel=\"Regime\", panel=2))\n",
    "    if \"predicted_p_old\" in df.columns and regime == 'P':\n",
    "        # Генерация цветовой карты по количеству уникальных режимов\n",
    "        apds.append(mpf.make_addplot(df[\"predicted_p_old\"], color=\"purple\", ylabel=\"Regime\", panel=2))\n",
    "\n",
    "    # Подготовка сигналов для отрисовки\n",
    "    buy_signals = df[df[\"buy_signal\"]]\n",
    "    sell_signals = df[df[\"sell_signal\"]]\n",
    "\n",
    "    # Создаем Series для сигналов, выровненные по индексу основного DataFrame\n",
    "    buy_series = pd.Series(index=df.index, dtype='float64')\n",
    "    buy_series[buy_signals.index] = buy_signals['open']\n",
    "\n",
    "    sell_series = pd.Series(index=df.index, dtype='float64')\n",
    "    sell_series[sell_signals.index] = sell_signals['open']\n",
    "\n",
    "    # Добавляем сигналы покупки\n",
    "    if not buy_signals.empty:\n",
    "        apds.append(mpf.make_addplot(\n",
    "            buy_series,\n",
    "            type='scatter',\n",
    "            markersize=50,\n",
    "            marker='^',\n",
    "            color='green',\n",
    "            label='Buy Signal',\n",
    "            panel=0\n",
    "        ))\n",
    "\n",
    "    # Добавляем сигналы продажи\n",
    "    if not sell_signals.empty:\n",
    "        apds.append(mpf.make_addplot(\n",
    "            sell_series,\n",
    "            type='scatter',\n",
    "            markersize=50,\n",
    "            marker='v',\n",
    "            color='red',\n",
    "            label='Sell Signal',\n",
    "            panel=0\n",
    "        ))\n",
    "\n",
    "    # Подготовка vlines (единый словарь)\n",
    "    vlines_dict = {}\n",
    "    if not buy_signals.empty:\n",
    "        vlines_dict['vlines'] = buy_signals.index.to_list()\n",
    "        vlines_dict['linewidths'] = 0.5\n",
    "        vlines_dict['colors'] = ['green'] * len(buy_signals)\n",
    "        vlines_dict['alpha'] = 0.5\n",
    "\n",
    "    if not sell_signals.empty:\n",
    "        if 'vlines' in vlines_dict:\n",
    "            vlines_dict['vlines'].extend(sell_signals.index.to_list())\n",
    "            vlines_dict['colors'].extend(['red'] * len(sell_signals))\n",
    "        else:\n",
    "            vlines_dict['vlines'] = sell_signals.index.to_list()\n",
    "            vlines_dict['linewidths'] = 0.5\n",
    "            vlines_dict['colors'] = ['red'] * len(sell_signals)\n",
    "            vlines_dict['alpha'] = 0.5\n",
    "\n",
    "    # Отрисовка графика с mplfinance\n",
    "    plot_kwargs = dict(\n",
    "        type=\"candle\",\n",
    "        style=\"yahoo\",\n",
    "        title=f\"График цены {ticker} с индикаторами\",\n",
    "        ylabel=\"Цена\",\n",
    "        addplot=apds,\n",
    "        show_nontrading=False,\n",
    "        figsize=(18, 10),\n",
    "        warn_too_much_data=len(df) + 1,\n",
    "    )\n",
    "\n",
    "    if \"volume\" in df.columns:\n",
    "        plot_kwargs[\"volume\"] = True\n",
    "        plot_kwargs[\"panel_ratios\"] = (6, 3)\n",
    "    if vlines_dict:\n",
    "        plot_kwargs[\"vlines\"] = vlines_dict\n",
    "\n",
    "    if save_path:\n",
    "        plot_kwargs[\"savefig\"] = save_path\n",
    "\n",
    "    mpf.plot(df, **plot_kwargs)\n",
    "    \n",
    "def plot_3d_metrics(trials, num_trial = [0,1,2],x_label='среднее значение f1', y_label='среднее значение AUC ROC', z_label='стандартное отклонение f1', deffs=None, directions=['максимизировать', 'максимизировать', 'минимизировать']):\n",
    "    \"\"\"\n",
    "    Функция для построения 3D-графика на основе результатов Optuna.\n",
    "\n",
    "    Параметры:\n",
    "    -----------\n",
    "    trials : list\n",
    "        Список объектов `optuna.trial.FrozenTrial` из study.trials.\n",
    "    x_label : str, optional\n",
    "        Название для оси X (по умолчанию 'среднее значение f1').\n",
    "    y_label : str, optional\n",
    "        Название для оси Y (по умолчанию 'среднее значение AUC ROC').\n",
    "    z_label : str, optional\n",
    "        Название для оси Z (по умолчанию 'стандартное отклонение f1').\n",
    "    deffs : list, optional\n",
    "        Пороговые значения для фильтрации trials (по умолчанию None).\n",
    "    directions : list, optional\n",
    "        Направления оптимизации для каждой метрики (по умолчанию ['максимизировать', 'максимизировать', 'минимизировать']).\n",
    "    \"\"\"\n",
    "    print(len(trials))\n",
    "\n",
    "    # Сопоставление направлений с операторами сравнения\n",
    "    direction_to_operator = {\n",
    "        'максимизировать': lambda a, b: a > b,\n",
    "        'минимизировать': lambda a, b: a < b\n",
    "    }\n",
    "\n",
    "    # Фильтрация trials\n",
    "    if deffs is None:\n",
    "        trials = [trial for trial in trials if trial.values is not None]\n",
    "    else:\n",
    "        trials = [\n",
    "            trial for trial in trials\n",
    "            if trial.values is not None\n",
    "            and direction_to_operator[directions[0]](trial.values[0], deffs[0])\n",
    "            and direction_to_operator[directions[1]](trial.values[1], deffs[1])\n",
    "            and direction_to_operator[directions[2]](trial.values[2], deffs[2])\n",
    "        ]\n",
    "\n",
    "    print(len(trials))\n",
    "\n",
    "    # Извлечение значений метрик\n",
    "    x_vals = [trial.values[num_trial[0]] for trial in trials]\n",
    "    y_vals = [trial.values[num_trial[1]] for trial in trials]\n",
    "    z_vals = [trial.values[num_trial[2]] for trial in trials]\n",
    "\n",
    "    # Форматирование параметров для hover text\n",
    "    def format_params(params):\n",
    "        return '<br>'.join([f\"{key}: {value}\" for key, value in params.items()])\n",
    "\n",
    "    # Создание текста для hover\n",
    "    hover_texts = [\n",
    "        f\"Number: {trial.number}<br>\"\n",
    "        f\"{x_label}: {trial.values[num_trial[0]]:.4f}<br>\"\n",
    "        f\"{y_label}: {trial.values[num_trial[1]]:.4f}<br>\"\n",
    "        f\"{z_label}: {trial.values[num_trial[2]]:.4f}<br>\"\n",
    "        f\"Params:<br>{format_params(trial.params)}\"\n",
    "        for trial in trials\n",
    "    ]\n",
    "\n",
    "    # Создание 3D-графика\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=x_vals, y=y_vals, z=z_vals,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=x_vals,  # Цветовая шкала может быть привязана к одной из метрик\n",
    "            colorscale='Viridis',\n",
    "            opacity=0.8\n",
    "        ),\n",
    "        text=hover_texts,  # Добавляем hover text\n",
    "        hoverinfo='text'   # Указываем, что при наведении нужно показывать текст\n",
    "    )])\n",
    "\n",
    "    # Добавление меток к осям\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=x_label,\n",
    "            yaxis_title=y_label,\n",
    "            zaxis_title=z_label\n",
    "        ),\n",
    "        title=\"3 метрики через Optuna\"\n",
    "    )\n",
    "\n",
    "    # Отображение графика\n",
    "    fig.show()\n",
    "\n",
    "def prepare_regime_params(optuna_params):\n",
    "    \"\"\"\n",
    "    Преобразует параметры из формата Optuna в два словаря: базовые параметры режимов и параметры расчета.\n",
    "    \n",
    "    Args:\n",
    "        optuna_params (dict): Словарь с параметрами из Optuna\n",
    "        \n",
    "    Returns:\n",
    "        dict: Словарь с двумя ключами: 'base_params' (параметры режимов) и 'calc_params' (остальные параметры)\n",
    "    \"\"\"\n",
    "    # Инициализируем словари для базовых параметров и параметров расчета\n",
    "    start_params = {}\n",
    "    base_params = {}\n",
    "    calc_params = {}\n",
    "    \n",
    "    # Сначала обрабатываем параметры режимов (0-4)\n",
    "\n",
    "    start_params['moving_average_length'] = optuna_params.get('moving_average_length', 14)\n",
    "    start_params['atr_period'] = optuna_params.get('atr_period', 10)\n",
    "    for regime in range(5):\n",
    "        regime_key = f'regime_{regime}_'\n",
    "        regime_params = {}\n",
    "        \n",
    "        # Основные параметры режима\n",
    "        regime_params['average_type'] = optuna_params.get(f'{regime_key}average_type', 'SMA')\n",
    "        regime_params['moving_average_length'] = optuna_params.get(f'{regime_key}ma_length', 50)\n",
    "        regime_params['atr_period'] = optuna_params.get(f'{regime_key}atr_period', 14)\n",
    "        regime_params['atr_multiplier'] = optuna_params.get(f'{regime_key}atr_multiplier', 3.0)\n",
    "        \n",
    "        # Параметры AMA, если они есть\n",
    "        ama_atr_period = optuna_params.get(f'{regime_key}ama_atr_period')\n",
    "        ama_min_period = optuna_params.get(f'{regime_key}ama_min_period')\n",
    "        ama_max_period = optuna_params.get(f'{regime_key}ama_max_period')\n",
    "        \n",
    "        if regime_params['average_type'] == 'AMA' and all(p is not None for p in [ama_atr_period, ama_min_period, ama_max_period]):\n",
    "            regime_params['ama_params'] = {\n",
    "                'atr_period': int(ama_atr_period),\n",
    "                'min_period': int(ama_min_period),\n",
    "                'max_period': int(ama_max_period)\n",
    "            }\n",
    "        \n",
    "        base_params[regime] = regime_params\n",
    "    \n",
    "    # Теперь собираем все остальные параметры в calc_params\n",
    "    other_params = [\n",
    "        'rsi_length', 'use_smoothing', 'smoothing_length', 'smoothing_type',\n",
    "        'alma_sigma', 'rsi_overbought', 'rsi_oversold', 'use_knn',\n",
    "        'knn_neighbors', 'knn_lookback', 'knn_weight', 'feature_count',\n",
    "        'use_filter', 'filter_method', 'filter_strength', 'sma_length',\n",
    "        'ema_length', 'rsi_helbuth'\n",
    "    ]\n",
    "    \n",
    "    for param in other_params:\n",
    "        if param in optuna_params:\n",
    "            calc_params[param] = optuna_params[param]\n",
    "    \n",
    "    return {\n",
    "        'start_params': start_params,\n",
    "        'base_params': base_params,\n",
    "        'calc_params': calc_params\n",
    "    }\n",
    "\n",
    "def pair_and_clean_signals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Оставляет только первый buy после flat и первый sell после buy.\n",
    "    Гарантирует, что каждая покупка имеет свою продажу.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # пусть V5 уже записал нам в df['buy_signal'], df['sell_signal'] — raw-сигналы\n",
    "    buy_raw  = df['buy_signal'].values\n",
    "    sell_raw = df['sell_signal'].values\n",
    "\n",
    "    # обнуляем\n",
    "    df['buy_signal']  = False\n",
    "    df['sell_signal'] = False\n",
    "\n",
    "    in_pos = False\n",
    "    for i in range(len(df)):\n",
    "        if not in_pos and buy_raw[i]:\n",
    "            # открываем новую сделку\n",
    "            df.iat[i, df.columns.get_loc('buy_signal')] = True\n",
    "            in_pos = True\n",
    "        elif in_pos and sell_raw[i]:\n",
    "            # закрываем\n",
    "            df.iat[i, df.columns.get_loc('sell_signal')] = True\n",
    "            in_pos = False\n",
    "        # все остальные raw-сигналы игнорируем\n",
    "\n",
    "    # если позиция осталась открытой — принудительный выход на последнем баре\n",
    "    if in_pos and len(df)>0:\n",
    "        df.iat[len(df)-1, df.columns.get_loc('sell_signal')] = True\n",
    "\n",
    "    return df\n",
    "\n",
    "class MachineLearningRSI:\n",
    "    def __init__(self,\n",
    "                 rsi_length=300,\n",
    "                 use_smoothing=True,\n",
    "                 smoothing_length=268,\n",
    "                 smoothing_type='ALMA',\n",
    "                 alma_sigma=6,\n",
    "                 rsi_overbought=70,\n",
    "                 rsi_oversold=30,\n",
    "                 use_knn=True,\n",
    "                 knn_neighbors=7,\n",
    "                 knn_lookback=500,\n",
    "                 knn_weight=0.6,\n",
    "                 feature_count=5,\n",
    "                 use_filter=True,\n",
    "                 filter_method='Kalman',\n",
    "                 filter_strength=0.7,\n",
    "                 sma_length=20 + 7*24*4*3,\n",
    "                 ema_length=21 + 7*24*4*3\n",
    "                 ):\n",
    "\n",
    "        # Базовые параметры\n",
    "        self.rsi_length = rsi_length\n",
    "        self.use_smoothing = use_smoothing\n",
    "        self.smoothing_length = smoothing_length\n",
    "        self.smoothing_type = smoothing_type\n",
    "        self.alma_sigma = alma_sigma\n",
    "\n",
    "        # Пороговые уровни\n",
    "        self.rsi_overbought = rsi_overbought\n",
    "        self.rsi_oversold = rsi_oversold\n",
    "\n",
    "        # Параметры KNN\n",
    "        self.use_knn = use_knn\n",
    "        self.knn_neighbors = knn_neighbors\n",
    "        self.knn_lookback = knn_lookback\n",
    "        self.knn_weight = knn_weight\n",
    "        self.feature_count = feature_count\n",
    "\n",
    "        # Фильтрация\n",
    "        self.use_filter = use_filter\n",
    "        self.filter_method = filter_method\n",
    "        self.filter_strength = filter_strength\n",
    "\n",
    "        self.sma_length = sma_length\n",
    "        self.ema_length = ema_length\n",
    "\n",
    "    def calculate_rsi(self, close: pd.Series, length: int) -> pd.Series:\n",
    "        \"\"\"Расчет RSI через RMA аналогично PineScript ta.rsi\"\"\"\n",
    "        delta = close.diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "        avg_gain = gain.ewm(alpha=1/length, min_periods=length, adjust=False).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/length, min_periods=length, adjust=False).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    def smooth(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Корректный ALMA\"\"\"\n",
    "        if self.smoothing_type == 'SMA':\n",
    "            return series.rolling(self.smoothing_length).mean()\n",
    "        elif self.smoothing_type == 'EMA':\n",
    "            return series.ewm(span=self.smoothing_length, adjust=False).mean()\n",
    "        elif self.smoothing_type == 'ALMA':\n",
    "            m = self.smoothing_length\n",
    "            offset = 0.85\n",
    "            sigma = self.alma_sigma\n",
    "\n",
    "            def alma(series):\n",
    "                window = np.arange(m)\n",
    "                weights = np.exp(-((window - offset * (m-1))**2) / (2*(sigma**2)))\n",
    "                weights /= weights.sum()\n",
    "                return np.convolve(series, weights, mode='valid')\n",
    "\n",
    "            def alma_causal(series: pd.Series, length: int = 9, offset: float = 0.85, sigma: float = 6) -> pd.Series:\n",
    "                \"\"\"\n",
    "                Казуальная реализация ALMA (Arnaud Legoux Moving Average)\n",
    "                Использует только прошлые и текущие значения, без lookahead bias.\n",
    "                \"\"\"\n",
    "                if length > len(series):\n",
    "                    return pd.Series(np.nan, index=series.index)\n",
    "\n",
    "                # Предвычисление весов ALMA\n",
    "                window = np.arange(length)\n",
    "                m = offset * (length - 1)\n",
    "                s = length / sigma\n",
    "                weights = np.exp(-((window - m) ** 2) / (2 * s ** 2))\n",
    "                weights /= weights.sum()\n",
    "\n",
    "                # Применяем ALMA казуально (rolling + dot product)\n",
    "                alma_vals = []\n",
    "                for i in range(length - 1, len(series)):\n",
    "                    window_data = series.iloc[i - length + 1:i + 1]\n",
    "                    if window_data.isnull().any():\n",
    "                        alma_vals.append(np.nan)\n",
    "                    else:\n",
    "                        alma_vals.append(np.dot(weights, window_data.values))\n",
    "\n",
    "                # Паддинг NaN в начало, чтобы сохранить индекс\n",
    "                alma_series = pd.Series([np.nan] * (length - 1) + alma_vals, index=series.index)\n",
    "\n",
    "                return alma_series\n",
    "\n",
    "            alma_series = alma_causal(series.fillna(method='ffill'), m, offset, sigma)#, index=series.index[pad:-pad])\n",
    "            #alma_series = alma_series.reindex(series.index, method='nearest')\n",
    "            return alma_series\n",
    "        else:\n",
    "            return series\n",
    "\n",
    "    def feature_extraction(self, close: pd.Series, rsi: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"Извлечение признаков для KNN\"\"\"\n",
    "        features = pd.DataFrame(index=close.index)\n",
    "        features['rsi'] = self.normalize(rsi, self.knn_lookback)\n",
    "\n",
    "        if self.feature_count >= 2:\n",
    "            features['momentum_rsi'] = self.normalize(rsi.diff(3), self.knn_lookback)\n",
    "        if self.feature_count >= 3:\n",
    "            features['volatility_rsi'] = self.normalize(rsi.rolling(10).std(), self.knn_lookback)\n",
    "        if self.feature_count >= 4:\n",
    "            features['slope_rsi'] = self.normalize(self.get_slope(rsi, 5), self.knn_lookback)\n",
    "        if self.feature_count >= 5:\n",
    "            features['momentum_price'] = self.normalize(close.diff(5), self.knn_lookback)\n",
    "\n",
    "        return features.dropna()\n",
    "\n",
    "    def normalize(self, series: pd.Series, period: int) -> pd.Series:\n",
    "        \"\"\"Мин-макс нормализация\"\"\"\n",
    "        min_val = series.rolling(period).min()\n",
    "        max_val = series.rolling(period).max()\n",
    "        norm = (series - min_val) / (max_val - min_val)\n",
    "        return norm.clip(0, 1)\n",
    "\n",
    "    def get_slope(self, series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"Расчет наклона линейной регрессии\"\"\"\n",
    "        idx = np.arange(window)\n",
    "        def linreg(x):\n",
    "            A = np.vstack([idx, np.ones(len(idx))]).T\n",
    "            m, c = np.linalg.lstsq(A, x, rcond=None)[0]\n",
    "            return m\n",
    "        return series.rolling(window).apply(linreg, raw=True)\n",
    "\n",
    "    def apply_knn(self, features: pd.DataFrame, rsi: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Оптимизированная, но идентичная оригиналу версия KNN.\n",
    "        Сохраняет точную математику оригинального apply_knn_b с ускорением через BallTree.\n",
    "        \"\"\"\n",
    "        # Сохраняем структуру как в оригинале\n",
    "        full_index = rsi.index\n",
    "        common_index = features.index.intersection(rsi.index)\n",
    "        features = features.loc[common_index]\n",
    "        rsi = rsi.loc[common_index]\n",
    "\n",
    "        enhanced_rsi = pd.Series(index=full_index, data=np.nan)\n",
    "        enhanced_rsi.loc[rsi.index] = rsi\n",
    "\n",
    "        if len(features) < self.knn_lookback:\n",
    "            return enhanced_rsi\n",
    "\n",
    "        feature_array = features.values\n",
    "        rsi_array = rsi.values\n",
    "\n",
    "        # Основное изменение: BallTree строится на скользящем окне\n",
    "        for t in range(self.knn_lookback, len(feature_array)):\n",
    "            window_start = t - self.knn_lookback\n",
    "            window_end = t\n",
    "            X_window = feature_array[window_start:window_end]\n",
    "            y_window = rsi_array[window_start:window_end]\n",
    "\n",
    "            # Строим дерево только на текущем окне\n",
    "            tree = BallTree(X_window, metric='euclidean')\n",
    "            distances, indices = tree.query(feature_array[t].reshape(1, -1),\n",
    "                                          k=self.knn_neighbors)\n",
    "\n",
    "            # Точное воспроизведение оригинальной логики взвешивания\n",
    "            weights = np.where(distances[0] < 1e-6, 1.0, 1.0 / distances[0])\n",
    "            prediction = np.average(y_window[indices[0]], weights=weights)\n",
    "\n",
    "            idx = common_index[t]\n",
    "            enhanced_rsi.loc[idx] = (1 - self.knn_weight) * rsi.loc[idx] + self.knn_weight * prediction\n",
    "\n",
    "        return enhanced_rsi\n",
    "\n",
    "    def kalman_filter(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Калман-фильтр с параметрами ближе к PineScript\"\"\"\n",
    "        n = len(series)\n",
    "        xhat = np.full(n, np.nan)\n",
    "        P = np.zeros(n)\n",
    "        R = self.filter_strength * 0.1  # Очень маленький measurement noise\n",
    "        Q = self.filter_strength * 0.01  # Очень маленький process noise\n",
    "\n",
    "        first_valid_idx = series.first_valid_index()\n",
    "        if first_valid_idx is None:\n",
    "            return pd.Series(xhat, index=series.index)\n",
    "\n",
    "        first_idx = series.index.get_loc(first_valid_idx)\n",
    "        xhat[first_idx] = series.iloc[first_idx]\n",
    "        P[first_idx] = 1.0\n",
    "\n",
    "        for k in range(first_idx + 1, n):\n",
    "            if np.isnan(series.iloc[k]):\n",
    "                xhat[k] = xhat[k - 1]\n",
    "                P[k] = P[k - 1] + Q\n",
    "            else:\n",
    "                xhatminus = xhat[k-1]\n",
    "                Pminus = P[k-1] + Q\n",
    "                K = Pminus / (Pminus + R)\n",
    "                xhat[k] = xhatminus + K * (series.iloc[k] - xhatminus)\n",
    "                P[k] = (1 - K) * Pminus\n",
    "\n",
    "        return pd.Series(xhat, index=series.index)\n",
    "\n",
    "    def filter_series(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Применение фильтрации к финальному RSI\"\"\"\n",
    "        if self.filter_method == 'None':\n",
    "            return series\n",
    "        elif self.filter_method == 'Kalman':\n",
    "            return self.kalman_filter(series)\n",
    "        elif self.filter_method == 'DoubleEMA':\n",
    "            ema1 = series.ewm(span=int(self.filter_strength * 10)).mean()\n",
    "            ema2 = ema1.ewm(span=int(self.filter_strength * 5)).mean()\n",
    "            return ema2\n",
    "        elif self.filter_method == 'ALMA':\n",
    "            return self.smooth(series)\n",
    "        else:\n",
    "            return series\n",
    "\n",
    "    def week_level(self, close):\n",
    "        sma_length = self.sma_length\n",
    "        ema_length = self.ema_length\n",
    "\n",
    "        # Вычисление 20-недельной SMA\n",
    "        SMA_20w = close.rolling(window=sma_length, min_periods=1).mean()\n",
    "\n",
    "        # Вычисление 21-недельной EMA\n",
    "        MA_21w = close.ewm(span=ema_length, adjust=False).mean()\n",
    "\n",
    "        return SMA_20w, MA_21w\n",
    "\n",
    "\n",
    "    def fit(self, close: pd.Series) -> pd.Series:\n",
    "        \"\"\"Основная функция расчёта\"\"\"\n",
    "        rsi = self.calculate_rsi(close, self.rsi_length)\n",
    "        if self.use_smoothing:\n",
    "            rsi = self.smooth(rsi)\n",
    "        if self.use_knn:\n",
    "            features = self.feature_extraction(close, rsi)\n",
    "\n",
    "            rsi = self.apply_knn(features, rsi)\n",
    "\n",
    "        if self.use_filter:\n",
    "            rsi = self.filter_series(rsi)\n",
    "\n",
    "        sma, ma = self.week_level(close)\n",
    "\n",
    "        return rsi.clip(0, 100), sma, ma\n",
    "\n",
    "\n",
    "class TinkoffHistoricalDataCollector:\n",
    "    def __init__(self):\n",
    "        self.sma_state = {}\n",
    "\n",
    "    def generateVar(self, high_array, low_array, moving_average_length=10):\n",
    "        valpha = 2 / (moving_average_length + 1)\n",
    "        hl2 = (high_array + low_array) / 2\n",
    "\n",
    "        before_val = hl2[0] if len(hl2) > 0 else 0\n",
    "\n",
    "        vud1 = []\n",
    "        vdd1 = []\n",
    "        for current_hl2 in hl2:\n",
    "            if current_hl2 > before_val:\n",
    "                vud1.append(current_hl2 - before_val)\n",
    "                vdd1.append(0)\n",
    "            elif current_hl2 < before_val:\n",
    "                vdd1.append(before_val - current_hl2)\n",
    "                vud1.append(0)\n",
    "            else:\n",
    "                vud1.append(0)\n",
    "                vdd1.append(0)\n",
    "            before_val = current_hl2\n",
    "\n",
    "        def calculate_window_sums(arr, window_size=9):\n",
    "          return [sum(arr[max(0, i - window_size + 1):i+1]) for i in range(len(arr))]\n",
    "\n",
    "        vUD = calculate_window_sums(vud1, 9)\n",
    "        vDD = calculate_window_sums(vdd1, 9)\n",
    "\n",
    "        vUD_ar = np.array(vUD)\n",
    "        vDD_ar = np.array(vDD)\n",
    "\n",
    "        epsilon = 1e-10\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            vCMO = np.divide(vUD_ar - vDD_ar, vUD_ar + vDD_ar + epsilon)\n",
    "\n",
    "        vCMO = np.nan_to_num(vCMO, nan=0.0)\n",
    "\n",
    "        var = []\n",
    "        var_before = 0.0\n",
    "        for i in range(len(hl2)):\n",
    "            if i < len(vCMO):\n",
    "                cmo = abs(vCMO[i])\n",
    "                var_current = (valpha * cmo * hl2[i]) + (1 - valpha * cmo) * var_before\n",
    "            else:\n",
    "                var_current = var_before\n",
    "            var.append(var_current)\n",
    "            var_before = var_current\n",
    "\n",
    "        return np.array(var)\n",
    "\n",
    "    def generateAma(self, high_array, low_array, close_array, atr_period=14, min_period=5, max_period=50):\n",
    "        \"\"\"\n",
    "        Генерация адаптивного скользящего среднего на основе волатильности.\n",
    "\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param close_array: Массив значений close.\n",
    "        :param atr_period: Период для расчета ATR.\n",
    "        :param min_period: Минимальный период скользящего среднего.\n",
    "        :param max_period: Максимальный период скользящего среднего.\n",
    "        :return: Массив значений адаптивного скользящего среднего.\n",
    "        \"\"\"\n",
    "        # Рассчитываем ATR\n",
    "        atr = self._calculate_atr(high_array, low_array, close_array, atr_period)\n",
    "\n",
    "        # Нормализуем ATR для использования в качестве коэффициента\n",
    "        normalized_atr = (atr - np.min(atr)) / (np.max(atr) - np.min(atr) + 1e-10)\n",
    "\n",
    "        # Рассчитываем динамический период\n",
    "        dynamic_period = min_period + (max_period - min_period) * normalized_atr\n",
    "\n",
    "        # Рассчитываем адаптивное скользящее среднее (гибрид SMA и EMA)\n",
    "        adaptive_ma = np.zeros_like(close_array)\n",
    "        for i in range(len(close_array)):\n",
    "            if i < int(dynamic_period[i]):\n",
    "                adaptive_ma[i] = np.mean(close_array[:i+1])  # SMA для начальных значений\n",
    "            else:\n",
    "                period = int(dynamic_period[i])\n",
    "                alpha = 2 / (period + 1)\n",
    "                adaptive_ma[i] = alpha * close_array[i] + (1 - alpha) * adaptive_ma[i-1]  # EMA\n",
    "\n",
    "        return adaptive_ma\n",
    "\n",
    "    def _calculate_atr(self, high_array, low_array, close_array, period=14):\n",
    "        \"\"\"\n",
    "        Рассчитывает Average True Range (ATR).\n",
    "\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param close_array: Массив значений close.\n",
    "        :param period: Период для расчета ATR.\n",
    "        :return: Массив значений ATR.\n",
    "        \"\"\"\n",
    "        tr = np.zeros_like(high_array)\n",
    "        tr[0] = high_array[0] - low_array[0]\n",
    "\n",
    "        for i in range(1, len(high_array)):\n",
    "            hl = high_array[i] - low_array[i]\n",
    "            hc = abs(high_array[i] - close_array[i-1])\n",
    "            lc = abs(low_array[i] - close_array[i-1])\n",
    "            tr[i] = max(hl, hc, lc)\n",
    "\n",
    "        atr = np.zeros_like(tr)\n",
    "        atr[period-1] = np.mean(tr[:period])\n",
    "\n",
    "        for i in range(period, len(tr)):\n",
    "            atr[i] = (atr[i-1] * (period-1) + tr[i]) / period\n",
    "\n",
    "        return atr\n",
    "\n",
    "    def generateAtr(self, high_array, low_array, close_array, period=14):\n",
    "\n",
    "        # Рассчитываем True Range (TR)\n",
    "        tr1 = high_array - low_array\n",
    "        tr2 = np.abs(high_array - np.roll(close_array, 1))\n",
    "        tr3 = np.abs(low_array - np.roll(close_array, 1))\n",
    "\n",
    "        tr = np.maximum(tr1, np.maximum(tr2, tr3))\n",
    "\n",
    "        # Рассчитываем ATR\n",
    "        atr = np.zeros_like(tr)\n",
    "        atr[period - 1] = np.mean(tr[:period])\n",
    "\n",
    "        for i in range(period, len(tr)):\n",
    "            atr[i] = (atr[i - 1] * (period - 1) + tr[i]) / period\n",
    "\n",
    "        return atr\n",
    "\n",
    "    def generateSma(self, high_array, low_array, window=10):\n",
    "        \"\"\"\n",
    "        Генерация Simple Moving Average (SMA).\n",
    "\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param window: Период SMA.\n",
    "        :return: Массив значений SMA.\n",
    "        \"\"\"\n",
    "        hl2 = (high_array + low_array) * 0.5\n",
    "\n",
    "        if window <= 1:\n",
    "            return hl2\n",
    "\n",
    "        # Создаем массив для результатов с NaN\n",
    "        sma = np.full_like(hl2, np.nan)\n",
    "\n",
    "        # Рассчитываем кумулятивную сумму\n",
    "        cumsum = np.cumsum(hl2)\n",
    "\n",
    "        # Создаем сдвинутый кумулятивный массив\n",
    "        shifted_cumsum = np.zeros_like(cumsum)\n",
    "        shifted_cumsum[window:] = cumsum[:-window]\n",
    "\n",
    "        # Вычисляем SMA для валидных периодов\n",
    "        valid = slice(window - 1, None)\n",
    "        sma[valid] = (cumsum[valid] - shifted_cumsum[valid]) / window\n",
    "\n",
    "        return sma\n",
    "\n",
    "    def generatePMax(self, var_array, close_array, high_array, low_array, atr_period, atr_multiplier):\n",
    "        \"\"\"\n",
    "        Генерация PMax (Profit Maximizer).\n",
    "\n",
    "        :param var_array: Массив значений скользящего среднего.\n",
    "        :param close_array: Массив значений close.\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param atr_period: Период для расчета ATR.\n",
    "        :param atr_multiplier: Множитель ATR.\n",
    "        :return: Массив значений PMax.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            atr = self.generateAtr(high_array, low_array, close_array, period=atr_period)\n",
    "        except Exception as exp:\n",
    "            print('exception in atr:', str(exp), flush=True)\n",
    "            return []\n",
    "\n",
    "        previous_final_upperband = 0\n",
    "        previous_final_lowerband = 0\n",
    "        final_upperband = 0\n",
    "        final_lowerband = 0\n",
    "        previous_var = 0\n",
    "        previous_pmax = 0\n",
    "        pmax = []\n",
    "        pmaxc = 0\n",
    "\n",
    "        for i in range(0, len(close_array)):\n",
    "            if np.isnan(close_array[i]):\n",
    "                pass\n",
    "            else:\n",
    "                atrc = atr[i]\n",
    "                varc = var_array[i]\n",
    "\n",
    "                if math.isnan(atrc):\n",
    "                    atrc = 0\n",
    "\n",
    "                basic_upperband = varc + atr_multiplier * atrc\n",
    "                basic_lowerband = varc - atr_multiplier * atrc\n",
    "\n",
    "                if basic_upperband < previous_final_upperband or previous_var > previous_final_upperband:\n",
    "                    final_upperband = basic_upperband\n",
    "                else:\n",
    "                    final_upperband = previous_final_upperband\n",
    "\n",
    "                if basic_lowerband > previous_final_lowerband or previous_var < previous_final_lowerband:\n",
    "                    final_lowerband = basic_lowerband\n",
    "                else:\n",
    "                    final_lowerband = previous_final_lowerband\n",
    "\n",
    "                if previous_pmax == previous_final_upperband and varc <= final_upperband:\n",
    "                    pmaxc = final_upperband\n",
    "                else:\n",
    "                    if previous_pmax == previous_final_upperband and varc >= final_upperband:\n",
    "                        pmaxc = final_lowerband\n",
    "                    else:\n",
    "                        if previous_pmax == previous_final_lowerband and varc >= final_lowerband:\n",
    "                            pmaxc = final_lowerband\n",
    "                        elif previous_pmax == previous_final_lowerband and varc <= final_lowerband:\n",
    "                            pmaxc = final_upperband\n",
    "\n",
    "                pmax.append(pmaxc)\n",
    "\n",
    "                previous_var = varc\n",
    "\n",
    "                previous_final_upperband = final_upperband\n",
    "\n",
    "                previous_final_lowerband = final_lowerband\n",
    "\n",
    "                previous_pmax = pmaxc\n",
    "\n",
    "        return pmax\n",
    "\n",
    "    def generate_signals(self, df, moving_average_length=10, atr_period=10, atr_multiplier=3, average_type='SMA',\n",
    "                        ama_params=None):\n",
    "        \"\"\"\n",
    "        Генерация сигналов на основе SMA или AMA.\n",
    "\n",
    "        :param df: DataFrame с данными.\n",
    "        :param moving_average_length: Период скользящего среднего.\n",
    "        :param atr_period: Период ATR.\n",
    "        :param atr_multiplier: Множитель ATR.\n",
    "        :param average_type: Тип скользящего среднего ('SMA' или 'AMA').\n",
    "        :param ama_params: Параметры для AMA (если используется).\n",
    "        :return: DataFrame с добавленными сигналами.\n",
    "        \"\"\"\n",
    "        high_array = df[\"high\"].values\n",
    "        low_array = df[\"low\"].values\n",
    "        close_array = df[\"close\"].values\n",
    "        df = df.copy()\n",
    "\n",
    "        if average_type == 'SMA':\n",
    "            ma_arr = self.generateSma(high_array, low_array, moving_average_length)\n",
    "        elif average_type == 'VAR':\n",
    "            ma_arr = self.generateVar(high_array, low_array, moving_average_length)\n",
    "        elif average_type == 'AMA':\n",
    "            if ama_params is None:\n",
    "                raise ValueError(\"Для AMA необходимо указать параметры ama_params.\")\n",
    "            ma_arr = self.generateAma(high_array, low_array, close_array, **ama_params)\n",
    "        else:\n",
    "            raise ValueError(\"Неподдерживаемый тип скользящего среднего.\")\n",
    "\n",
    "        pmax = self.generatePMax(ma_arr, close_array, high_array, low_array, atr_period, atr_multiplier)\n",
    "        df[\"pmax\"] = pmax\n",
    "        df[\"ma\"] = ma_arr\n",
    "        df[\"buy_signal\"] = (df[\"ma\"] > df[\"pmax\"]) & (df[\"ma\"].shift(1) < df[\"pmax\"].shift(1))\n",
    "        df[\"sell_signal\"] = (df[\"ma\"] < df[\"pmax\"]) & (df[\"ma\"].shift(1) > df[\"pmax\"].shift(1))\n",
    "\n",
    "        return df\n",
    "\n",
    "def calculate_target(df, threshold=3.0):\n",
    "    # Проверка необходимых колонок\n",
    "    required_columns = ['event_price', 'event_sell_price']\n",
    "    missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Отсутствуют необходимые колонки: {missing_cols}\")\n",
    "\n",
    "    # Копируем DataFrame чтобы не менять оригинал\n",
    "    result_df = df.copy()\n",
    "\n",
    "    # Рассчитываем процентное изменение\n",
    "    result_df['price_change_pct'] = (\n",
    "        (result_df['event_sell_price'] / result_df['event_price'] - 1) * 100\n",
    "    )\n",
    "\n",
    "    # Создаем целевой признак\n",
    "    result_df['target'] = (result_df['price_change_pct'] >= threshold).astype(int)\n",
    "\n",
    "    # Обработка случаев с отсутствующими данными\n",
    "    result_df['target'] = result_df['target'].where(\n",
    "        result_df[['event_price', 'event_sell_price']].notnull().all(axis=1),\n",
    "        other=0\n",
    "    )\n",
    "\n",
    "    # Обработка случаев с нулевой ценой покупки (если такие есть)\n",
    "    result_df['target'] = result_df['target'].where(\n",
    "        result_df['event_price'] != 0,\n",
    "        other=0\n",
    "    )\n",
    "\n",
    "    # Удаляем временную колонку\n",
    "    result_df.drop('price_change_pct', axis=1, inplace=True)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "class FastRollingMode:\n",
    "    def __init__(self, window_size):\n",
    "        self.window = deque(maxlen=window_size)\n",
    "        self.counts = {}\n",
    "        \n",
    "    def update(self, new_val):\n",
    "        if len(self.window) == self.window.maxlen:\n",
    "            old_val = self.window.popleft()\n",
    "            self.counts[old_val] -= 1\n",
    "            if self.counts[old_val] == 0:\n",
    "                del self.counts[old_val]\n",
    "        \n",
    "        self.window.append(new_val)\n",
    "        self.counts[new_val] = self.counts.get(new_val, 0) + 1\n",
    "        return max(self.counts.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "def extract_features(df: pd.DataFrame, window: int = 126):\n",
    "    \"\"\"\n",
    "    Вычисляет устойчивые признаки для кластеризации рыночных режимов.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_macd(df, macd_fast_periods=[12], macd_slow_periods=[26], macd_signal_periods=[9]):\n",
    "        \"\"\"\n",
    "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
    "        \"\"\"\n",
    "        close = df['close']\n",
    "\n",
    "        # Создаем множества для уникальных периодов\n",
    "        unique_fast = set(macd_fast_periods)\n",
    "        unique_slow = set(macd_slow_periods)\n",
    "\n",
    "\n",
    "        # Предварительно вычисляем все необходимые EMA и скользящие средние\n",
    "        ema_cache = {}\n",
    "        rolling_cache = {}\n",
    "\n",
    "        # Кешируем быстрые EMA\n",
    "        for fp in unique_fast:\n",
    "            ema_cache[f'ema_{fp}'] = close.ewm(span=fp, adjust=False).mean()\n",
    "\n",
    "        # Кешируем медленные EMA и скользящие средние\n",
    "        for sp in unique_slow:\n",
    "            ema_cache[f'ema_{sp}'] = close.ewm(span=sp, adjust=False).mean()\n",
    "            rolling_cache[f'rolling_{sp}'] = close.rolling(window=sp).mean()\n",
    "\n",
    "        # Основной цикл вычислений\n",
    "        for fp in macd_fast_periods:\n",
    "            ema_fast = ema_cache[f'ema_{fp}']\n",
    "            for sp in macd_slow_periods:\n",
    "                ema_slow = ema_cache[f'ema_{sp}']\n",
    "                rolling_mean = rolling_cache[f'rolling_{sp}']\n",
    "\n",
    "                # Вычисляем MACD и нормализацию\n",
    "                macd = ema_fast - ema_slow\n",
    "                macd_norm = macd / rolling_mean\n",
    "\n",
    "                # Сохраняем MACD только один раз для комбинации fp/sp\n",
    "\n",
    "                # Обрабатываем сигнальные периоды\n",
    "                for sig in macd_signal_periods:\n",
    "                    # Вычисляем сигнальную линию\n",
    "                    signal = macd.ewm(span=sig, adjust=False).mean()\n",
    "                    signal_norm = signal / rolling_mean\n",
    "\n",
    "        return pd.DataFrame([macd_norm, signal_norm, macd_norm - signal_norm]).T.fillna(0)\n",
    "\n",
    "    def calculate_atr(df, atr_window=14):\n",
    "        \"\"\"\n",
    "        Расчет ATR и его сдвигов.\n",
    "        \"\"\"\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close']\n",
    "    \n",
    "        tr1 = high - low\n",
    "        tr2 = np.abs(high - close.shift(1))\n",
    "        tr3 = np.abs(low - close.shift(1))\n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(atr_window).mean()\n",
    "    \n",
    "        return pd.Series(atr).fillna(0)\n",
    "    \n",
    "    def calculate_rsi(df, rsi_period=14):\n",
    "        \"\"\"\n",
    "        Расчет RSI и его сдвиги.\n",
    "        \"\"\"\n",
    "        close = df['close']\n",
    "        delta = close.diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gain.rolling(rsi_period).mean()\n",
    "        avg_loss = loss.rolling(rsi_period).mean()\n",
    "        rs = avg_gain / (avg_loss + 1e-10)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        return pd.Series(rsi).fillna(0)\n",
    "    \n",
    "    def calculate_bollinger_bands(df, bollinger_window=20):\n",
    "        \"\"\"\n",
    "        Расчет Bollinger Bands (ширины полос) и сдвигов.\n",
    "        \"\"\"\n",
    "        close = df['close']\n",
    "        ma = close.rolling(bollinger_window).mean()\n",
    "        std = close.rolling(bollinger_window).std()\n",
    "        bb_width = (2 * std) / ma\n",
    "    \n",
    "        return pd.Series(bb_width).fillna(0)\n",
    "    \n",
    "    def detect_market_regime(df: pd.DataFrame, window: int = 30, n_clusters: int = 3) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Классифицирует рыночную фазу на основе кластеризации признаков: волатильность, автокорреляция, наклон тренда.\n",
    "        Возвращает метку режима рынка для каждого окна.\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "    \n",
    "        features = []\n",
    "    \n",
    "        for i in range(len(df) - window + 1):\n",
    "            window_df = df.iloc[i:i+window]\n",
    "            close = window_df['close'].values\n",
    "    \n",
    "            # Волатильность (стандартное отклонение)\n",
    "            volatility = np.std(np.diff(close))\n",
    "    \n",
    "            # Наклон тренда (регрессия по времени)\n",
    "            x = np.arange(window)\n",
    "            y = close\n",
    "            slope = np.polyfit(x, y, deg=1)[0]\n",
    "    \n",
    "            # Автокорреляция лаг-1\n",
    "            autocorr = np.corrcoef(close[:-1], close[1:])[0, 1]\n",
    "    \n",
    "            features.append([volatility, slope, autocorr])\n",
    "    \n",
    "        features = np.array(features)\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(features_scaled)\n",
    "    \n",
    "        # Расширим метки до длины df\n",
    "        regime_series = pd.Series(np.nan, index=df.index)\n",
    "        regime_series.iloc[window - 1:] = labels\n",
    "    \n",
    "        return regime_series.fillna(0).ffill().astype(int)\n",
    "    macd_trend = calculate_macd(df, macd_slow_periods=[window], macd_fast_periods=[window//3], \n",
    "                                 macd_signal_periods=[window//6])\n",
    "    atr = calculate_atr(df, atr_window=window)\n",
    "    rel_volatility = atr / df[\"close\"]\n",
    "    rsi_ind = calculate_rsi(df, rsi_period=window//2)\n",
    "    volume_ratio = df['volume'].rolling(window).apply(\n",
    "        lambda x: x[-1]/x.mean(), raw=True\n",
    "    ).fillna(1).values\n",
    "\n",
    "    features = np.column_stack([\n",
    "        macd_trend,\n",
    "        rel_volatility,\n",
    "        rsi_ind,\n",
    "        volume_ratio\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "def find_best_trial_by_weighted_score_extended1(\n",
    "    trials: List[FrozenTrial],\n",
    "    pnl_weight: float = 0.45,\n",
    "    diff_weight: float = 0.45,\n",
    "    score_weight: float = 0.10\n",
    ") -> Tuple[float, int, Dict, str, float, float, float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Ищет лучший трейл по взвешенной сумме метрик для двух стратегий.\n",
    "    \n",
    "    Порядок метрик в trial.values:\n",
    "    0: pnl_rsi\n",
    "    1: pnl_full\n",
    "    2: diff_rsi\n",
    "    3: diff_full\n",
    "    4: score_rsi\n",
    "    5: score_full\n",
    "    \n",
    "    Параметры:\n",
    "        - pnl_weight: вес для метрики PnL (по умолчанию 0.45)\n",
    "        - diff_weight: вес для метрики diff (по умолчанию 0.45)\n",
    "        - score_weight: вес для метрики score (по умолчанию 0.10)\n",
    "    \n",
    "    Возвращает:\n",
    "        - best_score: наивысший взвешенный скор\n",
    "        - best_trial_number: номер лучшего трейла\n",
    "        - best_params: параметры лучшего трейла\n",
    "        - best_strategy: лучшая стратегия ('RSI' или 'RSI_SMA_EMA')\n",
    "        - best_pnl: лучшее PnL\n",
    "        - best_diff: лучшее diff\n",
    "        - best_score_n: лучшее score\n",
    "        - pnl_rsi: PnL RSI для лучшего трейла\n",
    "        - diff_rsi: diff RSI для лучшего трейла\n",
    "        - score_rsi: score RSI для лучшего трейла\n",
    "        - pnl_full: PnL полной стратегии для лучшего трейла\n",
    "        - diff_full: diff полной стратегии для лучшего трейла\n",
    "        - score_full: score полной стратегии для лучшего трейла\n",
    "    \"\"\"\n",
    "    # Проверка корректности весов\n",
    "    total_weight = pnl_weight + diff_weight + score_weight\n",
    "    if not np.isclose(total_weight, 1.0):\n",
    "        raise ValueError(f\"Сумма весов должна быть равна 1.0 (получено {total_weight})\")\n",
    "    \n",
    "    # Находим максимальные значения для нормализации\n",
    "    all_pnls = []\n",
    "    all_diffs = []\n",
    "    all_scores = []\n",
    "    \n",
    "    for trial in trials:\n",
    "        if trial.values and len(trial.values) >= 6:\n",
    "            all_pnls.extend([trial.values[0], trial.values[1]])  # pnl_rsi, pnl_full\n",
    "            all_diffs.extend([trial.values[2], trial.values[3]]) # diff_rsi, diff_full\n",
    "            all_scores.extend([trial.values[4], trial.values[5]]) # score_rsi, score_full\n",
    "    \n",
    "    if not all_pnls:\n",
    "        raise ValueError(\"Нет подходящих трейлов с шестью метриками.\")\n",
    "    \n",
    "    MAX_PNL = max(all_pnls) * 1.5  # Добавляем запас 50%\n",
    "    MAX_DIFF = max(all_diffs) * 1.2 if max(all_diffs) > 0 else 1.0\n",
    "    MAX_SCORE = max(all_scores) * 1.2 if max(all_scores) > 0 else 1.0\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    best_trial_number = -1\n",
    "    best_params = None\n",
    "    best_strategy = None\n",
    "    best_pnl = None\n",
    "    best_diff = None\n",
    "    best_score_n = None\n",
    "\n",
    "    for trial in trials:\n",
    "        if not trial.values or len(trial.values) < 6:\n",
    "            continue\n",
    "            \n",
    "        pnl_rsi, pnl_full, diff_rsi, diff_full, score_rsi, score_full = trial.values\n",
    "        \n",
    "        # Нормализация (с защитой от деления на 0)\n",
    "        norm_pnl_rsi = pnl_rsi / MAX_PNL if MAX_PNL > 0 else 0\n",
    "        norm_pnl_full = pnl_full / MAX_PNL if MAX_PNL > 0 else 0\n",
    "        norm_diff_rsi = diff_rsi / MAX_DIFF if MAX_DIFF > 0 else 0\n",
    "        norm_diff_full = diff_full / MAX_DIFF if MAX_DIFF > 0 else 0\n",
    "        norm_score_rsi = score_rsi / MAX_SCORE if MAX_SCORE > 0 and score_weight > 0 else 0\n",
    "        norm_score_full = score_full / MAX_SCORE if MAX_SCORE > 0 and score_weight > 0 else 0\n",
    "        \n",
    "        # Вычисляем взвешенные scores для обеих стратегий\n",
    "        score_rsi_weighted = (pnl_weight * norm_pnl_rsi + \n",
    "                             diff_weight * norm_diff_rsi + \n",
    "                             score_weight * norm_score_rsi)\n",
    "        \n",
    "        score_full_weighted = (pnl_weight * norm_pnl_full + \n",
    "                              diff_weight * norm_diff_full + \n",
    "                              score_weight * norm_score_full)\n",
    "        \n",
    "        # Выбираем лучшую стратегию в этом трейле\n",
    "        if score_rsi_weighted > score_full_weighted:\n",
    "            current_score = score_rsi_weighted\n",
    "            current_strategy = 'RSI'\n",
    "            current_pnl = pnl_rsi\n",
    "            current_diff = diff_rsi\n",
    "            current_score_n = score_rsi\n",
    "        else:\n",
    "            current_score = score_full_weighted\n",
    "            current_strategy = 'RSI_SMA_EMA'\n",
    "            current_pnl = pnl_full\n",
    "            current_diff = diff_full\n",
    "            current_score_n = score_full\n",
    "        \n",
    "        # Обновляем глобальный лучший результат\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_trial_number = trial.number\n",
    "            best_params = trial.params\n",
    "            best_strategy = current_strategy\n",
    "            best_pnl = current_pnl\n",
    "            best_diff = current_diff\n",
    "            best_score_n = current_score_n\n",
    "\n",
    "    # Получаем все метрики для лучшего трейла\n",
    "    best_trial = next(t for t in trials if t.number == best_trial_number)\n",
    "    pnl_rsi, pnl_full, diff_rsi, diff_full, score_rsi, score_full = best_trial.values\n",
    "\n",
    "    return (\n",
    "        best_score,\n",
    "        best_trial_number,\n",
    "        best_params,\n",
    "        best_strategy,\n",
    "        best_pnl,\n",
    "        best_diff,\n",
    "        best_score_n,\n",
    "        pnl_rsi,\n",
    "        diff_rsi,\n",
    "        score_rsi,\n",
    "        pnl_full,\n",
    "        diff_full,\n",
    "        score_full\n",
    "    )\n",
    "\n",
    "def fast_generate_var(high: np.ndarray,\n",
    "                      low:  np.ndarray,\n",
    "                      L:    int = 10,\n",
    "                      W:    int = 9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized generation of your VAR (CMO‐smoothed) series.\n",
    "    \"\"\"\n",
    "    α0 = 2.0 / (L + 1.0)\n",
    "    hl2 = (high + low) * 0.5\n",
    "\n",
    "    # 1) first diffs\n",
    "    diff = np.empty_like(hl2)\n",
    "    diff[0] = 0.0\n",
    "    diff[1:] = hl2[1:] - hl2[:-1]\n",
    "\n",
    "    # 2) up/down\n",
    "    up = np.where(diff > 0, diff, 0.0)\n",
    "    dn = np.where(diff < 0, -diff, 0.0)\n",
    "\n",
    "    # 3) W‐period sums via convolution\n",
    "    kernel = np.ones(W, dtype=np.float64)\n",
    "    sum_up = np.convolve(up, kernel, mode=\"full\")[: len(up)]\n",
    "    sum_dn = np.convolve(dn, kernel, mode=\"full\")[: len(dn)]\n",
    "\n",
    "    # 4) CMO\n",
    "    denom = sum_up + sum_dn + 1e-10\n",
    "    cmo   = np.abs((sum_up - sum_dn) / denom)\n",
    "\n",
    "    # 5) final EMA‐like smoothing\n",
    "    var = np.empty_like(hl2)\n",
    "    var[0] = 0.0\n",
    "    for i in range(1, len(hl2)):\n",
    "        α      = α0 * cmo[i]\n",
    "        var[i] = α * hl2[i] + (1 - α) * var[i - 1]\n",
    "\n",
    "    return var\n",
    "\n",
    "\n",
    "class MachineLearningRSI:\n",
    "    def __init__(self,\n",
    "                 rsi_length=300,\n",
    "                 use_smoothing=True,\n",
    "                 smoothing_length=268,\n",
    "                 smoothing_type='ALMA',\n",
    "                 alma_sigma=6,\n",
    "                 rsi_overbought=70,\n",
    "                 rsi_oversold=30,\n",
    "                 use_knn=True,\n",
    "                 knn_neighbors=7,\n",
    "                 knn_lookback=500,\n",
    "                 knn_weight=0.6,\n",
    "                 feature_count=5,\n",
    "                 use_filter=True,\n",
    "                 filter_method='Kalman',\n",
    "                 filter_strength=0.7,\n",
    "                 sma_length=20 + 7*24*4*3,\n",
    "                 ema_length=21 + 7*24*4*3\n",
    "                 ):\n",
    "\n",
    "        # Базовые параметры\n",
    "        self.rsi_length = rsi_length\n",
    "        self.use_smoothing = use_smoothing\n",
    "        self.smoothing_length = smoothing_length\n",
    "        self.smoothing_type = smoothing_type\n",
    "        self.alma_sigma = alma_sigma\n",
    "\n",
    "        # Пороговые уровни\n",
    "        self.rsi_overbought = rsi_overbought\n",
    "        self.rsi_oversold = rsi_oversold\n",
    "\n",
    "        # Параметры KNN\n",
    "        self.use_knn = use_knn\n",
    "        self.knn_neighbors = knn_neighbors\n",
    "        self.knn_lookback = knn_lookback\n",
    "        self.knn_weight = knn_weight\n",
    "        self.feature_count = feature_count\n",
    "\n",
    "        # Фильтрация\n",
    "        self.use_filter = use_filter\n",
    "        self.filter_method = filter_method\n",
    "        self.filter_strength = filter_strength\n",
    "\n",
    "        self.sma_length = sma_length\n",
    "        self.ema_length = ema_length\n",
    "\n",
    "    def calculate_rsi(self, close: pd.Series, length: int) -> pd.Series:\n",
    "        \"\"\"Расчет RSI через RMA аналогично PineScript ta.rsi\"\"\"\n",
    "        delta = close.diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "        avg_gain = gain.ewm(alpha=1/length, min_periods=length, adjust=False).mean()\n",
    "        avg_loss = loss.ewm(alpha=1/length, min_periods=length, adjust=False).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    def smooth(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Корректный ALMA\"\"\"\n",
    "        if self.smoothing_type == 'SMA':\n",
    "            return series.rolling(self.smoothing_length).mean()\n",
    "        elif self.smoothing_type == 'EMA':\n",
    "            return series.ewm(span=self.smoothing_length, adjust=False).mean()\n",
    "        elif self.smoothing_type == 'ALMA':\n",
    "            m = self.smoothing_length\n",
    "            offset = 0.85\n",
    "            sigma = self.alma_sigma\n",
    "\n",
    "            def alma(series):\n",
    "                window = np.arange(m)\n",
    "                weights = np.exp(-((window - offset * (m-1))**2) / (2*(sigma**2)))\n",
    "                weights /= weights.sum()\n",
    "                return np.convolve(series, weights, mode='valid')\n",
    "\n",
    "            def alma_causal(series: pd.Series, length: int = 9, offset: float = 0.85, sigma: float = 6) -> pd.Series:\n",
    "                \"\"\"\n",
    "                Казуальная реализация ALMA (Arnaud Legoux Moving Average)\n",
    "                Использует только прошлые и текущие значения, без lookahead bias.\n",
    "                \"\"\"\n",
    "                if length > len(series):\n",
    "                    return pd.Series(np.nan, index=series.index)\n",
    "\n",
    "                # Предвычисление весов ALMA\n",
    "                window = np.arange(length)\n",
    "                m = offset * (length - 1)\n",
    "                s = length / sigma\n",
    "                weights = np.exp(-((window - m) ** 2) / (2 * s ** 2))\n",
    "                weights /= weights.sum()\n",
    "\n",
    "                # Применяем ALMA казуально (rolling + dot product)\n",
    "                alma_vals = []\n",
    "                for i in range(length - 1, len(series)):\n",
    "                    window_data = series.iloc[i - length + 1:i + 1]\n",
    "                    if window_data.isnull().any():\n",
    "                        alma_vals.append(np.nan)\n",
    "                    else:\n",
    "                        alma_vals.append(np.dot(weights, window_data.values))\n",
    "\n",
    "                # Паддинг NaN в начало, чтобы сохранить индекс\n",
    "                alma_series = pd.Series([np.nan] * (length - 1) + alma_vals, index=series.index)\n",
    "\n",
    "                return alma_series\n",
    "\n",
    "            alma_series = alma_causal(series.fillna(method='ffill'), m, offset, sigma)#, index=series.index[pad:-pad])\n",
    "            #alma_series = alma_series.reindex(series.index, method='nearest')\n",
    "            return alma_series\n",
    "        else:\n",
    "            return series\n",
    "\n",
    "    def feature_extraction(self, close: pd.Series, rsi: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"Извлечение признаков для KNN\"\"\"\n",
    "        features = pd.DataFrame(index=close.index)\n",
    "        features['rsi'] = self.normalize(rsi, self.knn_lookback)\n",
    "\n",
    "        if self.feature_count >= 2:\n",
    "            features['momentum_rsi'] = self.normalize(rsi.diff(3), self.knn_lookback)\n",
    "        if self.feature_count >= 3:\n",
    "            features['volatility_rsi'] = self.normalize(rsi.rolling(10).std(), self.knn_lookback)\n",
    "        if self.feature_count >= 4:\n",
    "            features['slope_rsi'] = self.normalize(self.get_slope(rsi, 5), self.knn_lookback)\n",
    "        if self.feature_count >= 5:\n",
    "            features['momentum_price'] = self.normalize(close.diff(5), self.knn_lookback)\n",
    "\n",
    "        return features.dropna()\n",
    "\n",
    "    def normalize(self, series: pd.Series, period: int) -> pd.Series:\n",
    "        \"\"\"Мин-макс нормализация\"\"\"\n",
    "        min_val = series.rolling(period).min()\n",
    "        max_val = series.rolling(period).max()\n",
    "        norm = (series - min_val) / (max_val - min_val)\n",
    "        return norm.clip(0, 1)\n",
    "\n",
    "    def get_slope(self, series: pd.Series, window: int) -> pd.Series:\n",
    "        \"\"\"Расчет наклона линейной регрессии\"\"\"\n",
    "        idx = np.arange(window)\n",
    "        def linreg(x):\n",
    "            A = np.vstack([idx, np.ones(len(idx))]).T\n",
    "            m, c = np.linalg.lstsq(A, x, rcond=None)[0]\n",
    "            return m\n",
    "        return series.rolling(window).apply(linreg, raw=True)\n",
    "\n",
    "    def apply_knn(self, features: pd.DataFrame, rsi: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Оптимизированная, но идентичная оригиналу версия KNN.\n",
    "        Сохраняет точную математику оригинального apply_knn_b с ускорением через BallTree.\n",
    "        \"\"\"\n",
    "        # Сохраняем структуру как в оригинале\n",
    "        full_index = rsi.index\n",
    "        common_index = features.index.intersection(rsi.index)\n",
    "        features = features.loc[common_index]\n",
    "        rsi = rsi.loc[common_index]\n",
    "\n",
    "        enhanced_rsi = pd.Series(index=full_index, data=np.nan)\n",
    "        enhanced_rsi.loc[rsi.index] = rsi\n",
    "\n",
    "        if len(features) < self.knn_lookback:\n",
    "            return enhanced_rsi\n",
    "\n",
    "        feature_array = features.values\n",
    "        rsi_array = rsi.values\n",
    "\n",
    "        # Основное изменение: BallTree строится на скользящем окне\n",
    "        for t in range(self.knn_lookback, len(feature_array)):\n",
    "            window_start = t - self.knn_lookback\n",
    "            window_end = t\n",
    "            X_window = feature_array[window_start:window_end]\n",
    "            y_window = rsi_array[window_start:window_end]\n",
    "\n",
    "            # Строим дерево только на текущем окне\n",
    "            tree = BallTree(X_window, metric='euclidean')\n",
    "            distances, indices = tree.query(feature_array[t].reshape(1, -1),\n",
    "                                          k=self.knn_neighbors)\n",
    "\n",
    "            # Точное воспроизведение оригинальной логики взвешивания\n",
    "            weights = np.where(distances[0] < 1e-6, 1.0, 1.0 / distances[0])\n",
    "            prediction = np.average(y_window[indices[0]], weights=weights)\n",
    "\n",
    "            idx = common_index[t]\n",
    "            enhanced_rsi.loc[idx] = (1 - self.knn_weight) * rsi.loc[idx] + self.knn_weight * prediction\n",
    "\n",
    "        return enhanced_rsi\n",
    "\n",
    "    def kalman_filter(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Калман-фильтр с параметрами ближе к PineScript\"\"\"\n",
    "        n = len(series)\n",
    "        xhat = np.full(n, np.nan)\n",
    "        P = np.zeros(n)\n",
    "        R = self.filter_strength * 0.1  # Очень маленький measurement noise\n",
    "        Q = self.filter_strength * 0.01  # Очень маленький process noise\n",
    "\n",
    "        first_valid_idx = series.first_valid_index()\n",
    "        if first_valid_idx is None:\n",
    "            return pd.Series(xhat, index=series.index)\n",
    "\n",
    "        first_idx = series.index.get_loc(first_valid_idx)\n",
    "        xhat[first_idx] = series.iloc[first_idx]\n",
    "        P[first_idx] = 1.0\n",
    "\n",
    "        for k in range(first_idx + 1, n):\n",
    "            if np.isnan(series.iloc[k]):\n",
    "                xhat[k] = xhat[k - 1]\n",
    "                P[k] = P[k - 1] + Q\n",
    "            else:\n",
    "                xhatminus = xhat[k-1]\n",
    "                Pminus = P[k-1] + Q\n",
    "                K = Pminus / (Pminus + R)\n",
    "                xhat[k] = xhatminus + K * (series.iloc[k] - xhatminus)\n",
    "                P[k] = (1 - K) * Pminus\n",
    "\n",
    "        return pd.Series(xhat, index=series.index)\n",
    "\n",
    "    def filter_series(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Применение фильтрации к финальному RSI\"\"\"\n",
    "        if self.filter_method == 'None':\n",
    "            return series\n",
    "        elif self.filter_method == 'Kalman':\n",
    "            return self.kalman_filter(series)\n",
    "        elif self.filter_method == 'DoubleEMA':\n",
    "            ema1 = series.ewm(span=int(self.filter_strength * 10)).mean()\n",
    "            ema2 = ema1.ewm(span=int(self.filter_strength * 5)).mean()\n",
    "            return ema2\n",
    "        elif self.filter_method == 'ALMA':\n",
    "            return self.smooth(series)\n",
    "        else:\n",
    "            return series\n",
    "\n",
    "    def week_level(self, close):\n",
    "        sma_length = self.sma_length\n",
    "        ema_length = self.ema_length\n",
    "\n",
    "        # Вычисление 20-недельной SMA\n",
    "        SMA_20w = close.rolling(window=sma_length, min_periods=1).mean()\n",
    "\n",
    "        # Вычисление 21-недельной EMA\n",
    "        MA_21w = close.ewm(span=ema_length, adjust=False).mean()\n",
    "\n",
    "        return SMA_20w, MA_21w\n",
    "\n",
    "\n",
    "    def fit(self, close: pd.Series) -> pd.Series:\n",
    "        \"\"\"Основная функция расчёта\"\"\"\n",
    "        rsi = self.calculate_rsi(close, self.rsi_length)\n",
    "        if self.use_smoothing:\n",
    "            rsi = self.smooth(rsi)\n",
    "        if self.use_knn:\n",
    "            features = self.feature_extraction(close, rsi)\n",
    "\n",
    "            rsi = self.apply_knn(features, rsi)\n",
    "\n",
    "        if self.use_filter:\n",
    "            rsi = self.filter_series(rsi)\n",
    "\n",
    "        sma, ma = self.week_level(close)\n",
    "\n",
    "        return rsi.clip(0, 100), sma, ma\n",
    "\n",
    "\n",
    "class TinkoffHistoricalDataCollector:\n",
    "    def __init__(self):\n",
    "        self.sma_state = {}\n",
    "\n",
    "    def generateVar(self, high_array, low_array, moving_average_length=10):\n",
    "        valpha = 2 / (moving_average_length + 1)\n",
    "        hl2 = (high_array + low_array) / 2\n",
    "\n",
    "        before_val = hl2[0] if len(hl2) > 0 else 0\n",
    "\n",
    "        vud1 = []\n",
    "        vdd1 = []\n",
    "        for current_hl2 in hl2:\n",
    "            if current_hl2 > before_val:\n",
    "                vud1.append(current_hl2 - before_val)\n",
    "                vdd1.append(0)\n",
    "            elif current_hl2 < before_val:\n",
    "                vdd1.append(before_val - current_hl2)\n",
    "                vud1.append(0)\n",
    "            else:\n",
    "                vud1.append(0)\n",
    "                vdd1.append(0)\n",
    "            before_val = current_hl2\n",
    "\n",
    "        def calculate_window_sums(arr, window_size=9):\n",
    "          return [sum(arr[max(0, i - window_size + 1):i+1]) for i in range(len(arr))]\n",
    "\n",
    "        vUD = calculate_window_sums(vud1, 9)\n",
    "        vDD = calculate_window_sums(vdd1, 9)\n",
    "\n",
    "        vUD_ar = np.array(vUD)\n",
    "        vDD_ar = np.array(vDD)\n",
    "\n",
    "        epsilon = 1e-10\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            vCMO = np.divide(vUD_ar - vDD_ar, vUD_ar + vDD_ar + epsilon)\n",
    "\n",
    "        vCMO = np.nan_to_num(vCMO, nan=0.0)\n",
    "\n",
    "        var = []\n",
    "        var_before = 0.0\n",
    "        for i in range(len(hl2)):\n",
    "            if i < len(vCMO):\n",
    "                cmo = abs(vCMO[i])\n",
    "                var_current = (valpha * cmo * hl2[i]) + (1 - valpha * cmo) * var_before\n",
    "            else:\n",
    "                var_current = var_before\n",
    "            var.append(var_current)\n",
    "            var_before = var_current\n",
    "\n",
    "        return np.array(var)\n",
    "\n",
    "    def generateAma(self, high_array, low_array, close_array, atr_period=14, min_period=5, max_period=50):\n",
    "        \"\"\"\n",
    "        Генерация адаптивного скользящего среднего на основе волатильности.\n",
    "\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param close_array: Массив значений close.\n",
    "        :param atr_period: Период для расчета ATR.\n",
    "        :param min_period: Минимальный период скользящего среднего.\n",
    "        :param max_period: Максимальный период скользящего среднего.\n",
    "        :return: Массив значений адаптивного скользящего среднего.\n",
    "        \"\"\"\n",
    "        # Рассчитываем ATR\n",
    "        atr = self._calculate_atr(high_array, low_array, close_array, atr_period)\n",
    "\n",
    "        # Нормализуем ATR для использования в качестве коэффициента\n",
    "        normalized_atr = (atr - np.min(atr)) / (np.max(atr) - np.min(atr) + 1e-10)\n",
    "\n",
    "        # Рассчитываем динамический период\n",
    "        dynamic_period = min_period + (max_period - min_period) * normalized_atr\n",
    "\n",
    "        # Рассчитываем адаптивное скользящее среднее (гибрид SMA и EMA)\n",
    "        adaptive_ma = np.zeros_like(close_array)\n",
    "        for i in range(len(close_array)):\n",
    "            if i < int(dynamic_period[i]):\n",
    "                adaptive_ma[i] = np.mean(close_array[:i+1])  # SMA для начальных значений\n",
    "            else:\n",
    "                period = int(dynamic_period[i])\n",
    "                alpha = 2 / (period + 1)\n",
    "                adaptive_ma[i] = alpha * close_array[i] + (1 - alpha) * adaptive_ma[i-1]  # EMA\n",
    "\n",
    "        return adaptive_ma\n",
    "\n",
    "    def _calculate_atr(self, high_array, low_array, close_array, period=14):\n",
    "        \"\"\"\n",
    "        Рассчитывает Average True Range (ATR).\n",
    "\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param close_array: Массив значений close.\n",
    "        :param period: Период для расчета ATR.\n",
    "        :return: Массив значений ATR.\n",
    "        \"\"\"\n",
    "        tr = np.zeros_like(high_array)\n",
    "        tr[0] = high_array[0] - low_array[0]\n",
    "\n",
    "        for i in range(1, len(high_array)):\n",
    "            hl = high_array[i] - low_array[i]\n",
    "            hc = abs(high_array[i] - close_array[i-1])\n",
    "            lc = abs(low_array[i] - close_array[i-1])\n",
    "            tr[i] = max(hl, hc, lc)\n",
    "\n",
    "        atr = np.zeros_like(tr)\n",
    "        atr[period-1] = np.mean(tr[:period])\n",
    "\n",
    "        for i in range(period, len(tr)):\n",
    "            atr[i] = (atr[i-1] * (period-1) + tr[i]) / period\n",
    "\n",
    "        return atr\n",
    "\n",
    "    def generateAtr(self, high_array, low_array, close_array, period=14):\n",
    "\n",
    "        # Рассчитываем True Range (TR)\n",
    "        tr1 = high_array - low_array\n",
    "        tr2 = np.abs(high_array - np.roll(close_array, 1))\n",
    "        tr3 = np.abs(low_array - np.roll(close_array, 1))\n",
    "\n",
    "        tr = np.maximum(tr1, np.maximum(tr2, tr3))\n",
    "\n",
    "        # Рассчитываем ATR\n",
    "        atr = np.zeros_like(tr)\n",
    "        atr[period - 1] = np.mean(tr[:period])\n",
    "\n",
    "        for i in range(period, len(tr)):\n",
    "            atr[i] = (atr[i - 1] * (period - 1) + tr[i]) / period\n",
    "\n",
    "        return atr\n",
    "\n",
    "    def generateSma(self, high_array, low_array, window=10):\n",
    "        \"\"\"\n",
    "        Генерация Simple Moving Average (SMA).\n",
    "\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param window: Период SMA.\n",
    "        :return: Массив значений SMA.\n",
    "        \"\"\"\n",
    "        hl2 = (high_array + low_array) * 0.5\n",
    "\n",
    "        if window <= 1:\n",
    "            return hl2\n",
    "\n",
    "        # Создаем массив для результатов с NaN\n",
    "        sma = np.full_like(hl2, np.nan)\n",
    "\n",
    "        # Рассчитываем кумулятивную сумму\n",
    "        cumsum = np.cumsum(hl2)\n",
    "\n",
    "        # Создаем сдвинутый кумулятивный массив\n",
    "        shifted_cumsum = np.zeros_like(cumsum)\n",
    "        shifted_cumsum[window:] = cumsum[:-window]\n",
    "\n",
    "        # Вычисляем SMA для валидных периодов\n",
    "        valid = slice(window - 1, None)\n",
    "        sma[valid] = (cumsum[valid] - shifted_cumsum[valid]) / window\n",
    "\n",
    "        return sma\n",
    "\n",
    "    def generatePMax(self, var_array, close_array, high_array, low_array, atr_period, atr_multiplier):\n",
    "        \"\"\"\n",
    "        Генерация PMax (Profit Maximizer).\n",
    "\n",
    "        :param var_array: Массив значений скользящего среднего.\n",
    "        :param close_array: Массив значений close.\n",
    "        :param high_array: Массив значений high.\n",
    "        :param low_array: Массив значений low.\n",
    "        :param atr_period: Период для расчета ATR.\n",
    "        :param atr_multiplier: Множитель ATR.\n",
    "        :return: Массив значений PMax.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            atr = self.generateAtr(high_array, low_array, close_array, period=atr_period)\n",
    "        except Exception as exp:\n",
    "            print('exception in atr:', str(exp), flush=True)\n",
    "            return []\n",
    "\n",
    "        previous_final_upperband = 0\n",
    "        previous_final_lowerband = 0\n",
    "        final_upperband = 0\n",
    "        final_lowerband = 0\n",
    "        previous_var = 0\n",
    "        previous_pmax = 0\n",
    "        pmax = []\n",
    "        pmaxc = 0\n",
    "\n",
    "        for i in range(0, len(close_array)):\n",
    "            if np.isnan(close_array[i]):\n",
    "                pass\n",
    "            else:\n",
    "                atrc = atr[i]\n",
    "                varc = var_array[i]\n",
    "\n",
    "                if math.isnan(atrc):\n",
    "                    atrc = 0\n",
    "\n",
    "                basic_upperband = varc + atr_multiplier * atrc\n",
    "                basic_lowerband = varc - atr_multiplier * atrc\n",
    "\n",
    "                if basic_upperband < previous_final_upperband or previous_var > previous_final_upperband:\n",
    "                    final_upperband = basic_upperband\n",
    "                else:\n",
    "                    final_upperband = previous_final_upperband\n",
    "\n",
    "                if basic_lowerband > previous_final_lowerband or previous_var < previous_final_lowerband:\n",
    "                    final_lowerband = basic_lowerband\n",
    "                else:\n",
    "                    final_lowerband = previous_final_lowerband\n",
    "\n",
    "                if previous_pmax == previous_final_upperband and varc <= final_upperband:\n",
    "                    pmaxc = final_upperband\n",
    "                else:\n",
    "                    if previous_pmax == previous_final_upperband and varc >= final_upperband:\n",
    "                        pmaxc = final_lowerband\n",
    "                    else:\n",
    "                        if previous_pmax == previous_final_lowerband and varc >= final_lowerband:\n",
    "                            pmaxc = final_lowerband\n",
    "                        elif previous_pmax == previous_final_lowerband and varc <= final_lowerband:\n",
    "                            pmaxc = final_upperband\n",
    "\n",
    "                pmax.append(pmaxc)\n",
    "\n",
    "                previous_var = varc\n",
    "\n",
    "                previous_final_upperband = final_upperband\n",
    "\n",
    "                previous_final_lowerband = final_lowerband\n",
    "\n",
    "                previous_pmax = pmaxc\n",
    "\n",
    "        return pmax\n",
    "\n",
    "    def generate_signals(self, df, moving_average_length=10, atr_period=10, atr_multiplier=3, average_type='SMA',\n",
    "                        ama_params=None):\n",
    "        \"\"\"\n",
    "        Генерация сигналов на основе SMA или AMA.\n",
    "\n",
    "        :param df: DataFrame с данными.\n",
    "        :param moving_average_length: Период скользящего среднего.\n",
    "        :param atr_period: Период ATR.\n",
    "        :param atr_multiplier: Множитель ATR.\n",
    "        :param average_type: Тип скользящего среднего ('SMA' или 'AMA').\n",
    "        :param ama_params: Параметры для AMA (если используется).\n",
    "        :return: DataFrame с добавленными сигналами.\n",
    "        \"\"\"\n",
    "        high_array = df[\"high\"].values\n",
    "        low_array = df[\"low\"].values\n",
    "        close_array = df[\"close\"].values\n",
    "        df = df.copy()\n",
    "\n",
    "        if average_type == 'SMA':\n",
    "            ma_arr = self.generateSma(high_array, low_array, moving_average_length)\n",
    "        elif average_type == 'VAR':\n",
    "            ma_arr = self.generateVar(high_array, low_array, moving_average_length)\n",
    "        elif average_type == 'AMA':\n",
    "            if ama_params is None:\n",
    "                raise ValueError(\"Для AMA необходимо указать параметры ama_params.\")\n",
    "            ma_arr = self.generateAma(high_array, low_array, close_array, **ama_params)\n",
    "        else:\n",
    "            raise ValueError(\"Неподдерживаемый тип скользящего среднего.\")\n",
    "\n",
    "        pmax = self.generatePMax(ma_arr, close_array, high_array, low_array, atr_period, atr_multiplier)\n",
    "        df[\"pmax\"] = pmax\n",
    "        df[\"ma\"] = ma_arr\n",
    "        df[\"buy_signal\"] = (df[\"ma\"] > df[\"pmax\"]) & (df[\"ma\"].shift(1) < df[\"pmax\"].shift(1))\n",
    "        df[\"sell_signal\"] = (df[\"ma\"] < df[\"pmax\"]) & (df[\"ma\"].shift(1) > df[\"pmax\"].shift(1))\n",
    "\n",
    "        return df\n",
    "\n",
    "def prepare_regime_params(optuna_params):\n",
    "    \"\"\"\n",
    "    Преобразует параметры из формата Optuna в два словаря: базовые параметры режимов и параметры расчета.\n",
    "\n",
    "    Args:\n",
    "        optuna_params (dict): Словарь с параметрами из Optuna\n",
    "\n",
    "    Returns:\n",
    "        dict: Словарь с двумя ключами: 'base_params' (параметры режимов) и 'calc_params' (остальные параметры)\n",
    "    \"\"\"\n",
    "    # Инициализируем словари для базовых параметров и параметров расчета\n",
    "    start_params = {}\n",
    "    base_params = {}\n",
    "    calc_params = {}\n",
    "\n",
    "    # Сначала обрабатываем параметры режимов (0-4)\n",
    "\n",
    "    start_params['moving_average_length'] = optuna_params.get('moving_average_length', 14)\n",
    "    start_params['atr_period'] = optuna_params.get('atr_period', 10)\n",
    "    for regime in range(5):\n",
    "        regime_key = f'regime_{regime}_'\n",
    "        regime_params = {}\n",
    "\n",
    "        # Основные параметры режима\n",
    "        regime_params['average_type'] = optuna_params.get(f'{regime_key}average_type', 'SMA')\n",
    "        regime_params['moving_average_length'] = optuna_params.get(f'{regime_key}ma_length', 50)\n",
    "        regime_params['atr_period'] = optuna_params.get(f'{regime_key}atr_period', 14)\n",
    "        regime_params['atr_multiplier'] = optuna_params.get(f'{regime_key}atr_multiplier', 3.0)\n",
    "\n",
    "        # Параметры AMA, если они есть\n",
    "        ama_atr_period = optuna_params.get(f'{regime_key}ama_atr_period')\n",
    "        ama_min_period = optuna_params.get(f'{regime_key}ama_min_period')\n",
    "        ama_max_period = optuna_params.get(f'{regime_key}ama_max_period')\n",
    "\n",
    "        if regime_params['average_type'] == 'AMA' and all(p is not None for p in [ama_atr_period, ama_min_period, ama_max_period]):\n",
    "            regime_params['ama_params'] = {\n",
    "                'atr_period': int(ama_atr_period),\n",
    "                'min_period': int(ama_min_period),\n",
    "                'max_period': int(ama_max_period)\n",
    "            }\n",
    "\n",
    "        base_params[regime] = regime_params\n",
    "\n",
    "    # Теперь собираем все остальные параметры в calc_params\n",
    "    other_params = [\n",
    "        'rsi_length', 'use_smoothing', 'smoothing_length', 'smoothing_type',\n",
    "        'alma_sigma', 'rsi_overbought', 'rsi_oversold', 'use_knn',\n",
    "        'knn_neighbors', 'knn_lookback', 'knn_weight', 'feature_count',\n",
    "        'use_filter', 'filter_method', 'filter_strength', 'sma_length',\n",
    "        'ema_length', 'rsi_helbuth'\n",
    "    ]\n",
    "\n",
    "    for param in other_params:\n",
    "        if param in optuna_params:\n",
    "            calc_params[param] = optuna_params[param]\n",
    "\n",
    "    return {\n",
    "        'start_params': start_params,\n",
    "        'base_params': base_params,\n",
    "        'calc_params': calc_params\n",
    "    }\n",
    "\n",
    "\n",
    "class AdaptiveTradingSystem:\n",
    "    def __init__(self, regime_params: Dict[int, dict]):\n",
    "        self.regime_params = regime_params\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────\n",
    "    def generate_adaptive_signals(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        regime_series: pd.Series\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        df      = df.reset_index(drop=True)\n",
    "        regimes = regime_series.reset_index(drop=True).astype(int).values\n",
    "        n       = len(df)\n",
    "        high    = df['high'].values\n",
    "        low     = df['low'].values\n",
    "        close   = df['close'].values\n",
    "\n",
    "        # 0) создаём «коллектор» – единая точка доступа ко всем MA/ATR\n",
    "        collector = TinkoffHistoricalDataCollector()\n",
    "\n",
    "        # ────────────────────────────── 1. PRECOMPUTE  ────────────\n",
    "        ma_cache:  Dict[int, np.ndarray] = {}\n",
    "        atr_cache: Dict[int, np.ndarray] = {}\n",
    "\n",
    "        for regime, p in self.regime_params.items():\n",
    "            atype = p['average_type']\n",
    "            L     = p['moving_average_length']\n",
    "            P     = p['atr_period']\n",
    "\n",
    "            # ---- MA ------------------------------------------------\n",
    "            if atype == 'SMA':\n",
    "                # готовая реализация из collector\n",
    "                ma = collector.generateSma(high, low, window=L)\n",
    "\n",
    "            elif atype == 'VAR':\n",
    "                ma = collector.generateVar(high, low, moving_average_length=L)\n",
    "\n",
    "            elif atype == 'EMA':\n",
    "                ma = collector.generateEma(high, low, moving_average_length=L)\n",
    "\n",
    "            elif atype == 'AMA':\n",
    "                ama_pars = p.get('ama_params',                   # защита от None\n",
    "                                 {'atr_period': 14,\n",
    "                                  'min_period': 5,\n",
    "                                  'max_period': 50})\n",
    "\n",
    "                ma = collector.generateAma(\n",
    "                    high, low, close,\n",
    "                    atr_period=ama_pars['atr_period'],\n",
    "                    min_period=ama_pars['min_period'],\n",
    "                    max_period=ama_pars['max_period']\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown MA type {atype!r}\")\n",
    "\n",
    "            ma_cache[regime] = ma\n",
    "\n",
    "            # ---- ATR ----------------------------------------------\n",
    "            atr_cache[regime] = collector.generateAtr(\n",
    "                high, low, close, period=P\n",
    "            )\n",
    "\n",
    "        # ────────────────────────────── 2. МЭРДЖ ПО РЕЖИМАМ ───────\n",
    "        var_all = np.empty(n, dtype=np.float64)\n",
    "        atr_all = np.empty(n, dtype=np.float64)\n",
    "        mul_all = np.empty(n, dtype=np.float64)\n",
    "\n",
    "        for regime, p in self.regime_params.items():\n",
    "            mask          = (regimes == regime)\n",
    "            var_all[mask] = ma_cache[regime][mask]\n",
    "            atr_all[mask] = atr_cache[regime][mask]\n",
    "            mul_all[mask] = p['atr_multiplier']\n",
    "\n",
    "        # Заполняем возможные NaN в начале серии MA одной первой валидной точкой\n",
    "        if np.isnan(var_all[0]):\n",
    "            first_valid = var_all[~np.isnan(var_all)][0]\n",
    "            var_all[np.isnan(var_all)] = first_valid\n",
    "\n",
    "        # ────────────────────────────── 3. PMax STATE MACHINE ─────\n",
    "        pmax_all = np.empty(n, dtype=np.float64)\n",
    "\n",
    "        prev_var = var_all[0]\n",
    "        prev_atr = atr_all[0]\n",
    "        prev_mul = mul_all[0]\n",
    "\n",
    "        prev_fu = prev_var + prev_mul * prev_atr\n",
    "        prev_fl = prev_var - prev_mul * prev_atr\n",
    "        prev_p  = prev_fl                                # стартовое состояние\n",
    "        pmax_all[0] = prev_p\n",
    "\n",
    "        for i in range(1, n):\n",
    "            v   = var_all[i]\n",
    "            a   = atr_all[i]\n",
    "            m   = mul_all[i]\n",
    "\n",
    "            bu  = v + m * a\n",
    "            bl  = v - m * a\n",
    "\n",
    "            fu = bu if (bu < prev_fu or prev_var > prev_fu) else prev_fu\n",
    "            fl = bl if (bl > prev_fl or prev_var < prev_fl) else prev_fl\n",
    "\n",
    "            if prev_p == prev_fu:\n",
    "                p = fu if v <= fu else fl\n",
    "            else:  # prev_p == prev_fl\n",
    "                p = fl if v >= fl else fu\n",
    "\n",
    "            pmax_all[i] = p\n",
    "\n",
    "            prev_var, prev_fu, prev_fl, prev_p = v, fu, fl, p\n",
    "\n",
    "        # ────────────────────────────── 4. СИГНАЛЫ ────────────────\n",
    "        v_prev = np.concatenate(([var_all[0]], var_all[:-1]))\n",
    "        p_prev = np.concatenate(([pmax_all[0]], pmax_all[:-1]))\n",
    "\n",
    "        buy  = (v_prev < p_prev) & (var_all > pmax_all)\n",
    "        sell = (v_prev > p_prev) & (var_all < pmax_all)\n",
    "\n",
    "        # ────────────────────────────── 5. ВЫХОДНОЙ DataFrame ──────\n",
    "        out = df.copy()\n",
    "        out['ma']   = var_all\n",
    "        out['pmax'] = pmax_all\n",
    "        out['buy_signal']    = buy\n",
    "        out['sell_signal']   = sell\n",
    "        out['regime']        = regimes\n",
    "\n",
    "        return out\n",
    "def extract_features(df: pd.DataFrame, window: int = 126):\n",
    "    \"\"\"\n",
    "    Вычисляет устойчивые признаки для кластеризации рыночных режимов.\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_macd(df, macd_fast_periods=[12], macd_slow_periods=[26], macd_signal_periods=[9]):\n",
    "        \"\"\"\n",
    "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
    "        \"\"\"\n",
    "        close = df['close']\n",
    "\n",
    "        # Создаем множества для уникальных периодов\n",
    "        unique_fast = set(macd_fast_periods)\n",
    "        unique_slow = set(macd_slow_periods)\n",
    "\n",
    "\n",
    "        # Предварительно вычисляем все необходимые EMA и скользящие средние\n",
    "        ema_cache = {}\n",
    "        rolling_cache = {}\n",
    "\n",
    "        # Кешируем быстрые EMA\n",
    "        for fp in unique_fast:\n",
    "            ema_cache[f'ema_{fp}'] = close.ewm(span=fp, adjust=False).mean()\n",
    "\n",
    "        # Кешируем медленные EMA и скользящие средние\n",
    "        for sp in unique_slow:\n",
    "            ema_cache[f'ema_{sp}'] = close.ewm(span=sp, adjust=False).mean()\n",
    "            rolling_cache[f'rolling_{sp}'] = close.rolling(window=sp).mean()\n",
    "\n",
    "        # Основной цикл вычислений\n",
    "        for fp in macd_fast_periods:\n",
    "            ema_fast = ema_cache[f'ema_{fp}']\n",
    "            for sp in macd_slow_periods:\n",
    "                ema_slow = ema_cache[f'ema_{sp}']\n",
    "                rolling_mean = rolling_cache[f'rolling_{sp}']\n",
    "\n",
    "                # Вычисляем MACD и нормализацию\n",
    "                macd = ema_fast - ema_slow\n",
    "                macd_norm = macd / rolling_mean\n",
    "\n",
    "                # Сохраняем MACD только один раз для комбинации fp/sp\n",
    "\n",
    "                # Обрабатываем сигнальные периоды\n",
    "                for sig in macd_signal_periods:\n",
    "                    # Вычисляем сигнальную линию\n",
    "                    signal = macd.ewm(span=sig, adjust=False).mean()\n",
    "                    signal_norm = signal / rolling_mean\n",
    "\n",
    "        return pd.DataFrame([macd_norm, signal_norm, macd_norm - signal_norm]).T.fillna(0)\n",
    "\n",
    "    def calculate_atr(df, atr_window=14):\n",
    "        \"\"\"\n",
    "        Расчет ATR и его сдвигов.\n",
    "        \"\"\"\n",
    "        high = df['high']\n",
    "        low = df['low']\n",
    "        close = df['close']\n",
    "\n",
    "        tr1 = high - low\n",
    "        tr2 = np.abs(high - close.shift(1))\n",
    "        tr3 = np.abs(low - close.shift(1))\n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(atr_window).mean()\n",
    "\n",
    "        return pd.Series(atr).fillna(0)\n",
    "\n",
    "    def calculate_rsi(df, rsi_period=14):\n",
    "        \"\"\"\n",
    "        Расчет RSI и его сдвиги.\n",
    "        \"\"\"\n",
    "        close = df['close']\n",
    "        delta = close.diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gain.rolling(rsi_period).mean()\n",
    "        avg_loss = loss.rolling(rsi_period).mean()\n",
    "        rs = avg_gain / (avg_loss + 1e-10)\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "        return pd.Series(rsi).fillna(0)\n",
    "\n",
    "    def calculate_bollinger_bands(df, bollinger_window=20):\n",
    "        \"\"\"\n",
    "        Расчет Bollinger Bands (ширины полос) и сдвигов.\n",
    "        \"\"\"\n",
    "        close = df['close']\n",
    "        ma = close.rolling(bollinger_window).mean()\n",
    "        std = close.rolling(bollinger_window).std()\n",
    "        bb_width = (2 * std) / ma\n",
    "\n",
    "        return pd.Series(bb_width).fillna(0)\n",
    "\n",
    "    def detect_market_regime(df: pd.DataFrame, window: int = 30, n_clusters: int = 3) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Классифицирует рыночную фазу на основе кластеризации признаков: волатильность, автокорреляция, наклон тренда.\n",
    "        Возвращает метку режима рынка для каждого окна.\n",
    "        \"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        features = []\n",
    "\n",
    "        for i in range(len(df) - window + 1):\n",
    "            window_df = df.iloc[i:i+window]\n",
    "            close = window_df['close'].values\n",
    "\n",
    "            # Волатильность (стандартное отклонение)\n",
    "            volatility = np.std(np.diff(close))\n",
    "\n",
    "            # Наклон тренда (регрессия по времени)\n",
    "            x = np.arange(window)\n",
    "            y = close\n",
    "            slope = np.polyfit(x, y, deg=1)[0]\n",
    "\n",
    "            # Автокорреляция лаг-1\n",
    "            autocorr = np.corrcoef(close[:-1], close[1:])[0, 1]\n",
    "\n",
    "            features.append([volatility, slope, autocorr])\n",
    "\n",
    "        features = np.array(features)\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "        # Расширим метки до длины df\n",
    "        regime_series = pd.Series(np.nan, index=df.index)\n",
    "        regime_series.iloc[window - 1:] = labels\n",
    "\n",
    "        return regime_series.fillna(0).ffill().astype(int)\n",
    "    macd_trend = calculate_macd(df, macd_slow_periods=[window], macd_fast_periods=[window//3],\n",
    "                                 macd_signal_periods=[window//6])\n",
    "    atr = calculate_atr(df, atr_window=window)\n",
    "    rel_volatility = atr / df[\"close\"]\n",
    "    rsi_ind = calculate_rsi(df, rsi_period=window//2)\n",
    "    volume_ratio = df['volume'].rolling(window).apply(\n",
    "        lambda x: x[-1]/x.mean(), raw=True\n",
    "    ).fillna(1).values\n",
    "\n",
    "    features = np.column_stack([\n",
    "        macd_trend,\n",
    "        rel_volatility,\n",
    "        rsi_ind,\n",
    "        volume_ratio\n",
    "    ])\n",
    "\n",
    "    return features\n",
    "\n",
    "class FastRollingMode:\n",
    "    def __init__(self, window_size):\n",
    "        self.window = deque(maxlen=window_size)\n",
    "        self.counts = {}\n",
    "\n",
    "    def update(self, new_val):\n",
    "        if len(self.window) == self.window.maxlen:\n",
    "            old_val = self.window.popleft()\n",
    "            self.counts[old_val] -= 1\n",
    "            if self.counts[old_val] == 0:\n",
    "                del self.counts[old_val]\n",
    "\n",
    "        self.window.append(new_val)\n",
    "        self.counts[new_val] = self.counts.get(new_val, 0) + 1\n",
    "        return max(self.counts.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "def find_best_trial_equal_importance(\n",
    "    trials: List[FrozenTrial],\n",
    "    directions: List[str],\n",
    "    weights: Optional[List[float]] = None,\n",
    ") -> Tuple[float, int, Dict, List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Best trial selection for multi-objective Optuna study.\n",
    "    See docstring above for full description.\n",
    "    \"\"\"\n",
    "\n",
    "    if not trials:\n",
    "        raise ValueError(\"Список trials пуст.\")\n",
    "\n",
    "    # 1. Оставляем только completed-трейлы с валидными (finite) значениями\n",
    "    def _is_finite(vals):\n",
    "        return all(math.isfinite(v) for v in vals)\n",
    "\n",
    "    trials = [\n",
    "        t for t in trials\n",
    "        if (t.values is not None) and _is_finite(t.values)\n",
    "    ]\n",
    "\n",
    "    if not trials:\n",
    "        raise ValueError(\"Нет трейлов с конечными values (без nan/inf).\")\n",
    "\n",
    "    n_obj = len(directions)\n",
    "    if any(len(t.values) != n_obj for t in trials):\n",
    "        raise ValueError(\"Не у всех trials одинаковое число objectives.\")\n",
    "\n",
    "    # 2. Веса\n",
    "    if weights is None:\n",
    "        weights = [1.0 / n_obj] * n_obj\n",
    "    else:\n",
    "        total = sum(weights)\n",
    "        if not np.isclose(total, 1.0):\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "    # 3. min / max по каждому objective\n",
    "    all_vals = np.array([t.values for t in trials], dtype=float)\n",
    "    mins = np.min(all_vals, axis=0)\n",
    "    maxs = np.max(all_vals, axis=0)\n",
    "\n",
    "    # 4. span и защита от деления на 0\n",
    "    spans = np.where(maxs - mins == 0.0, 1.0, maxs - mins)\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_trial = None\n",
    "    best_norm_values = []\n",
    "\n",
    "    for t in trials:\n",
    "        norms = []\n",
    "        for i, (v, d) in enumerate(zip(t.values, directions)):\n",
    "            if d == \"minimize\":\n",
    "                norm = (maxs[i] - v) / spans[i]\n",
    "            elif d == \"maximize\":\n",
    "                norm = (v - mins[i]) / spans[i]\n",
    "            else:\n",
    "                raise ValueError(f\"Непонятное направление {d!r} (i={i})\")\n",
    "            norms.append(norm)\n",
    "\n",
    "        aggregated = float(np.dot(weights, norms))\n",
    "\n",
    "        if aggregated > best_score:\n",
    "            best_score = aggregated\n",
    "            best_trial = t\n",
    "            best_norm_values = norms\n",
    "\n",
    "    # Если все трейлы «не дотянули» (best_score остался −inf)\n",
    "    if best_trial is None:\n",
    "        raise RuntimeError(\"Не удалось выбрать лучший trial.\")\n",
    "\n",
    "    return (\n",
    "        best_score,\n",
    "        best_trial.number,\n",
    "        best_trial.params,\n",
    "        list(best_trial.values),\n",
    "        best_norm_values,\n",
    "    )\n",
    "\n",
    "def _whittaker_smooth(y: np.ndarray, lam: float = 50.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Whittaker–Eilers smoothing (penalized least squares, D2).\n",
    "    Линейный безфазовый фильтр, хорошо убирает шум, не смещая тренд.\n",
    "    \"\"\"\n",
    "    import scipy.sparse as sp\n",
    "    import scipy.sparse.linalg as spla\n",
    "\n",
    "    n = len(y)\n",
    "    if n <= 2:\n",
    "        return y.copy()\n",
    "\n",
    "    E = sp.eye(n, format=\"csc\")\n",
    "    # Вторая разность (D2): размер (n-2) x n\n",
    "    diagonals = [np.ones(n), -2*np.ones(n), np.ones(n)]\n",
    "    D2 = sp.diags(diagonals, [0, 1, 2], shape=(n-2, n), format=\"csc\")\n",
    "    coef = E + lam * (D2.T @ D2)\n",
    "    z = spla.spsolve(coef, y.astype(float))\n",
    "    return z\n",
    "\n",
    "def _rearrange_preserving_marginal(z_raw: np.ndarray, z_smooth: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Distribution-preserving smoothing via monotone rearrangement:\n",
    "    - сортируем исходный z_raw -> z_sorted,\n",
    "    - берём порядок (argsort) сглаженного z_smooth,\n",
    "    - раскладываем z_sorted по этому порядку.\n",
    "    В итоге: гладко по времени (за счёт порядка z_smooth), но эмпирическое\n",
    "    распределение строго совпадает с исходным (z_raw).\n",
    "    \"\"\"\n",
    "    if len(z_raw) == 0:\n",
    "        return z_raw\n",
    "    order = np.argsort(z_smooth)\n",
    "    z_sorted = np.sort(z_raw)\n",
    "    out = np.empty_like(z_raw)\n",
    "    out[order] = z_sorted\n",
    "    return out\n",
    "\n",
    "def calculate_smoothed_target_qnorm(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    batch_start: int = 0,\n",
    "    epsilon: float = 1e-6,\n",
    "    round_decimals: int = 1,\n",
    "\n",
    "    # базовая нормализация по окну (buy->sell)\n",
    "    tight_spread_thr: float = 1e-4,\n",
    "\n",
    "    # сглаживание в z-домене\n",
    "    smooth_method: Literal[\"gauss\", \"savgol\", \"whittaker\"] = \"gauss\",\n",
    "    gauss_sigma: float = 2.0,\n",
    "    savgol_window: int = 11,   # нечётное\n",
    "    savgol_poly: int = 3,\n",
    "    whittaker_lambda: float = 50.0,\n",
    "\n",
    "    # квантильная нормализация и глобальный маппинг\n",
    "    clip_z: float = 2.5,       # клип в z-подобной шкале перед [-1,1]\n",
    "    tanh_scale: float | None = None,\n",
    "\n",
    "    # джиттер перед ECDF\n",
    "    dequant_jitter: float = 1e-4,\n",
    "\n",
    "    # опция \"обязательно растянуть каждый батч\" (робастная эквализация по квантилям)\n",
    "    per_batch_equalize: bool = False,\n",
    "    per_batch_q: float = 0.01,  # напр. 1% и 99% -> [-1, 1]\n",
    "\n",
    "    random_state: int | None = 42,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Возвращает df с колонками:\n",
    "      - normalized_target ∈ [-1, 1] (единая глобальная шкала)\n",
    "      - batch (int64): идентификатор события (buy→sell)\n",
    "\n",
    "    Алгоритм:\n",
    "      1) Внутри каждого события: base(0..1) -> ECDF -> z_raw ~ N(0,1).\n",
    "      2) Сглаживаем (Gaussian/Savitzky–Golay/Whittaker) -> z_smooth.\n",
    "      3) Rearrangement: z_preserved = rearrange(z_raw, order-of z_smooth).\n",
    "         Это убирает шум, но полностью сохраняет исходное распределение.\n",
    "      4) Глобальная квантильная нормализация: приводим все батчи к общей\n",
    "         маргинальной функции Q_global(p).\n",
    "      5) Единая глобальная шкала (median/MAD), клип до [-clip_z, clip_z],\n",
    "         линейный маппинг в [-1, 1], опционально tanh.\n",
    "      6) (Опционально) per-batch эквализация по квантилям до [-1, 1].\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = df.copy()\n",
    "    df[\"normalized_target\"] = np.nan\n",
    "    df[\"batch\"] = np.nan\n",
    "    df[\"event_sell_time\"] = pd.to_datetime(df[\"event_sell_time\"], utc=True)\n",
    "\n",
    "    if savgol_window % 2 == 0:\n",
    "        savgol_window += 1\n",
    "    use_savgol = (savgol_window >= savgol_poly + 2)\n",
    "\n",
    "    all_indices: list[np.ndarray] = []\n",
    "    all_z_preserved: list[np.ndarray] = []\n",
    "    batch = batch_start\n",
    "\n",
    "    buy_rows = df.index[df[\"buy_signal\"] == True]\n",
    "    for start_i in buy_rows:\n",
    "        sell_time = df.at[start_i, \"event_sell_time\"]\n",
    "        if pd.isna(sell_time):\n",
    "            continue\n",
    "        sell_rows = df.index[df[\"time\"] == sell_time]\n",
    "        if len(sell_rows) == 0:\n",
    "            continue\n",
    "        end_i = sell_rows[0]\n",
    "\n",
    "        mask = (df.index >= start_i) & (df.index <= end_i)\n",
    "        idx = df.index[mask]\n",
    "        if idx.empty:\n",
    "            continue\n",
    "\n",
    "        high_s = df.loc[idx, \"high\"]\n",
    "        low_s  = df.loc[idx, \"low\"]\n",
    "\n",
    "        # Робастные полки только для базовой формы (0..1)\n",
    "        max_p = np.round(high_s.quantile(0.92), round_decimals)\n",
    "        min_p = np.round(low_s .quantile(0.08), round_decimals)\n",
    "        if max_p - min_p < tight_spread_thr:\n",
    "            max_p, min_p = float(high_s.max()), float(low_s.min())\n",
    "\n",
    "        use_profit_norm = (max_p - min_p) < tight_spread_thr\n",
    "\n",
    "        # 1) base 0..1\n",
    "        if not use_profit_norm:\n",
    "            base = (df.loc[idx, \"close\"] - min_p) / (max_p - min_p + 1e-12)\n",
    "        else:\n",
    "            buy_price = df.at[start_i, \"close\"]\n",
    "            max_prof = (high_s.max() - buy_price) / max(buy_price, 1e-12)\n",
    "            max_prof = max(max_prof, epsilon)\n",
    "            base = (df.loc[idx, \"close\"] - buy_price) / (buy_price * max_prof)\n",
    "            base = 0.5 + 0.5 * base\n",
    "\n",
    "        base = np.clip(base.to_numpy(float), epsilon, 1 - epsilon)\n",
    "\n",
    "        # Разбиваем дубликаты\n",
    "        if dequant_jitter and len(base) > 0:\n",
    "            base = base + rng.normal(scale=dequant_jitter, size=base.shape)\n",
    "            base = np.clip(base, epsilon, 1 - epsilon)\n",
    "\n",
    "        # 2) ECDF -> z_raw ~ N(0,1)\n",
    "        n = len(base)\n",
    "        ranks = pd.Series(base, index=idx).rank(method=\"first\").to_numpy()\n",
    "        ecdf = (ranks - 0.5) / max(n, 1)\n",
    "        ecdf = np.clip(ecdf, epsilon, 1 - epsilon)\n",
    "        z_raw = norm.ppf(ecdf)\n",
    "\n",
    "        # 3) Сглаживание (без фазовых сдвигов)\n",
    "        if n > 2:\n",
    "            if smooth_method == \"gauss\":\n",
    "                z_sm = gaussian_filter1d(z_raw, sigma=gauss_sigma, mode=\"nearest\")\n",
    "            elif smooth_method == \"savgol\" and use_savgol and n >= savgol_window:\n",
    "                z_sm = savgol_filter(z_raw, window_length=savgol_window, polyorder=savgol_poly, mode=\"interp\")\n",
    "            elif smooth_method == \"whittaker\":\n",
    "                z_sm = _whittaker_smooth(z_raw, lam=whittaker_lambda)\n",
    "            else:\n",
    "                # запасной вариант, если окно слишком короткое\n",
    "                z_sm = z_raw.copy()\n",
    "        else:\n",
    "            z_sm = z_raw.copy()\n",
    "\n",
    "        # 4) Rearrangement: сохраняем распределение z_raw, но используем порядок z_sm\n",
    "        z_preserved = _rearrange_preserving_marginal(z_raw, z_sm)\n",
    "        #z_preserved = np.where(z_preserved > 0, z_preserved * 1.2, z_preserved)  # Усиливаем положительные пики\n",
    "        #z_preserved = np.clip(z_preserved, -clip_z * 1.5, clip_z * 1.5)\n",
    "\n",
    "        all_indices.append(idx.to_numpy())\n",
    "        all_z_preserved.append(z_preserved)\n",
    "        df.loc[idx, \"batch\"] = batch\n",
    "        batch += 1\n",
    "\n",
    "    # Если событий не нашлось\n",
    "    if len(all_indices) == 0:\n",
    "        df[\"batch\"] = df[\"batch\"].fillna(batch_start).astype(\"int64\", errors=\"ignore\")\n",
    "        df[\"normalized_target\"] = df[\"normalized_target\"].fillna(0.0)\n",
    "        return df\n",
    "\n",
    "    # 5) Глобальная квантильная нормализация: одна общая маргинальная функция\n",
    "    z_pool = np.concatenate(all_z_preserved, axis=0)\n",
    "    z_pool_sorted = np.sort(z_pool)\n",
    "    N = len(z_pool_sorted)\n",
    "    # сетка перцентилей соответствующая отсортированным значениям\n",
    "    p_pool = (np.arange(N) + 0.5) / N\n",
    "\n",
    "    def q_global(p: np.ndarray) -> np.ndarray:\n",
    "        p = np.clip(p, p_pool[0], p_pool[-1])\n",
    "        return np.interp(p, p_pool, z_pool_sorted)\n",
    "\n",
    "    all_z_qn: list[np.ndarray] = []\n",
    "    for z_preserved in all_z_preserved:\n",
    "        n = len(z_preserved)\n",
    "        # перцентиль внутри батча\n",
    "        p_batch = (pd.Series(z_preserved).rank(method=\"first\").to_numpy() - 0.5) / max(n, 1)\n",
    "        z_qn = q_global(p_batch)  # теперь у батча та же маргиналка, что и у пула\n",
    "        all_z_qn.append(z_qn)\n",
    "\n",
    "    # 6) Единая глобальная шкала -> [-1, 1]\n",
    "    g_med = np.median(z_pool)\n",
    "    g_mad = np.median(np.abs(z_pool - g_med)) + 1e-12\n",
    "    scale = 1.4826 * g_mad\n",
    "\n",
    "    pos = 0\n",
    "    for idx, z_qn in zip(all_indices, all_z_qn):\n",
    "        z_g = (z_qn - g_med) / scale\n",
    "        y = np.clip(z_g, -clip_z, clip_z) / clip_z\n",
    "        if tanh_scale:\n",
    "            y = np.tanh(y * tanh_scale) / np.tanh(tanh_scale)\n",
    "\n",
    "        # 7) (опционально) робастная per-batch эквализация до [-1,1]\n",
    "        if per_batch_equalize and len(y) >= 3:\n",
    "            q = per_batch_q\n",
    "            lo, hi = np.quantile(y, [q, 1 - q])\n",
    "            if hi - lo > 1e-12:\n",
    "                y = (y - lo) / (hi - lo)  # [0..1]\n",
    "                y = 2.0 * np.clip(y, 0.0, 1.0) - 1.0  # [-1..1]\n",
    "            y = np.clip(y, -1.0, 1.0)\n",
    "\n",
    "        df.loc[idx, \"normalized_target\"] = y\n",
    "        pos += len(idx)\n",
    "\n",
    "    df[\"batch\"] = df[\"batch\"].astype(\"int64\", errors=\"ignore\")\n",
    "    return df\n",
    "'''def calculate_smoothed_target(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    batch_start: int = 0,\n",
    "    epsilon: float = 1e-6,\n",
    "    round_decimals: int = 1,\n",
    "\n",
    "    # сглаживание\n",
    "    ema_window: int = 8,\n",
    "    z_ema_window: int = 15,\n",
    "    gauss_sigma: float = 2.0,\n",
    "    savgol_window: int = 11,     # должно быть нечётным\n",
    "    savgol_poly: int = 3,\n",
    "\n",
    "    # rob-winsor / рескейл\n",
    "    winsor_pct: float = 0.01,\n",
    "    clip_z: float = 2.5,\n",
    "    tanh_scale: float | None = None,\n",
    "\n",
    "    # jitter\n",
    "    dequant_jitter: float = 1e-4,\n",
    "\n",
    "    tight_spread_thr: float = 1e-4,\n",
    "    random_state: int | None = 42,   # чтобы результат был детерминирован\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Плавный регрессионный таргет [-1 … 1] без «гребёнки» и выбросов.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df   = df.copy()\n",
    "    df[\"normalized_target\"] = np.nan\n",
    "    df[\"batch\"]             = np.nan\n",
    "    df[\"event_sell_time\"]   = pd.to_datetime(df[\"event_sell_time\"], utc=True)\n",
    "\n",
    "    batch = batch_start\n",
    "    for _, buy_row in df[df[\"buy_signal\"]].iterrows():\n",
    "        sell_row = df[df[\"time\"] == buy_row[\"event_sell_time\"]]\n",
    "        if sell_row.empty:\n",
    "            continue\n",
    "\n",
    "        start_i, end_i = buy_row.name, sell_row.index[0]\n",
    "        mask = (df.index >= start_i) & (df.index <= end_i)\n",
    "\n",
    "        high_s, low_s = df.loc[mask, \"high\"], df.loc[mask, \"low\"]\n",
    "\n",
    "        max_p = np.round(high_s.quantile(0.92), round_decimals)\n",
    "        min_p = np.round(low_s .quantile(0.08), round_decimals)\n",
    "        if max_p - min_p < tight_spread_thr:\n",
    "            max_p, min_p = high_s.max(), low_s.min()\n",
    "\n",
    "        use_profit_norm = (max_p - min_p) < tight_spread_thr\n",
    "\n",
    "        # ── 1. базовый 0…1 ────────────────────────────────────────────────\n",
    "        if not use_profit_norm:\n",
    "            base = (df.loc[mask, \"close\"] - min_p) / (max_p - min_p + 1e-12)\n",
    "        else:\n",
    "            buy_price = df.at[start_i, \"close\"]\n",
    "            max_prof  = (high_s.max() - buy_price) / max(buy_price, 1e-12)\n",
    "            max_prof  = max(max_prof, epsilon)\n",
    "            base = (df.loc[mask, \"close\"] - buy_price) / (buy_price * max_prof)\n",
    "            base = 0.5 + 0.5 * base\n",
    "\n",
    "        base = np.clip(base, epsilon, 1 - epsilon)\n",
    "\n",
    "        # ── 2. micro-jitter, чтобы разбить дубликаты ─────────────────────\n",
    "        if dequant_jitter:\n",
    "            base += rng.normal(scale=dequant_jitter, size=base.shape)\n",
    "            base = np.clip(base, epsilon, 1 - epsilon)\n",
    "\n",
    "        # ── 3. EMA по базе ───────────────────────────────────────────────\n",
    "        if ema_window > 1:\n",
    "            base = (\n",
    "                pd.Series(base, index=df.index[mask])\n",
    "                .ewm(alpha=2 / (ema_window + 1), adjust=False)\n",
    "                .mean()\n",
    "                .to_numpy()\n",
    "            )\n",
    "            base = np.clip(base, epsilon, 1 - epsilon)\n",
    "\n",
    "        # ── 4. ECDF → z-score (unique ranks) ─────────────────────────────\n",
    "        ranks = pd.Series(base).rank(method=\"first\").to_numpy()\n",
    "        ecdf  = (ranks - 0.5) / len(ranks)\n",
    "        ecdf  = np.clip(ecdf, epsilon, 1 - epsilon)\n",
    "        z     = norm.ppf(ecdf)\n",
    "\n",
    "        # ── 5. EMA + Gaussian ────────────────────────────────────────────\n",
    "        if z_ema_window > 1:\n",
    "            z = (\n",
    "                pd.Series(z, index=df.index[mask])\n",
    "                .ewm(span=z_ema_window, adjust=False)\n",
    "                .mean()\n",
    "                .to_numpy()\n",
    "            )\n",
    "\n",
    "        if gauss_sigma and len(z) > 1:\n",
    "            z = gaussian_filter1d(z, sigma=gauss_sigma, mode=\"nearest\")\n",
    "\n",
    "        # ── 6. Savitzky-Golay (убираем «зубцы») ──────────────────────────\n",
    "        if len(z) >= savgol_window and savgol_window >= savgol_poly + 2:\n",
    "            if savgol_window % 2 == 0:                   # делаем нечётным\n",
    "                savgol_window += 1\n",
    "            z = savgol_filter(z, window_length=savgol_window,\n",
    "                              polyorder=savgol_poly, mode=\"interp\")\n",
    "\n",
    "        # ── 7. robust-winsor + рескейл ───────────────────────────────────\n",
    "        med = np.median(z)\n",
    "        mad = np.median(np.abs(z - med)) + 1e-12\n",
    "        z_r = (z - med) / (1.4826 * mad)\n",
    "\n",
    "        if winsor_pct:\n",
    "            lo, hi = np.quantile(z_r, [winsor_pct, 1 - winsor_pct])\n",
    "            z_r = np.clip(z_r, lo, hi)\n",
    "\n",
    "        z_r = np.clip(z_r, -clip_z, clip_z)\n",
    "        z_scaled = z_r / clip_z        # → [-1 … 1]\n",
    "\n",
    "        if tanh_scale:\n",
    "            z_scaled = np.tanh(z_scaled * tanh_scale) / np.tanh(tanh_scale)\n",
    "\n",
    "        # ── 8. запись ────────────────────────────────────────────────────\n",
    "        df.loc[mask, [\"normalized_target\", \"batch\"]] = np.column_stack(\n",
    "            [z_scaled, np.full(mask.sum(), batch)]\n",
    "        )\n",
    "        batch += 1\n",
    "    #df['batch'] = df['batch'].astype('int')\n",
    "\n",
    "    return df'''\n",
    "\n",
    "\n",
    "\n",
    "def calculate_smoothed_multi_target(\n",
    "    df: pd.DataFrame,\n",
    "    horizons: list[int] = [0, 5, 10, 20],  # Horizons (0 for original)\n",
    "    future_window_size: int = 10,  # Fixed window size for future max/min calculation to reduce correlation\n",
    "    *,\n",
    "    batch_start: int = 0,\n",
    "    epsilon: float = 1e-6,\n",
    "    round_decimals: int = 1,\n",
    "    ema_window: int = 8,\n",
    "    z_ema_window: int = 15,\n",
    "    gauss_sigma: float = 2.0,\n",
    "    savgol_window: int = 11,\n",
    "    savgol_poly: int = 3,\n",
    "    winsor_pct: float = 0.01,\n",
    "    clip_z: float = 2.5,\n",
    "    tanh_scale: float | None = None,\n",
    "    dequant_jitter: float = 1e-4,\n",
    "    tight_spread_thr: float = 1e-4,\n",
    "    random_state: int | None = 42,\n",
    ") -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = df.copy()\n",
    "    # Initialize columns\n",
    "    tgt_cols = ['normalized_target'] if 0 in horizons else []\n",
    "    tgt_cols += [f\"multi_target_{h}\" for h in horizons if h != 0]\n",
    "    for col in tgt_cols:\n",
    "        df[col] = np.nan\n",
    "    df[\"batch\"] = np.nan\n",
    "    df[\"event_sell_time\"] = pd.to_datetime(df[\"event_sell_time\"], utc=True)\n",
    "\n",
    "    batch = batch_start\n",
    "    for _, buy_row in df[df[\"buy_signal\"]].iterrows():\n",
    "        sell_row = df[df[\"time\"] == buy_row[\"event_sell_time\"]]\n",
    "        if sell_row.empty:\n",
    "            continue\n",
    "\n",
    "        start_i, end_i = buy_row.name, sell_row.index[0]\n",
    "        mask = (df.index >= start_i) & (df.index <= end_i)\n",
    "        batch_df = df.loc[mask].copy()  # Copy for local mods\n",
    "        batch_len = len(batch_df)\n",
    "        batch_indices = batch_df.index\n",
    "\n",
    "        buy_price = batch_df.at[batch_indices[0], \"close\"]\n",
    "\n",
    "        for h in horizons:\n",
    "            tgt_col = 'normalized_target' if h == 0 else f\"multi_target_{h}\"\n",
    "\n",
    "            if h >= batch_len + future_window_size:  # Way too short\n",
    "                df.loc[mask, tgt_col] = 0.0  # Neutral\n",
    "                continue\n",
    "\n",
    "            # Compute per-bar base using individual future windows\n",
    "            base = np.zeros(batch_len)\n",
    "            for i in range(batch_len):\n",
    "                window_start = i + h\n",
    "                # For h=0, include current bar in the window\n",
    "                if h == 0:\n",
    "                    window_start = i\n",
    "                window_end = min(window_start + future_window_size, batch_len)\n",
    "\n",
    "                if window_start >= batch_len or window_end <= window_start:\n",
    "                    # Empty future window: neutral or conservative\n",
    "                    base[i] = 0.5\n",
    "                    continue\n",
    "\n",
    "                slice_df = batch_df.iloc[window_start:window_end]\n",
    "                fut_high = slice_df['high'].max()\n",
    "                fut_low = slice_df['low'].min()\n",
    "\n",
    "                # Round for discretization\n",
    "                fut_high = np.round(fut_high, round_decimals)\n",
    "                fut_low = np.round(fut_low, round_decimals)\n",
    "\n",
    "                spread = fut_high - fut_low\n",
    "                use_profit_norm = spread < tight_spread_thr\n",
    "\n",
    "                if not use_profit_norm:\n",
    "                    base[i] = (batch_df['close'].iloc[i] - fut_low) / (spread + 1e-12)\n",
    "                else:\n",
    "                    # Profit-based normalization, relative to buy_price\n",
    "                    max_prof = max((fut_high - buy_price) / max(buy_price, 1e-12), epsilon)\n",
    "                    base[i] = (batch_df['close'].iloc[i] - buy_price) / (buy_price * max_prof)\n",
    "                    base[i] = 0.5 + 0.5 * base[i]\n",
    "\n",
    "                base[i] = np.clip(base[i], epsilon, 1 - epsilon)\n",
    "\n",
    "            # Now apply all smoothing steps to the per-bar base series\n",
    "            if dequant_jitter:\n",
    "                base += rng.normal(scale=dequant_jitter, size=base.shape)\n",
    "                base = np.clip(base, epsilon, 1 - epsilon)\n",
    "\n",
    "            if ema_window > 1:\n",
    "                base = pd.Series(base, index=batch_indices).ewm(alpha=2 / (ema_window + 1), adjust=False).mean().to_numpy()\n",
    "                base = np.clip(base, epsilon, 1 - epsilon)\n",
    "\n",
    "            ranks = pd.Series(base).rank(method=\"first\").to_numpy()\n",
    "            ecdf = (ranks - 0.5) / len(ranks)\n",
    "            ecdf = np.clip(ecdf, epsilon, 1 - epsilon)\n",
    "            z = norm.ppf(ecdf)\n",
    "\n",
    "            if z_ema_window > 1:\n",
    "                z = pd.Series(z, index=batch_indices).ewm(span=z_ema_window, adjust=False).mean().to_numpy()\n",
    "\n",
    "            if gauss_sigma and len(z) > 1:\n",
    "                z = gaussian_filter1d(z, sigma=gauss_sigma, mode=\"nearest\")\n",
    "\n",
    "            if len(z) >= savgol_window and savgol_window >= savgol_poly + 2:\n",
    "                if savgol_window % 2 == 0:\n",
    "                    savgol_window += 1\n",
    "                z = savgol_filter(z, window_length=savgol_window, polyorder=savgol_poly, mode=\"interp\")\n",
    "\n",
    "            med = np.median(z)\n",
    "            mad = np.median(np.abs(z - med)) + 1e-12\n",
    "            z_r = (z - med) / (1.4826 * mad)\n",
    "\n",
    "            if winsor_pct:\n",
    "                lo, hi = np.quantile(z_r, [winsor_pct, 1 - winsor_pct])\n",
    "                z_r = np.clip(z_r, lo, hi)\n",
    "\n",
    "            z_r = np.clip(z_r, -clip_z, clip_z)\n",
    "            z_scaled = z_r / clip_z\n",
    "\n",
    "            if tanh_scale:\n",
    "                z_scaled = np.tanh(z_scaled * tanh_scale) / np.tanh(tanh_scale)\n",
    "\n",
    "            # Write to DF\n",
    "            df.loc[mask, tgt_col] = z_scaled\n",
    "\n",
    "        df.loc[mask, \"batch\"] = batch\n",
    "        batch += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def deep_elbow(imp: np.ndarray, win: int = 5, eps: float = 0.02) -> int:\n",
    "    \"\"\"\n",
    "    Берём окно длиной win, считаем средний относительный спад.\n",
    "    Первое место, где спад < eps, считаем плато.\n",
    "    \"\"\"\n",
    "    if len(imp) <= win:\n",
    "        return len(imp)\n",
    "    dif = np.abs(np.diff(imp) / (imp[:-1] + 1e-9))\n",
    "    # скользящее среднее\n",
    "    m = np.convolve(dif, np.ones(win) / win, mode=\"valid\")\n",
    "    flat = np.nonzero(m < eps)[0]\n",
    "    return int(flat[0] + win) if flat.size else len(imp)\n",
    "\n",
    "# ───────────────────────────────────────────────────────\n",
    "# 2.  корреляционная чистка (быстрая, исправленная)\n",
    "# ───────────────────────────────────────────────────────\n",
    "def corr_prune(df: pd.DataFrame, feats: list[str], thr=.95) -> list[str]:\n",
    "    if len(feats) < 2 or thr >= 1:\n",
    "        return feats\n",
    "    X  = df[feats].apply(pd.to_numeric, errors='ignore')\n",
    "    C  = X.corr().abs().to_numpy()\n",
    "    keep = []\n",
    "    for i in range(len(feats)):\n",
    "        if not keep or C[i, keep].max() < thr:\n",
    "            keep.append(i)\n",
    "    return [feats[i] for i in keep]\n",
    "\n",
    "# ───────────────────────────────────────────────────────\n",
    "# 3.  универсальный быстрый селектор\n",
    "# ───────────────────────────────────────────────────────\n",
    "def fast_feature_select(\n",
    "        res           : pd.DataFrame,      # feature / importance\n",
    "        df_full       : pd.DataFrame,      # датасет для corr-prune\n",
    "        target_col    : str = \"target\",\n",
    "        *,\n",
    "        method        : str = \"elbow\",     # elbow | deep_elbow | percentile | quantile | top_k\n",
    "        top_k         : int = 150,         # для method=\"top_k\"\n",
    "        perc_limit    : float = .90,       # для method=\"percentile\"\n",
    "        quantile_q    : float = .10,       # для method=\"quantile\"\n",
    "        elbow_eps     : float = .05,       # (> flat %) для (shallow) elbow\n",
    "        deep_win      : int = 5,           # окно для deep_elbow\n",
    "        deep_eps      : float = .02,       # порог для deep_elbow\n",
    "        corr_thr      : float = .95        # корреляционный порог\n",
    ") -> list[str]:\n",
    "\n",
    "    ranked = res.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "    feats  = ranked.feature.to_numpy()\n",
    "    imps   = ranked.importance.to_numpy()\n",
    "\n",
    "    # ---------- 1) сколько оставить  ----------\n",
    "    if method == \"elbow\":                 # одношаговое колено\n",
    "        k = np.argmax(np.abs(np.diff(imps) / (imps[:-1] + 1e-9)) < elbow_eps) + 1\n",
    "        if k == 1:        # колено не найдено\n",
    "            k = len(imps)\n",
    "    elif method == \"deep_elbow\":\n",
    "        k = deep_elbow(imps, win=deep_win, eps=deep_eps)\n",
    "    elif method == \"percentile\":          # кумулятивная доля\n",
    "        cum = np.cumsum(imps)\n",
    "        k   = np.searchsorted(cum / cum[-1], perc_limit) + 1\n",
    "    elif method == \"quantile\":\n",
    "        thr = np.quantile(imps, 1 - quantile_q)\n",
    "        k   = int((imps >= thr).sum())\n",
    "    elif method == \"top_k\":\n",
    "        k = min(top_k, len(feats))\n",
    "    else:\n",
    "        raise ValueError(\"unknown method\")\n",
    "\n",
    "    selected = feats[:k].tolist()\n",
    "\n",
    "    # ---------- 2) корреляционная чистка ----------\n",
    "    selected = corr_prune(\n",
    "        df_full.drop(columns=[target_col], errors='ignore'),\n",
    "        selected,\n",
    "        thr=corr_thr\n",
    "    )\n",
    "\n",
    "    return selected\n",
    "    \n",
    "def patch_feature_timings(cls):\n",
    "    \"\"\"\n",
    "    Оборачивает все методы cls, начинающиеся на _feat_,\n",
    "    и складывает затраченное время в self._timings[method_name].\n",
    "    Вызывать сразу после объявления класса.\n",
    "    \"\"\"\n",
    "    def timed(func):\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            t0 = time.perf_counter()\n",
    "            result = func(self, *args, **kwargs)\n",
    "            dt = time.perf_counter() - t0\n",
    "            # заводим словарь при первом же вызове\n",
    "            if not hasattr(self, \"_timings\"):\n",
    "                self._timings = {}\n",
    "            self._timings[func.__name__] = dt\n",
    "            return result\n",
    "        return wrapper\n",
    "\n",
    "    for name, method in inspect.getmembers(cls, inspect.isfunction):\n",
    "        if name.startswith(\"_feat_\"):             # ← только расчётные функции\n",
    "            setattr(cls, name, timed(method))\n",
    "\n",
    "    return cls\n",
    "\n",
    "def _slope(y):\n",
    "        x = np.arange(len(y))\n",
    "        # линейная регрессия «по формуле»\n",
    "        xm, ym = x.mean(), y.mean()\n",
    "        beta = ((x - xm) * (y - ym)).sum() / ((x - xm)**2).sum()\n",
    "        return beta\n",
    "\n",
    "@njit\n",
    "def _rolling_entropy_exact_numba(x, window):\n",
    "    \"\"\" Точная реализация rolling entropy с bins='auto' для каждого окна. \"\"\"\n",
    "    n = len(x)\n",
    "    res = np.empty(n, dtype=np.float32)\n",
    "    res[:] = np.nan\n",
    "    if window < 2:\n",
    "        return res\n",
    "    for end in range(window - 1, n):\n",
    "        win = x[end - window + 1 : end + 1]\n",
    "        a_min = np.min(win)\n",
    "        a_max = np.max(win)\n",
    "        if a_min == a_max:\n",
    "            res[end] = 0.0\n",
    "            continue\n",
    "        sorted_win = np.sort(win)\n",
    "        idx25 = int(0.25 * window)\n",
    "        idx75 = int(0.75 * window)\n",
    "        q25 = sorted_win[idx25]\n",
    "        q75 = sorted_win[idx75]\n",
    "        iqr = q75 - q25\n",
    "        sturges = int(np.ceil(np.log2(window) + 1))\n",
    "        if iqr > 0:\n",
    "            bin_width = 2.0 * iqr / (window ** (1.0 / 3.0))\n",
    "            fd = int(np.ceil((a_max - a_min) / bin_width))\n",
    "        else:\n",
    "            fd = 1\n",
    "        nbins = max(sturges, fd, 1)\n",
    "        edges = np.empty(nbins + 1, dtype=np.float32)\n",
    "        step = (a_max - a_min) / nbins\n",
    "        edges[0] = a_min\n",
    "        for i in range(1, nbins):\n",
    "            edges[i] = a_min + i * step\n",
    "        edges[nbins] = a_max\n",
    "        counts = np.zeros(nbins, dtype=np.int32)  # int32 достаточно для window<=1e9\n",
    "        for val in win:\n",
    "            idx = np.searchsorted(edges, val, side='right') - 1\n",
    "            if 0 <= idx < nbins:\n",
    "                counts[idx] += 1\n",
    "        ent = 0.0\n",
    "        total = float(window)\n",
    "        for c in counts:\n",
    "            if c > 0:\n",
    "                p = c / total\n",
    "                ent -= p * np.log(p + 1e-10)\n",
    "        res[end] = ent\n",
    "    return res\n",
    "\n",
    "@jit(nopython=True)\n",
    "def rolling_autocorr(arr, window):\n",
    "    n = len(arr)\n",
    "    result = np.full(n, 0.0)\n",
    "    for i in range(n):\n",
    "        start = max(0, i - window + 1)\n",
    "        w = i - start + 1\n",
    "        if w <= 1:\n",
    "            continue\n",
    "        s = arr[start: i + 1]\n",
    "        a = s[1:]\n",
    "        b = s[:-1]\n",
    "        n_pts = w - 1\n",
    "        mean_a = np.sum(a) / n_pts\n",
    "        mean_b = np.sum(b) / n_pts\n",
    "        cov = np.dot(a, b) / n_pts - mean_a * mean_b\n",
    "        var_a = np.dot(a, a) / n_pts - mean_a * mean_a\n",
    "        var_b = np.dot(b, b) / n_pts - mean_b * mean_b\n",
    "        if var_a <= 0 or var_b <= 0:\n",
    "            result[i] = np.nan\n",
    "        else:\n",
    "            std_a = np.sqrt(var_a)\n",
    "            std_b = np.sqrt(var_b)\n",
    "            result[i] = cov / (std_a * std_b)\n",
    "    return result\n",
    "   \n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "@patch_feature_timings\n",
    "class FeatureCalculatorForRegression:\n",
    "    \"\"\"\n",
    "    df  -- исходный OHLCV-DataFrame.\n",
    "    required_features -- список имён колонок, которые нужны модели.\n",
    "    params -- { primitive_name: {... гиперпараметры ...}, 'stat_window': int }.\n",
    "    \"\"\"\n",
    "\n",
    "    _PRIMITIVES = {\n",
    "        \"MEDPRICE\":               \"_feat_base\",\n",
    "        \"MACD\":                   \"_feat_macd\",\n",
    "        \"MACD_Hist\":              \"_feat_macd\",\n",
    "        \"Overbought_Oversold\":    \"_feat_overbought\",\n",
    "        \"Overbought_Oversold_Index_mean\": \"_feat_overbought\",\n",
    "        \"Price_MADist%\":          \"_feat_madist\",\n",
    "        \"Mean_Reversion\":         \"_feat_mean_reversion\",\n",
    "        \"Fear_Greed\":             \"_feat_fear_greed\",\n",
    "        \"perc_var_open_close\":    \"_feat_price_variation\",\n",
    "        \"pmax_norm\":              \"_feat_pmax_ma\",\n",
    "        \"ma_norm\":                \"_feat_pmax_ma\",\n",
    "        \"ma_pmax_norm_rage\":      \"_feat_pmax_ma\",\n",
    "        \"ma_pmax_norm_rage_pct\":  \"_feat_pmax_ma\",\n",
    "        \"slope_trend\":            \"_feat_slope\",\n",
    "        \"ema_trend\":              \"_feat_ema_trend\",\n",
    "        \"hp_trend\":               \"_feat_hp_trend\",\n",
    "        \"trade_bars_counter\":     \"_feat_trade_duration\",\n",
    "        \"ROC\":                    \"_feat_roc\",\n",
    "        \"ATR_norm\":               \"_feat_atr\",\n",
    "        \"BB_Width\":               \"_feat_bb_width\",\n",
    "        \"Asset_Growth\":           \"_feat_asset_growth\",\n",
    "        \"ema_acceleration\":       \"_feat_ema_acceleration\",\n",
    "        \"price_change\":           \"_feat_price_change\",\n",
    "        \"Asset_To_Equity_Ratio\":  \"_feat_asset_to_equity_ratio\",\n",
    "        \"volume_ratio\":           \"_feat_fear_greed_index\",\n",
    "        \"WILLR\":                  \"_feat_willr\",\n",
    "        \"kf_trend\":               \"_feat_kf_trend\",\n",
    "        \"Fractal_Dim\":            \"_feat_fractal_dim\",\n",
    "        \"Peak_Exhaustion_Score\":  \"_feat_peak_exhaustion\",\n",
    "        \"%B_BB\":                  \"_feat_bb_percent\",\n",
    "        \"Kurtosis_roll\":          \"_feat_kurtosis_roll\",\n",
    "        \"OBV_div\":                \"_feat_obv_div\",\n",
    "        \"RSI_slope\":              \"_feat_rsi_slope\",\n",
    "        \"Vol_Decay\":              \"_feat_vol_decay\",\n",
    "        \"Accel_Decay\":            \"_feat_accel_decay\",\n",
    "        \"Entropy_roll\":           \"_feat_entropy_roll\",\n",
    "        \"Wavelet_Var_Ratio\":      \"_feat_wavelet_var\",\n",
    "        \"Autocorr_Lag1\":          \"_feat_autocorr\",\n",
    "        \"Beta_Market\":            \"_feat_beta\",\n",
    "        \"PSC\":                    \"_feat_peak_squeeze_curvature\",\n",
    "        \"PSC_raw\":                \"_feat_peak_squeeze_curvature\",\n",
    "        \"PSC_z\":                  \"_feat_peak_squeeze_curvature\",\n",
    "        \"PSC_sigmoid\":            \"_feat_peak_squeeze_curvature\",\n",
    "    }\n",
    "\n",
    "    # СТАРАЯ: r\"^ago_(\\d+)_\"\n",
    "    # НОВАЯ: умеет и \"ago50_\", и \"ago_50_\"\n",
    "    _LAG_RE  = re.compile(r\"^ago_?(\\d+)_\")\n",
    "    _STAT_RE = re.compile(r\"_(mean|min|max|std|skew|kurt|quantile(\\d{2}))$\")\n",
    "    _LOGSF   = \"_logsf\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        f64 = self.df.select_dtypes(\"float64\").columns\n",
    "        self.df[f64] = self.df[f64].astype(np.float32)\n",
    "        if \"time\" in self.df:\n",
    "            ts = pd.to_datetime(self.df[\"time\"], utc=True, errors=\"coerce\")\n",
    "            self.df[\"hour\"]        = ts.dt.hour.astype(\"int8\")\n",
    "            self.df[\"day_of_week\"] = ts.dt.day_of_week.astype(\"int8\")\n",
    "\n",
    "    def calculate_features(\n",
    "        self,\n",
    "        required_features: Iterable[str],\n",
    "        params: Mapping[str, Mapping[str, Any]] | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "        saved_cols = ['regime', 'normalized_target', 'batch', 'time', 'open', 'close', 'high', 'low', 'volume', 'buy_signal', \n",
    "                      'sell_signal', 'event_sell_time', 'event_sell_price', 'event_time', 'event_price', 'event_sell_time', \n",
    "                      'event_sell_price', 'target', 'pnl', 'ma', 'pmax']\n",
    "        self._params      = defaultdict(dict, params or {})\n",
    "        self._stat_window = self._params.get(\"stat_window\", 50)\n",
    "        for col in required_features:\n",
    "            self._ensure_column(col)\n",
    "        out = self.df[list(required_features)].copy()\n",
    "        f64 = out.select_dtypes(\"float64\").columns\n",
    "        out[f64] = out[f64].astype(np.float32)\n",
    "\n",
    "        for mandatory_col in saved_cols:\n",
    "            if mandatory_col in self.df.columns:\n",
    "                out[mandatory_col] = self.df[mandatory_col]\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def calculate_all_possible_features(self, params: Mapping[str, Mapping[str, Any]] | None = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Вычисляет все возможные фичи, исключая lag-версии для указанных колонок.\n",
    "        Все бесконечные значения (np.inf/-np.inf) заменяются на 0.\n",
    "        Порядок вычислений:\n",
    "        1. Все базовые примитивы\n",
    "        2. Lag-версии фич (кроме исключенных)\n",
    "        3. Статистики для всех фич\n",
    "        \"\"\"\n",
    "        # Инициализация параметров\n",
    "        if not hasattr(self, '_params'):\n",
    "            self._params = defaultdict(dict, params or {})\n",
    "        self._stat_window = self._params.get(\"stat_window\", 50)\n",
    "        \n",
    "        # Колонки, для которых не нужно создавать lag-версии\n",
    "        EXCLUDE_FROM_LAGS = {\n",
    "            'time', 'open', 'close', 'high', 'low', 'volume', \n",
    "            'ma', 'pmax', 'buy_signal', 'sell_signal', 'regime',\n",
    "            'event_time', 'event_price', 'event_sell_time', \n",
    "            'event_sell_price', 'pnl', 'target', 'normalized_target',\n",
    "            'batch', 'hour', 'day_of_week', 'trade_bars_counter'\n",
    "        }\n",
    "        \n",
    "        # 1. Вычисляем все базовые примитивы\n",
    "        all_primitives = list(self._PRIMITIVES.keys())\n",
    "        for primitive in all_primitives:\n",
    "            method_name = self._PRIMITIVES[primitive]\n",
    "            primitive_params = self._params.get(primitive, {})\n",
    "            try:\n",
    "                getattr(self, method_name)(**primitive_params)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при вычислении примитива {primitive}: {str(e)}\")\n",
    "    \n",
    "        # 2. Добавляем lag-версии только для разрешенных фич\n",
    "        numeric_cols = [\n",
    "            col for col in self.df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
    "            if col not in EXCLUDE_FROM_LAGS and  # Исключаем указанные колонки\n",
    "            not self._LAG_RE.match(col) and      # Исключаем уже lag-фичи\n",
    "            not col.endswith(self._LOGSF) and    # Исключаем logsf-фичи\n",
    "            not self._STAT_RE.search(col)        # Исключаем статистики\n",
    "        ]\n",
    "        \n",
    "        lag_periods = [1, 2, 3, 5, 10, 20, 50]  # Стандартные лаги\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            for lag in lag_periods:\n",
    "                lag_col = f\"ago_{lag}_{col}\"\n",
    "                if lag_col not in self.df.columns:\n",
    "                    self.df[lag_col] = self.df[col].shift(lag)\n",
    "        \n",
    "        # 3. Добавляем статистики для всех фич (кроме исключенных)\n",
    "        all_cols_for_stats = [\n",
    "            col for col in self.df.columns \n",
    "            if col not in EXCLUDE_FROM_LAGS and\n",
    "            not col.endswith(self._LOGSF) and\n",
    "            not self._STAT_RE.search(col)\n",
    "        ]\n",
    "        \n",
    "        stats = ['mean', 'std', 'min', 'max', 'skew', 'kurt']\n",
    "        \n",
    "        for col in all_cols_for_stats:\n",
    "            for stat in stats:\n",
    "                stat_col = f\"{col}_{stat}\"\n",
    "                if stat_col not in self.df.columns:\n",
    "                    try:\n",
    "                        self._add_stat(col, stat)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Ошибка при вычислении статистики {stat} для {col}: {str(e)}\")\n",
    "        \n",
    "        # 4. Добавляем logsf-версии только для разрешенных фич\n",
    "        main_cols_for_logsf = [\n",
    "            col for col in numeric_cols \n",
    "            if not col.startswith('ago_') and\n",
    "            not col.endswith(self._LOGSF) and\n",
    "            col not in EXCLUDE_FROM_LAGS\n",
    "        ]\n",
    "        \n",
    "        for col in main_cols_for_logsf:\n",
    "            logsf_col = f\"{col}{self._LOGSF}\"\n",
    "            if logsf_col not in self.df.columns:\n",
    "                try:\n",
    "                    self.df[logsf_col] = norm.logsf(self.df[col])\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при вычислении logsf для {col}: {str(e)}\")\n",
    "    \n",
    "        # 5. Заменяем бесконечные значения на 0\n",
    "        numeric_cols_all = self.df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
    "        self.df[numeric_cols_all] = self.df[numeric_cols_all].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        # Сохраняем все оригинальные колонки\n",
    "        for col in EXCLUDE_FROM_LAGS:\n",
    "            if col in self.df.columns and col not in self.df:\n",
    "                self.df[col] = self.df[col]\n",
    "        \n",
    "        return self.df.copy()\n",
    "\n",
    "    def _ensure_column(self, name: str):\n",
    "        if name in self.df:\n",
    "            return\n",
    "\n",
    "        # 1) lag-префикс \"ago50_\" или \"ago_50_\"\n",
    "        m = self._LAG_RE.match(name)\n",
    "        if m:\n",
    "            lag  = int(m.group(1))\n",
    "            base = name[m.end():]\n",
    "            self._ensure_column(base)\n",
    "            self.df[name] = self.df[base].shift(lag)\n",
    "            return\n",
    "\n",
    "        # 2) _logsf\n",
    "        if name.endswith(self._LOGSF):\n",
    "            base = name[:-len(self._LOGSF)]\n",
    "            self._ensure_column(base)\n",
    "            self.df[name] = norm.logsf(self.df[base])\n",
    "            return\n",
    "\n",
    "        # 3) статистический суффикс\n",
    "        m = self._STAT_RE.search(name)\n",
    "        if m:\n",
    "            stat = m.group(1)\n",
    "            base = name[:m.start()]\n",
    "            self._ensure_column(base)\n",
    "            self._add_stat(base, stat)\n",
    "            return\n",
    "\n",
    "        # 4) примитив\n",
    "        prim = name\n",
    "        if prim.startswith(\"Overbought_Oversold\"):\n",
    "            prim = \"Overbought_Oversold\"\n",
    "        if prim.startswith(\"Fear_Greed\"):\n",
    "            prim = \"Fear_Greed\"\n",
    "        if prim not in self._PRIMITIVES:\n",
    "            raise KeyError(f\"Не знаю, как получить примитив «{prim}» для «{name}»\")\n",
    "        getattr(self, self._PRIMITIVES[prim])(**self._params.get(prim, {}))\n",
    "        if name not in self.df:\n",
    "            raise RuntimeError(f\"После _feat_{prim}() нет колонки «{name}»\")\n",
    "\n",
    "    def _add_stat(self, base: str, stat: str):\n",
    "        col = f\"{base}_{stat}\"\n",
    "        if col in self.df:\n",
    "            return\n",
    "        s = self.df[base]; w = self._stat_window\n",
    "        if stat == \"mean\":\n",
    "            self.df[col] = s.rolling(w).mean()\n",
    "        elif stat == \"std\":\n",
    "            self.df[col] = s.rolling(w).std()\n",
    "        elif stat == \"min\":\n",
    "            self.df[col] = s.rolling(w).min()\n",
    "        elif stat == \"max\":\n",
    "            self.df[col] = s.rolling(w).max()\n",
    "        elif stat == \"skew\":\n",
    "            self.df[col] = s.rolling(w).skew()\n",
    "        elif stat == \"kurt\":\n",
    "            self.df[col] = s.rolling(w).kurt()\n",
    "        elif stat.startswith(\"quantile\"):\n",
    "            q = int(stat[-2:]) / 100\n",
    "            self.df[col] = s.rolling(w).quantile(q)\n",
    "        else:\n",
    "            raise ValueError(f\"Неизвестная stat «{stat}»\")\n",
    "\n",
    "    # ---------------------- ПРИМИТИВЫ ----------------------\n",
    "\n",
    "    def _feat_base(self, medprice: int = 50):\n",
    "        if \"MEDPRICE\" in self.df:\n",
    "            return\n",
    "        self.df[\"MEDPRICE\"]      = (self.df[\"high\"] + self.df[\"low\"]) / 2\n",
    "        self.df[\"MEDPRICE_std\"] = self.df[\"MEDPRICE\"].rolling(medprice).std()\n",
    "\n",
    "    def _feat_macd(self, fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "        \"\"\"\n",
    "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
    "        \"\"\"\n",
    "        if {\"MACD\",\"MACD_Hist\"}.issubset(self.df.columns):\n",
    "            return\n",
    "            \n",
    "        close = self.df['close']\n",
    "        # Создаем множества для уникальных периодов\n",
    "        ema_cache_fp = close.ewm(span=fast, adjust=False).mean()\n",
    "        \n",
    "        ema_cache_sp = close.ewm(span=slow, adjust=False).mean()\n",
    "        rolling_cache = close.rolling(window=slow).mean()\n",
    "        \n",
    "        # Основной цикл вычислений\n",
    "        ema_fast = ema_cache_fp\n",
    "        ema_slow = ema_cache_sp\n",
    "        rolling_mean = rolling_cache\n",
    "        macd = ema_fast - ema_slow\n",
    "        macd_norm = macd / rolling_mean\n",
    "        self.df[f'MACD'] = macd_norm\n",
    "        signal = macd.ewm(span=signal, adjust=False).mean()\n",
    "        signal_norm = signal / rolling_mean\n",
    "    \n",
    "        # Сохраняем результаты\n",
    "        self.df[f'MACD_Hist'] = macd_norm - signal_norm\n",
    "\n",
    "    '''def _feat_macd(self, fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "        if {\"MACD\",\"MACD_Hist\"}.issubset(self.df.columns):\n",
    "            return\n",
    "        c  = self.df[\"close\"]\n",
    "        ef = c.ewm(span=fast, adjust=False).mean()\n",
    "        es = c.ewm(span=slow, adjust=False).mean()\n",
    "        macd = (ef - es) / (c.rolling(slow).mean().add(1e-10))\n",
    "        sig  = macd.ewm(span=signal, adjust=False).mean()\n",
    "        self.df[\"MACD\"]      = macd\n",
    "        self.df[\"MACD_Hist\"] = macd - sig'''\n",
    "\n",
    "    def _feat_overbought(self, rsi_p: int = 14, stoch_p: int = 14):\n",
    "        name = \"Overbought_Oversold_Index\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        c   = self.df[\"close\"]; d = c.diff()\n",
    "        g   = d.clip(lower=0); l = (-d).clip(lower=0)\n",
    "        rs  = g.rolling(rsi_p).mean() / (l.rolling(rsi_p).mean().add(1e-10))\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        lo  = self.df[\"low\"].rolling(stoch_p).min()\n",
    "        hi  = self.df[\"high\"].rolling(stoch_p).max()\n",
    "        st  = 100*(c - lo)/(hi - lo + 1e-10)\n",
    "        self.df[name] = (rsi + st)/2\n",
    "\n",
    "    def _feat_madist(self, span_lenght: int = 200):\n",
    "        name = \"Price_MADist%\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        ema = self.df[\"close\"].ewm(span=span_lenght, adjust=False).mean()\n",
    "        self.df[name] = (self.df[\"close\"]/ema - 1)*100\n",
    "\n",
    "    def _feat_mean_reversion(self, window: int = 20):\n",
    "        name = \"Mean_Reversion\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        ma = self.df[\"close\"].rolling(window).mean()\n",
    "        self.df[name] = self.df[\"close\"] - ma\n",
    "\n",
    "    def _feat_fear_greed(self, window: int = 14):\n",
    "        name = \"Fear_Greed_Index\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        v  = self.df[\"close\"].pct_change().rolling(window).std()\n",
    "        vc = self.df[\"volume\"].pct_change().rolling(window).mean()\n",
    "        tr = self.df[\"close\"]/self.df[\"close\"].rolling(window).mean()\n",
    "        self.df[name] = (v + vc + tr)/3*100\n",
    "\n",
    "    def _feat_price_variation(self):\n",
    "        name = \"perc_var_open_close\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        eps = 1e-10\n",
    "        self.df[name] = (self.df[\"close\"]-self.df[\"open\"])/(self.df[\"open\"]+eps)*100\n",
    "\n",
    "    def _feat_pmax_ma(self,\n",
    "        pmax_ma_length: int = 10,\n",
    "        pmax_ma_length_roll: int = 50,\n",
    "        pct_window: int = 5\n",
    "    ):\n",
    "        need = {\n",
    "            \"pmax_norm\", \"ma_norm\",\n",
    "            \"ma_pmax_norm_rage\", \"ma_pmax_norm_rage_pct\"\n",
    "        }\n",
    "        if need.issubset(self.df.columns):\n",
    "            return\n",
    "        if {\"pmax\",\"ma\"}.difference(self.df.columns):\n",
    "            raise ValueError(\"Нужны 'pmax' и 'ma'\")\n",
    "        c = self.df[\"close\"]\n",
    "        self.df[\"pmax_norm\"]             = (c-self.df[\"pmax\"])/self.df[\"pmax\"]\n",
    "        self.df[\"ma_norm\"]               = (c-self.df[\"ma\"])/self.df[\"ma\"]\n",
    "        self.df[\"ma_pmax_norm_rage\"]     = self.df[\"ma_norm\"] - self.df[\"pmax_norm\"]\n",
    "        # новый примитив — pct-динамика\n",
    "        self.df[\"ma_pmax_norm_rage_pct\"] = \\\n",
    "          self.df[\"ma_pmax_norm_rage\"].pct_change(pct_window).fillna(0)\n",
    "\n",
    "    def _feat_slope(self, slope_lag: int = 300, pct_window: int = 6):\n",
    "        name = \"slope_trend\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
    "        self.df[name] = r.rolling(slope_lag, min_periods=slope_lag)\\\n",
    "                         .apply(_slope, raw=True)\n",
    "\n",
    "    def _feat_ema_trend(self, span: int = 300, pct_window: int = 6):\n",
    "        name = \"ema_trend\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
    "        e = r.ewm(span=span, adjust=False).mean()\n",
    "        self.df[name] = e.diff().fillna(0)\n",
    "\n",
    "    def _feat_asset_to_equity_ratio(self):\n",
    "        \"\"\"\n",
    "        Вычисление коэффициента соотношения активов и собственного капитала.\n",
    "        \"\"\"\n",
    "        name = \"Asset_To_Equity_Ratio\"\n",
    "        asset = self.df['close']\n",
    "        equity = self.df['low']\n",
    "        # Добавляем в DataFrame\n",
    "        self.df[name] = asset / (equity + 1e-10)\n",
    "\n",
    "    def _feat_hp_trend(self, lamb: float = 1600):\n",
    "        name = \"hp_trend\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        y    = np.log(self.df[\"close\"]).fillna(method=\"ffill\")\n",
    "        coef = lamb/(1+lamb)\n",
    "        tr   = np.empty(len(y), dtype=float)\n",
    "        tr[0] = y.iloc[0]\n",
    "        for i in range(1, len(y)):\n",
    "            tr[i] = coef*y.iloc[i] + (1-coef)*tr[i-1]\n",
    "        self.df[name] = np.append([0], np.diff(tr))\n",
    "\n",
    "    def _feat_kf_trend(self,\n",
    "        pct_window: int = 6,\n",
    "        obs_var: float = 1e-4, # σ² ε_t (шум наблюдения)\n",
    "        level_var: float = 1e-5 # σ² η_t (шум уровня)\n",
    "        ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Добавляет к DataFrame колонки:\n",
    "        kf_trend — one-sided Калман-оценка тренда доходностей\n",
    "        kf_trend_logsf — лог-survival-function (z-score) тренда\n",
    "        Полностью каузально, обновляется тик-за-тиком.\n",
    "        \"\"\"\n",
    "\n",
    "        name = 'kf_trend'\n",
    "        # 1. Доходности\n",
    "        r = self.df['close'].pct_change(pct_window).fillna(0)\n",
    "        # 2. Local-level модель: y_t = μ_t + ε_t ;  μ_t = μ_{t-1} + η_t\n",
    "        mod = sm.tsa.UnobservedComponents(r, level='llevel')\n",
    "        \n",
    "        # 3. Параметры модели в log-шкале (требование statsmodels)\n",
    "        params = np.log([obs_var, level_var])\n",
    "        \n",
    "        # 4. Только forward-filter → нет look-ahead bias\n",
    "        res = mod.filter(params)                       # <— односторонний Калман\n",
    "        trend = pd.Series(res.filtered_state[0], index=self.df.index)\n",
    "        \n",
    "        # 5. Запись результата\n",
    "        self.df['kf_trend'] = trend\n",
    "\n",
    "    def _feat_willr(self, window=14):\n",
    "        \"\"\"\n",
    "        Вычисление %R по методу Уильямса (WILLR).\n",
    "        \"\"\"\n",
    "        name = 'WILLR'\n",
    "        high = self.df['high']\n",
    "        low = self.df['low']\n",
    "        close = self.df['close']\n",
    "\n",
    "        highest_high = high.rolling(window).max()\n",
    "        lowest_low = low.rolling(window).min()\n",
    "        \n",
    "        willr = ((highest_high - close) / (highest_high - lowest_low)) * -100\n",
    "        \n",
    "        # Добавляем в DataFrame\n",
    "        self.df[name] = willr\n",
    "\n",
    "    def _feat_fear_greed_index(self, window: int = 14):\n",
    "        \"\"\"\n",
    "        Расчет объема как отношение последнего объема к скользящему среднему.\n",
    "        \"\"\"\n",
    "        name = \"volume_ratio\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        s = self.df[\"volume\"]\n",
    "        self.df[name] = s / s.rolling(window).mean()\n",
    "\n",
    "    def _feat_trade_duration(self):\n",
    "        name = \"trade_bars_counter\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        self.df[name] = np.nan\n",
    "        entries = self.df.index[self.df[\"event_time\"].notna()]\n",
    "        last    = self.df.index[-1]\n",
    "        for st in entries:\n",
    "            sell = self.df.at[st, \"event_sell_time\"]\n",
    "            ends = self.df.index[self.df[\"time\"] == sell]\n",
    "            end  = ends[0] if len(ends) else last\n",
    "            s,e  = self.df.index.get_loc(st), self.df.index.get_loc(end)\n",
    "            self.df.loc[self.df.index[s:e+1], name] = np.arange(e-s+1, dtype=np.float32)\n",
    "\n",
    "    def _feat_roc(self, window: int = 5):\n",
    "        name = \"ROC\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        self.df[name] = self.df[\"close\"].pct_change(window)\n",
    "\n",
    "    def _feat_atr(self, atr_window: int = 14):\n",
    "        name = \"ATR_norm\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        h,l,c = self.df[\"high\"], self.df[\"low\"], self.df[\"close\"]\n",
    "        tr1 = h-l\n",
    "        tr2 = (h-c.shift()).abs()\n",
    "        tr3 = (l-c.shift()).abs()\n",
    "        tr  = pd.concat([tr1,tr2,tr3], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(atr_window).mean()\n",
    "        self.df[name] = atr/c\n",
    "\n",
    "    def _feat_bb_width(self, bb_window: int = 20):\n",
    "        name = \"BB_Width\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        c   = self.df[\"close\"]\n",
    "        ma  = c.rolling(bb_window).mean()\n",
    "        std = c.rolling(bb_window).std()\n",
    "        self.df[name] = 2*std/ma\n",
    "\n",
    "    def _feat_asset_growth(self, window: int = 3):\n",
    "        name = \"Asset_Growth\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        self.df[name] = self.df[\"close\"].pct_change(window).fillna(0)*100\n",
    "\n",
    "    def _feat_ema_acceleration(self, pct_window: int = 3, ema_window: int = 300):\n",
    "        name = \"ema_acceleration\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
    "        e = r.ewm(span=ema_window).mean()\n",
    "        self.df[name] = e.diff(4)\n",
    "\n",
    "    def _feat_price_change(self, window: int = 1):\n",
    "        name = \"price_change\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        self.df[name] = self.df[\"close\"].pct_change(window).fillna(0)\n",
    "        \n",
    "    def _feat_peak_exhaustion(\n",
    "        self,\n",
    "        price_win: int = 60,    # окно \"локального максимума\"\n",
    "        mom_win:   int = 10,    # окно для momentum\n",
    "        vol_win:   int = 20,\n",
    "        atr_win:   int = 14,\n",
    "        z_win:     int = 100    # z-score нормализация\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Peak-Exhaustion Score  ~ 0…1\n",
    "        1 → почти наверху, импульс затух, объём падает, ATR высок.\n",
    "        \"\"\"\n",
    "        name = \"Peak_Exhaustion_Score\"\n",
    "        c = self.df[\"close\"]\n",
    "    \n",
    "        # 1) расстояние до локального max\n",
    "        roll_max = c.rolling(price_win).max()\n",
    "        dist_max = (roll_max - c) / roll_max          # 0 — на max, >0 — ниже\n",
    "    \n",
    "        # 2) ослабевающий импульс\n",
    "        roc_now  = c.pct_change(mom_win)\n",
    "        roc_hist = roc_now.rolling(price_win).max()   # max импульса в окне\n",
    "        momentum_div = 1 - (roc_now / (roc_hist + 1e-12))   # 0 → свежий high\n",
    "    \n",
    "        # 3) сушащийся объём\n",
    "        vol_ratio = self.df[\"volume\"] / \\\n",
    "            self.df[\"volume\"].rolling(vol_win).mean()\n",
    "    \n",
    "        # 4) расширенный спред (ATR/price)\n",
    "        tr  = pd.concat([\n",
    "                self.df[\"high\"]  - self.df[\"low\"],\n",
    "                (self.df[\"high\"] - c.shift()).abs(),\n",
    "                (self.df[\"low\"]  - c.shift()).abs()\n",
    "            ], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(atr_win).mean()\n",
    "        atr_norm = atr / c\n",
    "    \n",
    "        # 5) агрегируем, переводим в z-score, squash σ → 0…1\n",
    "        raw = (dist_max + momentum_div + (1/vol_ratio) + atr_norm) / 4\n",
    "        z   = (raw - raw.rolling(z_win).mean()) / (raw.rolling(z_win).std() + 1e-9)\n",
    "        self.df[name] = 1 / (1 + np.exp(-z))   # σ(z)\n",
    "        \n",
    "    def _feat_fractal_dim(self, short_win=20, long_win=40):\n",
    "        \"\"\"Вычисляет фрактальную размерность на основе отношения ATR разных периодов\"\"\"\n",
    "        name = \"Fractal_Dim\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        \n",
    "        # Вычисляем ATR для короткого периода\n",
    "        h, l, c = self.df['high'], self.df['low'], self.df['close']\n",
    "        tr1 = h - l\n",
    "        tr2 = (h - c.shift()).abs()\n",
    "        tr3 = (l - c.shift()).abs()\n",
    "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "        atr_short = tr.rolling(short_win).mean()\n",
    "        \n",
    "        # Вычисляем ATR для длинного периода\n",
    "        atr_long = tr.rolling(long_win).mean()\n",
    "        \n",
    "        # Вычисляем фрактальную размерность\n",
    "        ratio = atr_long / (atr_short + 1e-10)  # Добавляем небольшое значение для избежания деления на 0\n",
    "        self.df[name] = np.log(ratio) / np.log(2)\n",
    "        \n",
    "    def _feat_bb_percent(self, window=20, std_mult=2):\n",
    "        name = \"%B_BB\"\n",
    "        if name in self.df: return\n",
    "        ma = self.df[\"close\"].rolling(window).mean()\n",
    "        std = self.df[\"close\"].rolling(window).std()\n",
    "        self.df[name] = (self.df[\"close\"] - (ma - std_mult * std)) / (4 * std)\n",
    "        \n",
    "    def _feat_kurtosis_roll(self, window=50):\n",
    "        name = \"Kurtosis_roll\"\n",
    "        if name in self.df: return\n",
    "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
    "        self.df[name] = ret.rolling(window).kurt()\n",
    "        \n",
    "    def _feat_obv_div(self, window=10):\n",
    "        name = \"OBV_div\"\n",
    "        if name in self.df: return\n",
    "        sign = np.sign(self.df[\"close\"].diff())\n",
    "        obv = (sign * self.df[\"volume\"]).cumsum()\n",
    "        price_chg = self.df[\"close\"].pct_change(window)\n",
    "        obv_chg = obv.pct_change(window)\n",
    "        self.df[name] = price_chg - obv_chg\n",
    "        \n",
    "    def _feat_rsi_slope(self, rsi_p=14, diff_win=5):\n",
    "        name = \"RSI_slope\"\n",
    "        if name in self.df: return\n",
    "        delta = self.df[\"close\"].diff()\n",
    "        gain = delta.clip(lower=0).rolling(rsi_p).mean()\n",
    "        loss = -delta.clip(lower=0).rolling(rsi_p).mean()\n",
    "        rsi = 100 - 100 / (1 + gain / (loss + 1e-10))\n",
    "        self.df[name] = rsi.diff(diff_win)\n",
    "        \n",
    "    def _feat_vol_decay(self, window=20):\n",
    "        name = \"Vol_Decay\"\n",
    "        if name in self.df: return\n",
    "        vol_ema = self.df[\"volume\"].ewm(span=window).mean()\n",
    "        self.df[name] = self.df[\"volume\"] / vol_ema - 1\n",
    "        \n",
    "    def _feat_accel_decay(self, window=10):\n",
    "        name = \"Accel_Decay\"\n",
    "        if name in self.df: return\n",
    "        vel = self.df[\"close\"].diff(window)\n",
    "        accel = vel.diff(window)\n",
    "        self.df[name] = accel / (vel.abs() + 1e-10)\n",
    "        \n",
    "    '''def _feat_entropy_roll(self, window=50):\n",
    "        name = \"Entropy_roll\"\n",
    "        if name in self.df: \n",
    "            return\n",
    "            \n",
    "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
    "        \n",
    "        def _entropy(x):\n",
    "            if len(x) < 2:\n",
    "                return 0\n",
    "            hist = np.histogram(x, bins='auto')[0]\n",
    "            hist = hist / hist.sum()  # Нормализуем\n",
    "            return -np.sum(hist * np.log(hist + 1e-10))\n",
    "    \n",
    "        self.df[name] = ret.rolling(window).apply(_entropy, raw=True)'''\n",
    "\n",
    "    def _feat_entropy_roll(self, window: int = 50):\n",
    "        name = \"Entropy_roll\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        ret = self.df[\"close\"].pct_change().fillna(0.0).to_numpy(dtype=np.float32)\n",
    "        ent = _rolling_entropy_exact_numba(ret, window)\n",
    "        self.df[name] = ent\n",
    "    \n",
    "    def _feat_wavelet_var(self, short_win=10, long_win=50):\n",
    "        name = \"Wavelet_Var_Ratio\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
    "        var_short = ret.rolling(short_win).var()\n",
    "        var_long = ret.rolling(long_win).var()\n",
    "        self.df[name] = var_short / (var_long + 1e-10)\n",
    "        \n",
    "    '''def _feat_autocorr(self, window=50):\n",
    "        name = \"Autocorr_Lag1\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
    "        self.df[name] = ret.rolling(window).apply(lambda x: x.autocorr(lag=1) if len(x) > 1 else 0, raw=False)'''\n",
    "\n",
    "    def _feat_autocorr(self, window=50):\n",
    "        name = \"Autocorr_Lag1\"\n",
    "        if name in self.df:\n",
    "            return\n",
    "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
    "        arr = ret.to_numpy()  # Use to_numpy() for compatibility\n",
    "        autocorrs = rolling_autocorr(arr, window)\n",
    "        self.df[name] = autocorrs\n",
    "        \n",
    "    def _feat_beta(self, window=50):\n",
    "        name = \"Beta_Market\"\n",
    "        if name in self.df or \"market_close\" not in self.df:\n",
    "            return\n",
    "        ret_stock = self.df[\"close\"].pct_change().fillna(0)\n",
    "        ret_market = self.df[\"market_close\"].pct_change().fillna(0)\n",
    "        cov = ret_stock.rolling(window).cov(ret_market)\n",
    "        var_market = ret_market.rolling(window).var()\n",
    "        self.df[name] = cov / (var_market + 1e-10)\n",
    "        \n",
    "    def _feat_peak_squeeze_curvature(self,\n",
    "                                vel_win: int = 5,\n",
    "                                acc_win: int = 5,\n",
    "                                vol_win: int = 20,\n",
    "                                atr_win: int = 14,\n",
    "                                z_win : int = 60):\n",
    "        \"\"\"\n",
    "        Возвращает 3 колонки:\n",
    "        PSC_raw, PSC_z, PSC_sigmoid ∈ [0,1]\n",
    "        \"\"\"\n",
    "        name = \"PSC\"\n",
    "        cols_need = {\"PSC_raw\",\"PSC_z\",\"PSC_sigmoid\"}\n",
    "        if cols_need.issubset(self.df.columns): return\n",
    "        c = self.df['close']\n",
    "    \n",
    "        # 1) speed & accel\n",
    "        speed  = c.pct_change(vel_win).fillna(0)\n",
    "        accel  = speed.diff(acc_win).fillna(0)\n",
    "        curvature = accel / (speed.abs() + 1e-10)\n",
    "    \n",
    "        # 2) squeeze = ATR_norm ↘ & HV_norm ↘\n",
    "        h,l = self.df['high'], self.df['low']\n",
    "        tr = pd.concat([h-l, (h-c.shift()).abs(), (l-c.shift()).abs()], axis=1).max(axis=1)\n",
    "        atr = tr.rolling(atr_win).mean()\n",
    "        hv  = c.pct_change().rolling(vol_win).std()\n",
    "        squeeze = - (atr / (c+1e-10)).diff().clip(upper=0)   # падение ATR\n",
    "        squeeze += - hv.diff().clip(upper=0)                 # падение HV\n",
    "        squeeze /= 2\n",
    "    \n",
    "        # 3) агрегируем\n",
    "        raw = 0.6*curvature + 0.4*squeeze\n",
    "    \n",
    "        # 4) z-score + σ(z)\n",
    "        mu  = raw.rolling(z_win).mean()\n",
    "        std = raw.rolling(z_win).std()\n",
    "        z = (raw - mu)/(std + 1e-9)\n",
    "        sigm = 1/(1+np.exp(-z))\n",
    "    \n",
    "        self.df[f'{name}_raw']     = raw\n",
    "        self.df[f'{name}_z']       = z.clip(-5, 5)\n",
    "        self.df[f'{name}_sigmoid'] = sigm\n",
    "\n",
    "\n",
    "def calculate_metrics(test_data, y_test, y_pred, target_column='normalized_target'):\n",
    "    \"\"\"\n",
    "    Функция для расчета метрик по данным теста и предсказаниям модели.\n",
    "\n",
    "    Параметры:\n",
    "    - test_data: pd.DataFrame — тестовые данные с колонками batch, high, close и другими.\n",
    "    - y_test: pd.Series или np.array — фактические значения целевой переменной.\n",
    "    - y_pred: pd.Series или np.array — предсказанные моделью значения.\n",
    "    - target_column: str — название колонки целевой переменной в test_data.\n",
    "\n",
    "    Возвращает:\n",
    "    - avg_mse: float — среднеквадратическая ошибка.\n",
    "    - avg_r2: float — средняя R².\n",
    "    - std_r2: float — стандартное отклонение R².\n",
    "    - corr_mean: float — средняя корреляция.\n",
    "    - corr_std: float — стандартное отклонение корреляции.\n",
    "    - avg_missed: float — средний процент упущенной прибыли.\n",
    "    \"\"\"\n",
    "    # Инициализация метрик\n",
    "    mse_scores, r2_scores, corr_scores, missed_pnl = [], [], [], []\n",
    "\n",
    "    # Расчет корреляции целевой переменной и предсказаний\n",
    "    y_pred_series = pd.Series(y_pred, index=y_test.index)\n",
    "    corr_score = test_data[target_column].corr(y_pred_series)\n",
    "    corr_scores.append(corr_score)\n",
    "\n",
    "    # MSE и R2\n",
    "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "    r2_scores.append(r2_score(y_test, y_pred))\n",
    "\n",
    "    # Расчет missed_pnl для каждого batch\n",
    "    \"\"\"for batch in test_data['batch'].unique():\n",
    "        mask = test_data['batch'] == batch\n",
    "        max_high = test_data.loc[mask, 'high'].max()\n",
    "        pred = y_pred[mask]  # предполагается, что y_pred соответствует normalized_target\n",
    "        sell_idx = np.argmin(pred)  # продажа на минимальном предсказанном значении\n",
    "        sell_price = test_data.loc[mask].iloc[sell_idx]['close']\n",
    "        missed = (max_high - sell_price) / (max_high - test_data.loc[mask].iloc[0]['close'])  # % упущенной прибыли\n",
    "        missed_pnl.append(missed)\"\"\"\n",
    "\n",
    "    # Усреднение метрик\n",
    "    avg_mse = float(np.mean(mse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))\n",
    "    std_r2 = float(np.std(r2_scores))\n",
    "    corr_mean = float(np.mean(corr_scores))\n",
    "    corr_std = float(np.std(corr_scores))\n",
    "    #avg_missed = float(np.mean(missed_pnl))\n",
    "\n",
    "    # Проверка на корректность результатов\n",
    "    if np.isfinite([avg_mse, avg_r2, std_r2, corr_mean, corr_std]).all(): #avg_missed\n",
    "        return avg_mse, avg_r2, std_r2, corr_mean, corr_std, #avg_missed\n",
    "    else:\n",
    "        return float('inf'), float('inf'), float('inf'), float('inf'), float('inf'), float('inf')\n",
    "\n",
    "def to_dense(X):\n",
    "    \"\"\"Преобразует разреженную матрицу в плотную.\"\"\"\n",
    "    if issparse(X):\n",
    "        return X.toarray()\n",
    "    return X\n",
    "    \n",
    "def prepare_data(df, target_col):\n",
    "    \"\"\"\n",
    "    Подготавливает данные: разделяет на числовые и категориальные признаки, создает конвейер преобразования.\n",
    "    \"\"\"\n",
    "    if type(target_col) == str:\n",
    "        df.dropna(inplace=True)\n",
    "        X = df.drop([target_col, 'batch'], axis=1)\n",
    "        y = df[target_col]\n",
    "    elif type(target_col) == list:\n",
    "        df.dropna(inplace=True)\n",
    "        X = df.drop(target_col+['batch'], axis=1)\n",
    "        y = df[target_col]\n",
    "\n",
    "    # Разделение на числовые и категориальные признаки\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Создание конвейера преобразования\n",
    "    preprocessing = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('normalize', PowerTransformer(method='yeo-johnson')),\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ]), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return X, y, preprocessing\n",
    "\n",
    "\n",
    "def calculate_indicators(df, features, params=None, mode=None, multy=False):\n",
    "    \n",
    "    fc = FeatureCalculatorForRegression(df)\n",
    "    if mode==None:\n",
    "        df_features = fc.calculate_features(params=params, required_features=features)\n",
    "    else:\n",
    "        df_features = fc.calculate_all_possible_features()\n",
    "    if multy == False:\n",
    "        df1 = df_features[df_features['normalized_target'].notna()]\n",
    "    else:\n",
    "        df1 = df_features[df_features['multi_target_5'].notna()]\n",
    "    features = ['open', 'close', 'high', 'low', 'volume', 'buy_signal', 'sell_signal','event_sell_time','event_sell_price',\n",
    "                'event_time','event_price','event_sell_time','event_sell_price','target', 'pnl', 'ma', 'pmax'] #time\n",
    "    df1['regime'] = df1['regime'].astype('object')\n",
    "    df1[['batch','trade_bars_counter']] = df1[['batch', 'trade_bars_counter']].astype('int')\n",
    "    df1 = df1.drop(features, axis=1)\n",
    "    #df1 = df1.dropna()\n",
    "    return df1, fc._timings\n",
    "\n",
    "def sample_feature_params(params) -> dict:\n",
    "        \"\"\"\n",
    "        Draws *one* sample of the whole feature-engineering hyper-parameter set.\n",
    "        Rule of thumb for ranges:\n",
    "          • lower bound = ‘sane minimum‘ from domain knowledge\n",
    "          • upper bound = ‘sane maximum’\n",
    "        Adjust them if you feel the search space is too wide or too narrow.\n",
    "        \"\"\"\n",
    "        # ---- helpers for monotone constraints ----------------------------------\n",
    "        fast  = params['macd_fast']\n",
    "        slow  = params['macd_slow']\n",
    "    \n",
    "        slope_lag_min = params['slope_lag_min']\n",
    "        slope_lag     = params['slope_lag']\n",
    "    \n",
    "        # ---- finally compose the nested dict -----------------------------------\n",
    "        return {\n",
    "            'base': {\n",
    "                'medprice': params['medprice']\n",
    "            },\n",
    "            'macd': {\n",
    "                'fast'      : fast,\n",
    "                'slow'      : slow,\n",
    "                'signal'    : params['macd_signal'],\n",
    "                'macd_roll' : params['macd_roll']\n",
    "            },\n",
    "            'overbought': {\n",
    "                'rsi_p'         : params['rsi_p'],\n",
    "                'stoch_p'       : params['stoch_p'],\n",
    "                'oversold_roll' : params['oversold_roll']\n",
    "            },\n",
    "            'madist': {\n",
    "                'span_lenght'   : params['madist_span'],\n",
    "                'madist_lenght' : params['madist_len']\n",
    "            },\n",
    "            'mean_reversion': {\n",
    "                'window' : params['mr_window']\n",
    "            },\n",
    "            'fear_greed': {\n",
    "                'greed_pct'    : params['fg_greed_pct'],\n",
    "                'volume_ratio_scr' : params['fg_vol_ratio'],\n",
    "                'window'       : params['fg_window'],\n",
    "                'greed_roll'   : params['fg_roll']\n",
    "            },\n",
    "            'price_variation': {\n",
    "                'variation_lenght': params['pv_len']\n",
    "            },\n",
    "            'pmax_ma': {\n",
    "                'pmax_ma_lenght'      : params['pmax_len'],\n",
    "                'pmax_ma_lenght_roll' : params['pmax_roll']\n",
    "            },\n",
    "            'slope': {\n",
    "                'slope_lag'     : slope_lag,\n",
    "                'slope_lag_min' : slope_lag_min,\n",
    "                'sloap_pct'     : params['slope_pct'],\n",
    "                'sloap_roll'    : params['slope_roll']\n",
    "            },\n",
    "            # _trade_duration_features – no params\n",
    "        }\n",
    "\n",
    "def build_feature_params(\n",
    "    flat_params: Dict[str, Any],\n",
    "    extra_alias: Optional[Dict[str, Tuple[str, str | None]]] = None\n",
    ") -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Преобразует «плоский» словарь от Optuna в структуру,\n",
    "    которую понимает FeatureCalculatorForRegression.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Базовая явная таблица соответствий\n",
    "    alias: Dict[str, Tuple[str, str | None]] = {\n",
    "        'hp_lamb'          : ('hp_trend'           , 'lamb'),\n",
    "        'ea_pct'           : ('ema_acceleration'   , 'pct_window'),\n",
    "        'ea_ema'           : ('ema_acceleration'   , 'ema_window'),\n",
    "        'mr_window'        : ('Mean_Reversion'     , 'window'),\n",
    "        'ag_window'        : ('Asset_Growth'       , 'window'),\n",
    "        'medprice'         : ('MEDPRICE'           , 'medprice'),\n",
    "        'bb_window'        : ('BB_Width'           , 'bb_window'),\n",
    "        'macd_fast'        : ('MACD'               , 'fast'),\n",
    "        'macd_slow'        : ('MACD'               , 'slow'),\n",
    "        'macd_signal'      : ('MACD'               , 'signal'),\n",
    "        'fg_window'        : ('Fear_Greed'         , 'window'),\n",
    "        'atr_window'       : ('ATR_norm'           , 'atr_window'),\n",
    "        'vr_window'        : ('volume_ratio'       , 'window'),\n",
    "        'madist_span'      : ('Price_MADist%'      , 'span_lenght'),\n",
    "        'slope_lag'        : ('slope_trend'        , 'slope_lag'),\n",
    "        'slope_lag_min'    : ('slope_trend'        , 'slope_lag'),\n",
    "        'rsi_p'            : ('Overbought_Oversold', 'rsi_p'),\n",
    "        'stoch_p'          : ('Overbought_Oversold', 'stoch_p'),\n",
    "        'pmax_len'         : ('pmax_norm'          , 'pmax_ma_length'),\n",
    "        'pmax_roll'        : ('pmax_norm'          , 'pmax_ma_length_roll'),\n",
    "        'pc_window'        : ('pmax_norm'          , 'pct_window'),\n",
    "        'ema_trend_span'   : ('ema_trend'          , 'span'),\n",
    "        'ema_trend_pct'    : ('ema_trend'          , 'pct_window'),\n",
    "        'stat_window'      : ('stat_window', None),\n",
    "        # --- новые алиасы ---\n",
    "        'roc_window'        : ('ROC'          , 'window'),\n",
    "        'willr_window'      : ('WILLR'          , 'window'),\n",
    "        'fractal_short_win' : ('Fractal_Dim', 'short_win'),\n",
    "        'fractal_long_win'  : ('Fractal_Dim', 'long_win'),\n",
    "        'peak_price_win'    : ('Peak_Exhaustion_Score', 'price_win'),\n",
    "        'peak_mom_win'      : ('Peak_Exhaustion_Score', 'mom_win'),\n",
    "        'peak_vol_win'      : ('Peak_Exhaustion_Score', 'vol_win'),\n",
    "        'peak_atr_win'      : ('Peak_Exhaustion_Score', 'atr_win'),\n",
    "        'peak_z_win'        : ('Peak_Exhaustion_Score', 'z_win'),\n",
    "        'bb_window'         : ('%B_BB', 'window'),\n",
    "        'bb_std_mult'       : ('%B_BB', 'std_mult'),\n",
    "        'kurt_window'       : ('Kurtosis_roll', 'window'),\n",
    "        'obv_window'        : ('OBV_div', 'window'),\n",
    "        'rsi_slope_rsi_p'   : ('RSI_slope', 'rsi_p'),\n",
    "        'rsi_diff_win'      : ('RSI_slope', 'diff_win'),\n",
    "        'voldec_window'     : ('Vol_Decay', 'window'),\n",
    "        'acceldec_window'   : ('Accel_Decay', 'window'),\n",
    "        'ent_window'        : ('Entropy_roll', 'window'),\n",
    "        'wlt_short_win'     : ('Wavelet_Var_Ratio', 'short_win'),\n",
    "        'wlt_long_win'      : ('Wavelet_Var_Ratio', 'long_win'),\n",
    "        'acorr_window'      : ('Autocorr_Lag1', 'window'),\n",
    "        'beta_window'       : ('Beta_Market', 'window'),\n",
    "        'psc_vel_win'       : ('PSC', 'vel_win'),\n",
    "        'psc_acc_win'       : ('PSC', 'acc_win'),\n",
    "        'psc_vol_win'       : ('PSC', 'vol_win'),\n",
    "        'psc_atr_win'       : ('PSC', 'atr_win'),\n",
    "        'psc_z_win'         : ('PSC', 'z_win'),\n",
    "    }\n",
    "\n",
    "    # 2. Пользовательские переопределения\n",
    "    if extra_alias:\n",
    "        alias.update(extra_alias)\n",
    "\n",
    "    # 3. Автоматический разбор префиксов (fallback)\n",
    "    prefix_map: Dict[str, str] = {\n",
    "        'macd'        : 'MACD',\n",
    "        'hp'          : 'hp_trend',\n",
    "        'ea'          : 'ema_acceleration',\n",
    "        'mr'          : 'Mean_Reversion',\n",
    "        'ag'          : 'Asset_Growth',\n",
    "        'bb'          : 'BB_Width',\n",
    "        'fg'          : 'Fear_Greed',\n",
    "        'atr'         : 'ATR_norm',\n",
    "        'vr'          : 'volume_ratio',\n",
    "        'madist'      : 'Price_MADist%',\n",
    "        'slope'       : 'slope_trend',\n",
    "        'pmax'        : 'pmax_norm',\n",
    "        'ema_trend'   : 'ema_trend',\n",
    "        'rsi'         : 'Overbought_Oversold',\n",
    "        'stoch'       : 'Overbought_Oversold',\n",
    "        'fractal'     : 'Fractal_Dim',\n",
    "        'peak'        : 'Peak_Exhaustion_Score',\n",
    "        'bb'          : '%B_BB',\n",
    "        'kurt'        : 'Kurtosis_roll',\n",
    "        'obv'         : 'OBV_div',\n",
    "        'rsi_slope'   : 'RSI_slope',\n",
    "        'voldec'      : 'Vol_Decay',\n",
    "        'acceldec'    : 'Accel_Decay',\n",
    "        'entropy'     : 'Entropy_roll',\n",
    "        'wavelet'     : 'Wavelet_Var_Ratio',\n",
    "        'acorr'       : 'Autocorr_Lag1',\n",
    "        'beta'        : 'Beta_Market',\n",
    "        'psc'         : 'PSC',\n",
    "    }\n",
    "\n",
    "    nested: Dict[str, Dict[str, Any]] = defaultdict(dict)\n",
    "\n",
    "    for key, val in flat_params.items():\n",
    "\n",
    "        # 3.1 Явное соответствие\n",
    "        if key in alias:\n",
    "            prim, arg = alias[key]\n",
    "            if prim == 'stat_window' or arg is None:\n",
    "                nested['stat_window'] = val\n",
    "            else:\n",
    "                nested[prim][arg] = val\n",
    "            continue\n",
    "\n",
    "        # 3.2 Игнорируем вспомогательные ключи вида *_min, *_max, если\n",
    "        #     они не нужны никакому примитиву.\n",
    "        if key.endswith('_min') or key.endswith('_max'):\n",
    "            continue\n",
    "\n",
    "        # 3.3 Fallback-разбор <prefix>_<arg>\n",
    "        if '_' in key:\n",
    "            prefix, arg = key.split('_', 1)\n",
    "            if prefix in prefix_map:\n",
    "                nested[prefix_map[prefix]][arg] = val\n",
    "                continue\n",
    "\n",
    "        # 3.4 Неизвестный ключ — игнорируем или логируем\n",
    "        # print(f'Warning: parameter \"{key}\" was not mapped')\n",
    "\n",
    "    return {p: d for p, d in nested.items()}\n",
    "\n",
    "_ORIG_INTERP = F.interpolate\n",
    "\n",
    "\n",
    "def _collapse_pred_to_bt(y_pred: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Приводим предсказание к виду (B, T), считая последнюю ось временем (горизонтом).\n",
    "    Все промежуточные оси (кроме batch=ось 0 и time=последняя ось) усредняем.\n",
    "    Пример: (B, 1, 4, 10) -> mean по осям (1,2) -> (B,10)\n",
    "    (B, 10) -> ок\n",
    "    (B, 1, 10) -> squeeze -> (B,10)\n",
    "    \"\"\"\n",
    "    if not isinstance(y_pred, torch.Tensor):\n",
    "        raise TypeError(f\"y_pred must be a tensor, got {type(y_pred)}\")\n",
    "\n",
    "    # Сначала уберём все единичные оси\n",
    "    if any(s == 1 for s in y_pred.shape[1:-1]):\n",
    "        # squeeze не трогает последнюю ось, если она не равна 1\n",
    "        y_pred = y_pred.squeeze()\n",
    "        # Если squeeze убрал не только единичные, но и привёл к (B, T) — хорошо.\n",
    "\n",
    "    if y_pred.dim() == 1:\n",
    "        # (B,) — интерпретируем как T=1, сделаем (B,1)\n",
    "        y_pred = y_pred.unsqueeze(-1)\n",
    "        return y_pred\n",
    "\n",
    "    if y_pred.dim() == 2:\n",
    "        # (B, T) — уже как надо\n",
    "        return y_pred\n",
    "\n",
    "    # Если размерностей больше 2: считаем last dim = time, batch = 0\n",
    "    # Все промежуточные оси схлопываем усреднением\n",
    "    reduce_dims = tuple(range(1, y_pred.dim() - 1))\n",
    "    if len(reduce_dims) > 0:\n",
    "        y_pred = y_pred.mean(dim=reduce_dims)\n",
    "    # На выходе (B, T)\n",
    "    if y_pred.dim() != 2:\n",
    "        # На всякий случай добьёмся (B, T)\n",
    "        y_pred = y_pred.view(y_pred.size(0), -1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def _install_safe_interpolate_patch():\n",
    "    \"\"\"\n",
    "    Патч делает F.interpolate детерминированным при включённом torch.use_deterministic_algorithms(True)\n",
    "    для CUDA и режимов linear/bilinear/bicubic, прогоняя вычисление на CPU.\n",
    "    Идемпотентен и не меняет сигнатуру.\n",
    "    \"\"\"\n",
    "    if getattr(F.interpolate, \"_is_deterministic_wrapper\", False):\n",
    "        return\n",
    "\n",
    "    _orig_interpolate = F.interpolate\n",
    "\n",
    "    # Какие режимы считаем потенциально недетерминируемыми на CUDA\n",
    "    _CUDA_UNSAFE_MODES = {\"linear\", \"bilinear\", \"bicubic\"}  # 1d/2d/2d\n",
    "\n",
    "    def _needs_cpu_fallback(input, mode):\n",
    "        if not torch.is_tensor(input):\n",
    "            return False\n",
    "        if input.is_cuda and mode in _CUDA_UNSAFE_MODES:\n",
    "            # upsample_linear1d_backward_out_cuda и др. — недетерминируемы\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    @functools.wraps(_orig_interpolate)\n",
    "    def _deterministic_interpolate(\n",
    "        input: torch.Tensor,\n",
    "        size=None,\n",
    "        scale_factor=None,\n",
    "        mode=\"nearest\",\n",
    "        align_corners=None,\n",
    "        recompute_scale_factor=None,\n",
    "        antialias=False,\n",
    "    ):\n",
    "        # Если не нужно, просто вызовем оригинал\n",
    "        if not _needs_cpu_fallback(input, mode):\n",
    "            return _orig_interpolate(\n",
    "                input,\n",
    "                size=size,\n",
    "                scale_factor=scale_factor,\n",
    "                mode=mode,\n",
    "                align_corners=align_corners,\n",
    "                recompute_scale_factor=recompute_scale_factor,\n",
    "                antialias=antialias,\n",
    "            )\n",
    "\n",
    "        # CUDA + linear/bilinear/bicubic → CPU fallback\n",
    "        x = input\n",
    "        dev = x.device\n",
    "        orig_dtype = x.dtype\n",
    "\n",
    "        # Для стабильности переводим в float32 на CPU\n",
    "        x_cpu = x.detach().to(\"cpu\", dtype=torch.float32).requires_grad_(x.requires_grad)\n",
    "\n",
    "        y_cpu = _orig_interpolate(\n",
    "            x_cpu,\n",
    "            size=size,\n",
    "            scale_factor=scale_factor,\n",
    "            mode=mode,\n",
    "            align_corners=align_corners,\n",
    "            recompute_scale_factor=recompute_scale_factor,\n",
    "            antialias=antialias,\n",
    "        )\n",
    "\n",
    "        # Возвращаем на исходное устройство и тип\n",
    "        y = y_cpu.to(dev, dtype=orig_dtype)\n",
    "\n",
    "        return y\n",
    "\n",
    "    _deterministic_interpolate._is_deterministic_wrapper = True  # type: ignore[attr-defined]\n",
    "    F.interpolate = _deterministic_interpolate\n",
    "\n",
    "\n",
    "_install_safe_interpolate_patch()\n",
    "\n",
    "\n",
    "def _unpack_pf_batch(batch):\n",
    "    \"\"\"\n",
    "    Унифицированная распаковка батча из TimeSeriesDataSet.to_dataloader(...)\n",
    "    Возвращает: x (dict), y (Tensor|None), weight (Tensor|None)\n",
    "    \"\"\"\n",
    "    if isinstance(batch, (list, tuple)):\n",
    "        if len(batch) == 3:\n",
    "            x, y, weight = batch\n",
    "        elif len(batch) == 2:\n",
    "            x, y = batch\n",
    "            weight = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected batch tuple length: {len(batch)}\")\n",
    "    elif isinstance(batch, dict):\n",
    "        # На всякий случай поддержим dict → возьмём таргет из decoder_target, если есть\n",
    "        x = batch\n",
    "        y = batch.get(\"decoder_target\", None)\n",
    "        weight = None\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
    "    return x, y, weight\n",
    "\n",
    "\n",
    "# F.interpolate = _deterministic_interpolate\n",
    "\n",
    "\n",
    "def _extract_pred_tensor(y_pred):\n",
    "    # извлекаем тензор предикта из любых обёрток\n",
    "    if isinstance(y_pred, dict):\n",
    "        for key in (\"prediction\", \"output\", \"decoder_output\"):\n",
    "            if key in y_pred and torch.is_tensor(y_pred[key]):\n",
    "                return y_pred[key]\n",
    "        # если не нашли — попробуем fallback: первый тензор в dict\n",
    "        for v in y_pred.values():\n",
    "            if torch.is_tensor(v):\n",
    "                return v\n",
    "        raise ValueError(\"Could not extract prediction tensor from dict y_pred.\")\n",
    "\n",
    "    if isinstance(y_pred, (list, tuple)):\n",
    "        # обычно y_pred[0] — предсказание\n",
    "        return y_pred[0]\n",
    "\n",
    "    if torch.is_tensor(y_pred):\n",
    "        return y_pred\n",
    "\n",
    "    raise TypeError(f\"Unsupported y_pred type: {type(y_pred)}\")\n",
    "\n",
    "\n",
    "class EventTimeSeriesSplit(BaseCrossValidator):\n",
    "    \"\"\"\n",
    "    Кросс-валидация по событиям (batch), хронологическая, с эмбарго.\n",
    "    groups: массив той же длины, что и df, со значениями batch\n",
    "    times:  массив pd.Timestamp (или sortable), та же длина, что и df\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits: int = 5, embargo_events: int = 1, min_train_events: int = 5):\n",
    "        self.n_splits = n_splits\n",
    "        self.embargo_events = embargo_events\n",
    "        self.min_train_events = min_train_events\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X, y=None, groups=None, times: Optional[pd.Series] = None) -> Iterator[\n",
    "        Tuple[np.ndarray, np.ndarray]]:\n",
    "        if groups is None or times is None:\n",
    "            raise ValueError(\"Pass groups=batch and times=time columns\")\n",
    "\n",
    "        groups = np.asarray(groups)\n",
    "        times = pd.to_datetime(times)\n",
    "\n",
    "        # порядок событий по старт-времени\n",
    "        df_tmp = pd.DataFrame({\"group\": groups, \"time\": times}).reset_index(names=\"row_idx\")\n",
    "        first_time = df_tmp.groupby(\"group\")[\"time\"].min().sort_values()\n",
    "        uniq_groups = first_time.index.to_numpy()\n",
    "\n",
    "        n_events = len(uniq_groups)\n",
    "        if n_events < (self.n_splits + self.min_train_events):\n",
    "            # уменьшаем число сплитов, если событий мало\n",
    "            eff_splits = max(1, n_events - self.min_train_events)\n",
    "        else:\n",
    "            eff_splits = self.n_splits\n",
    "\n",
    "        # на каждой итерации расширяем train вправо\n",
    "        for split_idx in range(1, eff_splits + 1):\n",
    "            # доля событий для валидации\n",
    "            val_events = max(1, n_events // (eff_splits + 1))\n",
    "            train_end = n_events - (eff_splits - split_idx + 1) * val_events\n",
    "\n",
    "            if train_end < self.min_train_events:\n",
    "                continue\n",
    "\n",
    "            # эмбарго\n",
    "            embargoed_end = max(0, train_end - self.embargo_events)\n",
    "\n",
    "            train_groups = uniq_groups[:embargoed_end]\n",
    "            val_groups = uniq_groups[train_end: train_end + val_events]\n",
    "\n",
    "            train_idx = df_tmp.index[df_tmp[\"group\"].isin(train_groups)].to_numpy()\n",
    "            val_idx = df_tmp.index[df_tmp[\"group\"].isin(val_groups)].to_numpy()\n",
    "\n",
    "            # индексы исходной X (если это DataFrame — у вас совпадают позиции с row_idx)\n",
    "            yield (train_idx, val_idx)\n",
    "\n",
    "\n",
    "class MinimalRichProgressBar(RichProgressBar):\n",
    "    def on_validation_start(self, trainer, pl_module):\n",
    "        pass\n",
    "\n",
    "    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        pass\n",
    "\n",
    "\n",
    "class NoValidationBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        # возвращаем полностью отключённый tqdm для валидации\n",
    "        return tqdm_class(disable=True)\n",
    "\n",
    "class CustomTFT(TemporalFusionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.mask_prob = kwargs.pop(\"mask_prob\", 0.05)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._val_preds = []\n",
    "        self._val_trues = []\n",
    "        self._val_gids = []\n",
    "        self.scheduled_prob = 0.0\n",
    "        \n",
    "        safe_val = torch.tensor(\n",
    "            torch.finfo(torch.float16).min,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "    \n",
    "        m = self.multihead_attn.attention\n",
    "        if hasattr(m, \"mask_bias\") and not isinstance(m.mask_bias, torch.Tensor):\n",
    "            delattr(m, \"mask_bias\")\n",
    "        m.register_buffer(\"mask_bias\", safe_val)\n",
    "\n",
    "    def on_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.max_epochs > 0:\n",
    "            self.scheduled_prob = min(1.0, trainer.current_epoch / (trainer.max_epochs * 0.8))\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, weight = _unpack_pf_batch(batch)\n",
    "    \n",
    "        if torch.rand(1).item() < self.mask_prob and \"encoder_target\" in x:\n",
    "            enc = x[\"encoder_target\"]\n",
    "            noise = torch.normal(0, 0.15, size=enc.shape, device=enc.device)\n",
    "            x = {**x, \"encoder_target\": noise}\n",
    "            del enc, noise\n",
    "    \n",
    "        if torch.rand(1).item() < self.scheduled_prob and y is not None:\n",
    "            with torch.no_grad():\n",
    "                out = self(x)\n",
    "                y_pred = self.loss.to_prediction(out)\n",
    "                y_bt = _collapse_pred_to_bt(y_pred)\n",
    "                dec_tgt = y_bt if y_bt.dim() == 2 else y_bt.unsqueeze(-1)\n",
    "                x['decoder_target'] = dec_tgt.detach()\n",
    "                del out, y_pred, y_bt, dec_tgt\n",
    "    \n",
    "        batch = (x, y, weight) if weight is not None else (x, y)\n",
    "        result = super().training_step(batch, batch_idx)\n",
    "        \n",
    "        if batch_idx % 50 == 0:  # Rare for speed\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, weight = _unpack_pf_batch(batch)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            out_temp = self(x)\n",
    "            y_pred_temp = self.loss.to_prediction(out_temp)\n",
    "            y_bt_temp = _collapse_pred_to_bt(y_pred_temp)\n",
    "            del out_temp, y_pred_temp\n",
    "\n",
    "        if \"decoder_target\" in x:\n",
    "            enc_tgt = x.get(\"encoder_target\")\n",
    "            if enc_tgt is not None:\n",
    "                batch_mean = enc_tgt.mean(dim=-1, keepdim=True)\n",
    "                batch_std = enc_tgt.std(dim=-1, keepdim=True) + 1e-8\n",
    "                mean_tensor = batch_mean.expand_as(x[\"decoder_target\"])\n",
    "                std_tensor = (0.1 * batch_std).expand_as(x[\"decoder_target\"])\n",
    "                noise_fill = torch.normal(mean_tensor, std_tensor)\n",
    "                del batch_mean, std_tensor, mean_tensor\n",
    "            else:\n",
    "                dec_tgt = x[\"decoder_target\"]\n",
    "                device = dec_tgt.device\n",
    "                noise_fill = torch.full_like(dec_tgt, self.global_target_mean)\n",
    "                batch_size = dec_tgt.size(0)\n",
    "                batch_std = torch.full((batch_size,), self.global_target_std, device=device).unsqueeze(-1)\n",
    "            \n",
    "            if torch.rand(1).item() < self.scheduled_prob:\n",
    "                decoder_fill = y_bt_temp\n",
    "            else:\n",
    "                jitter_size = noise_fill.shape\n",
    "                additional_jitter = torch.randn(jitter_size, device=noise_fill.device) * (0.05 * batch_std.expand_as(noise_fill))\n",
    "                decoder_fill = noise_fill + additional_jitter\n",
    "                del additional_jitter, noise_fill\n",
    "            \n",
    "            decoder_fill = torch.clamp(decoder_fill, -1.0, 1.0)\n",
    "            x[\"decoder_target\"] = decoder_fill\n",
    "            del y_bt_temp, batch_std\n",
    "    \n",
    "        out = self(x)\n",
    "        y_pred_raw = self.loss.to_prediction(out)\n",
    "        del out\n",
    "    \n",
    "        y_pred_metrics = torch.clamp(y_pred_raw, -1.0, 1.0)\n",
    "        del y_pred_raw\n",
    "    \n",
    "        y_actual = y if y is not None else x.get(\"decoder_target\")\n",
    "        if y_actual is None:\n",
    "            return\n",
    "    \n",
    "        try:\n",
    "            y_pred_aligned, y_actual_aligned = _align_pred_target(y_pred_metrics, y_actual)\n",
    "            del y_pred_metrics, y_actual\n",
    "        except Exception:\n",
    "            return\n",
    "    \n",
    "        self._val_preds.append(y_pred_aligned.detach())\n",
    "        self._val_trues.append(y_actual_aligned.detach())\n",
    "        del y_pred_aligned, y_actual_aligned\n",
    "    \n",
    "        gid = x.get(\"group_ids\")\n",
    "        if gid is not None and isinstance(gid, torch.Tensor):\n",
    "            self._val_gids.append(gid.detach())\n",
    "        else:\n",
    "            self._val_gids.append(None)\n",
    "        \n",
    "        if batch_idx % 50 == 0:  # Rare for speed\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if len(self._val_preds) == 0:\n",
    "            return\n",
    "    \n",
    "        yp = torch.cat(self._val_preds, dim=0)\n",
    "        yt = torch.cat(self._val_trues, dim=0)\n",
    "    \n",
    "        self._val_preds.clear()\n",
    "        self._val_trues.clear()\n",
    "    \n",
    "        se = (yp - yt) ** 2\n",
    "        val_mse = float(se.mean().item())\n",
    "        val_mse_std = float(se.std(unbiased=False).item())\n",
    "        del yp, yt\n",
    "    \n",
    "        self.log(\"val_mse\", val_mse, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_mse_std\", val_mse_std, prog_bar=False, on_step=False, on_epoch=True)\n",
    "    \n",
    "        has_any_gid = any(g is not None for g in self._val_gids)\n",
    "        if has_any_gid:\n",
    "            gid_list = []\n",
    "            valid = True\n",
    "            for g in self._val_gids:\n",
    "                if g is None:\n",
    "                    valid = False\n",
    "                    break\n",
    "                gid_list.append(g)\n",
    "    \n",
    "            if valid and len(gid_list) > 0:\n",
    "                gid_all = torch.cat(gid_list, dim=0).numpy().ravel()\n",
    "                se_np = se.numpy().ravel()\n",
    "                del se\n",
    "    \n",
    "                if gid_all.shape[0] == se_np.shape[0]:\n",
    "                    uniq = np.unique(gid_all)\n",
    "                    g_mse = [se_np[gid_all == u].mean() for u in uniq if (gid_all == u).any()]\n",
    "                    if len(g_mse) > 0:\n",
    "                        g_mse = np.asarray(g_mse, dtype=float)\n",
    "                        val_mse_group_mean = float(g_mse.mean())\n",
    "                        val_mse_group_std = float(g_mse.std(ddof=0))\n",
    "                        self.log(\"val_mse_group_mean\", val_mse_group_mean, prog_bar=False, on_step=False, on_epoch=True)\n",
    "                        self.log(\"val_mse_group_std\", val_mse_group_std, prog_bar=True, on_step=False, on_epoch=True)\n",
    "                    del g_mse, uniq\n",
    "    \n",
    "                del gid_all, se_np\n",
    "    \n",
    "        self._val_gids.clear()\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    def log(self, name, value, *args, **kwargs):\n",
    "        if value is None:\n",
    "            return\n",
    "        super().log(name, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "def _worker_init_fn(worker_id: int, seed: int):\n",
    "    \"\"\"\n",
    "    Глобальная функция для инициализации worker-а DataLoader-а.\n",
    "    pickle её «видит» и может передать в подпроцессы.\n",
    "    \"\"\"\n",
    "    set_seeds(seed + worker_id)\n",
    "\n",
    "\n",
    "def _extract_tensor(x, role=\"pred\"):\n",
    "    \"\"\"\n",
    "    Извлекает torch.Tensor из различных контейнеров/структур.\n",
    "    - dict: сперва пробуем ключи, характерные для предсказаний/таргета\n",
    "    - tuple/list: берём первый тензор или первый элемент, приводимый к тензору\n",
    "    - tensor: возвращаем как есть\n",
    "    \"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "\n",
    "    if isinstance(x, dict):\n",
    "        # Наиболее типичные ключи в pytorch-forecasting / lightning шагах\n",
    "        preferred_keys = [\n",
    "            \"prediction\", \"pred\", \"output\", \"y_pred\", \"yhat\", \"y\", \"target\"\n",
    "        ]\n",
    "        for k in preferred_keys:\n",
    "            if k in x and isinstance(x[k], torch.Tensor):\n",
    "                return x[k]\n",
    "        # Если значения-словари/кортежи — попробуем рекурсивно\n",
    "        for v in x.values():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                return v\n",
    "            if isinstance(v, (list, tuple, dict)):\n",
    "                try:\n",
    "                    t = _extract_tensor(v, role=role)\n",
    "                    if isinstance(t, torch.Tensor):\n",
    "                        return t\n",
    "                except Exception:\n",
    "                    pass\n",
    "        raise TypeError(f\"Cannot extract tensor from dict for role={role}. Keys={list(x.keys())}\")\n",
    "\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        for item in x:\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                return item\n",
    "        # если нет прямого тензора — попробуем рекурсивно\n",
    "        for item in x:\n",
    "            if isinstance(item, (list, tuple, dict)):\n",
    "                try:\n",
    "                    t = _extract_tensor(item, role=role)\n",
    "                    if isinstance(t, torch.Tensor):\n",
    "                        return t\n",
    "                except Exception:\n",
    "                    pass\n",
    "        raise TypeError(f\"Cannot extract tensor from {type(x)} for role={role}\")\n",
    "\n",
    "    # Последняя попытка — у объектов некоторых библиотек есть .values или .tensor\n",
    "    for attr in (\"values\", \"tensor\", \"data\"):\n",
    "        if hasattr(x, attr):\n",
    "            v = getattr(x, attr)\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                return v\n",
    "\n",
    "    raise TypeError(f\"Unsupported type for tensor extraction (role={role}): {type(x)}\")\n",
    "\n",
    "\n",
    "def _maybe_squeeze_last(x):\n",
    "    \"\"\"\n",
    "    Безопасно убираем последнюю размерность, если она равна 1.\n",
    "    Если x не тензор — возвращаем как есть.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    if x.dim() > 0 and x.size(-1) == 1:\n",
    "        return x.squeeze(-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _align_pred_target(y_pred, y_actual):\n",
    "    \"\"\"\n",
    "    Приводим предсказания и таргет к совместимым формам для MSE:\n",
    "    - Извлекаем тензоры из возможных контейнеров.\n",
    "    - Сводим предсказание к (B, T_pred) с последней осью как временем.\n",
    "    - Таргет сводим к (B,) или (B, T_act).\n",
    "    - Если таргет (B,) — берём последний горизонт из предсказаний.\n",
    "    - Если таргет (B, T_act) — подгоняем по времени (обрезаем/проверяем равенство).\n",
    "    \"\"\"\n",
    "    # 1) Достаём тензоры\n",
    "    y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
    "    y_actual = _extract_tensor(y_actual, role=\"target\")\n",
    "\n",
    "    # 2) Сжимаем последнюю единичную ось\n",
    "    y_pred = _maybe_squeeze_last(y_pred)\n",
    "    y_actual = _maybe_squeeze_last(y_actual)\n",
    "\n",
    "    # Быстрый путь: формы совпали\n",
    "    if isinstance(y_pred, torch.Tensor) and isinstance(y_actual, torch.Tensor):\n",
    "        if y_pred.shape == y_actual.shape:\n",
    "            return y_pred, y_actual\n",
    "\n",
    "    # 3) Приводим предсказание к (B, T_pred)\n",
    "    y_pred_bt = _collapse_pred_to_bt(y_pred)  # (B, T_pred)\n",
    "\n",
    "    # 4) Приведём таргет к (B,) или (B, T_act)\n",
    "    if y_actual.dim() == 1:\n",
    "        # (B,) — ожидаем 1 шаг на таргет → берём последний горизонт из предсказаний\n",
    "        if y_pred_bt.dim() != 2 or y_pred_bt.size(0) != y_actual.size(0):\n",
    "            raise ValueError(f\"Batch mismatch: pred={tuple(y_pred_bt.shape)} vs target={tuple(y_actual.shape)}\")\n",
    "        y_pred_aligned = y_pred_bt[:, -1]  # последний шаг горизонта\n",
    "        return y_pred_aligned, y_actual\n",
    "\n",
    "    if y_actual.dim() == 2:\n",
    "        # (B, T_act)\n",
    "        if y_pred_bt.size(0) != y_actual.size(0):\n",
    "            raise ValueError(f\"Batch mismatch: pred={tuple(y_pred_bt.shape)} vs target={tuple(y_actual.shape)}\")\n",
    "        T_pred = y_pred_bt.size(1)\n",
    "        T_act = y_actual.size(1)\n",
    "        if T_pred == T_act:\n",
    "            return y_pred_bt, y_actual\n",
    "        if T_pred > T_act:\n",
    "            # Обрежем последние T_act шагов, чтобы соответствовать таргету\n",
    "            y_pred_bt = y_pred_bt[:, -T_act:]\n",
    "            return y_pred_bt, y_actual\n",
    "        # Если предсказаний по времени меньше, чем в таргете — это логическая ошибка настройки\n",
    "        raise ValueError(\n",
    "            f\"Prediction horizon shorter than target: pred T={T_pred}, target T={T_act} \"\n",
    "            f\"(pred shape={tuple(y_pred_bt.shape)}, target shape={tuple(y_actual.shape)})\"\n",
    "        )\n",
    "\n",
    "    # Случай редкий: если таргет внезапно >2D — пробуем схлопнуть по всем, кроме батча\n",
    "    if y_actual.dim() > 2:\n",
    "        # Схлопнём таргет к (B, T_act) по последней оси\n",
    "        reduce_dims = tuple(range(1, y_actual.dim() - 1))\n",
    "        if len(reduce_dims) > 0:\n",
    "            y_actual_bt = y_actual.mean(dim=reduce_dims)\n",
    "        else:\n",
    "            y_actual_bt = y_actual\n",
    "        # Рекурсивно выровняем теперь как (B, ?)\n",
    "        return _align_pred_target(y_pred_bt, y_actual_bt)\n",
    "\n",
    "    # Если таргет скалярный (редко, но вдруг), расширим до (B,) повтором\n",
    "    if y_actual.dim() == 0:\n",
    "        y_actual = y_actual.expand(y_pred_bt.size(0))\n",
    "        y_pred_aligned = y_pred_bt[:, -1]\n",
    "        return y_pred_aligned, y_actual\n",
    "\n",
    "    # Если сюда дошли — что-то совсем нетипичное\n",
    "    raise ValueError(\n",
    "        f\"Shapes still mismatch after alignment: pred={tuple(y_pred.shape)} vs target={tuple(y_actual.shape)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Фиксируем seeds для воспроизводимости и стабильности\n",
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    pl.seed_everything(seed, verbose=False)\n",
    "\n",
    "\n",
    "class PeakFriendlyHuber(Metric):\n",
    "  \n",
    "    def __init__(\n",
    "        self,\n",
    "        delta: float = 0.5,\n",
    "        peak_thr: float = 0.85,\n",
    "        peak_weight: float = 1.6,  # Увеличено до 1.6 для stronger поощрения пиков\n",
    "        contrast_weight: float = 0.02,\n",
    "        center_band: float = 0.3,\n",
    "        clip_scale: float = 1.5,  # Новый: scale для soft-clip (tanh * scale, чтобы не сжимать середину сильно)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.delta = float(delta)\n",
    "        self.peak_thr = float(peak_thr)\n",
    "        self.peak_weight = float(peak_weight)\n",
    "        self.contrast_weight = float(contrast_weight)\n",
    "        self.center_band = float(center_band)\n",
    "        self.clip_scale = float(clip_scale)  # Новый параметр\n",
    "        self.mse = MeanSquaredError()\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth_l1(diff, delta):\n",
    "        absd = diff.abs()\n",
    "        return torch.where(absd < delta, 0.5 * (diff ** 2) / delta, absd - 0.5 * delta)\n",
    "\n",
    "    def to_prediction(self, y_pred):\n",
    "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
    "        return super().to_prediction(y_pred)\n",
    "\n",
    "    def to_quantiles(self, y_pred, quantiles=None, **kwargs):\n",
    "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
    "        return super().to_quantiles(y_pred, quantiles=quantiles, **kwargs)\n",
    "\n",
    "    def loss(self, y_pred, y_actual, **kwargs):\n",
    "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
    "        y_actual = _extract_tensor(y_actual, role=\"target\")\n",
    "        y_pred, y_actual = _align_pred_target(y_pred, y_actual)\n",
    "\n",
    "        # Soft-clip (без изменений)\n",
    "        y_pred = torch.tanh(y_pred * self.clip_scale) / self.clip_scale\n",
    "\n",
    "        diff = y_pred - y_actual\n",
    "        base = self._smooth_l1(diff, self.delta)\n",
    "        \n",
    "        # Адаптивный peak_weight: средний по батчу, scale от доли пиков\n",
    "        if self.peak_weight > 1.0:\n",
    "            with torch.no_grad():\n",
    "                peak_mag = torch.relu(y_actual.abs() - self.peak_thr)\n",
    "                peak_frac = (peak_mag > 0).float().mean()  # Доля пиков в батче\n",
    "                adaptive_weight = 1.0 + (self.peak_weight - 1.0) * peak_frac  # Больше веса, если много пиков\n",
    "                w = adaptive_weight * torch.clamp(peak_mag / (1.0 - self.peak_thr + 1e-8), 0.0, 1.0) + 1.0\n",
    "            huber_term = (base * w).mean()\n",
    "        else:\n",
    "            huber_term = base.mean()\n",
    "        \n",
    "        # Лёгкий «anti-flatness» у центра: штрафим чрезмерно малую амплитуду,\n",
    "        # но только там, где таргет далеко от 0.\n",
    "        if self.contrast_weight > 0.0:\n",
    "            with torch.no_grad():\n",
    "                far_mask = (y_actual.abs() >= self.center_band).float()\n",
    "                near_mask = (y_actual.abs() < self.center_band).float()\n",
    "            \n",
    "            # Прямая амплитуда предсказания\n",
    "            far_amp = (y_pred.abs() * far_mask).sum() / (far_mask.sum() + 1e-8)\n",
    "            near_amp = (y_pred.abs() * near_mask).sum() / (near_mask.sum() + 1e-8)\n",
    "            \n",
    "            # Хотим far_amp >= near_amp + margin; введём небольшой margin\n",
    "            margin = 0.05\n",
    "            contrast = torch.relu((near_amp + margin) - far_amp)\n",
    "            loss = huber_term + self.contrast_weight * contrast\n",
    "        else:\n",
    "            loss = huber_term\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def __call__(self, y_pred, y_actual, **kwargs):\n",
    "        return self.loss(y_pred, y_actual, **kwargs)\n",
    "\n",
    "    def update(self, y_pred, y_actual, **kwargs):\n",
    "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
    "        y_actual = _extract_tensor(y_actual, role=\"target\")\n",
    "        y_pred, y_actual = _align_pred_target(y_pred, y_actual)\n",
    "        self.mse.update(y_pred, y_actual)\n",
    "\n",
    "    def compute(self):\n",
    "        return self.mse.compute()\n",
    "\n",
    "    def reset(self):\n",
    "        self.mse.reset()\n",
    "\n",
    "    def name(self):\n",
    "        return \"PeakFriendlyHuber\"\n",
    "\n",
    "class TFTAdapter(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Обёртка над TemporalFusionTransformer,\n",
    "    чтобы Trainer воспринимал модель нужного типа\n",
    "    и наш tft.training_step видел непустой self.trainer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tft: TemporalFusionTransformer):\n",
    "        super().__init__()\n",
    "        self.tft = tft\n",
    "\n",
    "    def on_fit_start(self) -> None:\n",
    "        # вызовется перед стартом Trainer.fit\n",
    "        # прикрепляем Trainer к внутреннему tft\n",
    "        self.tft.trainer = self.trainer\n",
    "        # и логгеры\n",
    "        self.tft.log = self.log\n",
    "        self.tft.log_dict = self.log_dict\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.tft(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.tft.trainer = self.trainer\n",
    "        result = self.tft.training_step(batch, batch_idx)\n",
    "        if isinstance(result, dict):\n",
    "            return result.get(\"loss\")\n",
    "        elif isinstance(result, tuple) and len(result) >= 2:\n",
    "            log_dict, out = result[:2]  # Берем первые два элемента\n",
    "            if isinstance(log_dict, dict):\n",
    "                return log_dict.get(\"loss\")\n",
    "        # Если ни один вариант не подошел\n",
    "        raise ValueError(f\"Unexpected return type from tft.training_step: {type(result)}\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # обеспечим корректную ссылку на тренер внутри tft (если нужно)\n",
    "        self.tft.trainer = self.trainer\n",
    "        self.tft.validation_step(batch, batch_idx)\n",
    "        return\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # Просто передаём вызов внутреннему TFT, без dataloader_idx (не нужен для TFT)\n",
    "        return self.tft.predict_step(batch, batch_idx)\n",
    "\n",
    "    def predict(self, *args, **kwargs):\n",
    "        return self.tft.predict(*args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.tft.configure_optimizers()\n",
    "\n",
    "\n",
    "def to_dense(X):\n",
    "    \"\"\"Преобразование sparse matrix в dense numpy array\"\"\"\n",
    "    if hasattr(X, 'toarray'):\n",
    "        return X.toarray()\n",
    "    return np.asarray(X)\n",
    "\n",
    "def prepare_data_transformer(df, target_col):\n",
    "    df = df.dropna(subset=[target_col])\n",
    "    if 'time' in df.columns and 'time_idx' not in df.columns:\n",
    "        df = df.rename(columns={'time': 'time_idx'})\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col].copy()\n",
    "\n",
    "    exclude = ['time_idx', 'batch']\n",
    "    numeric_features = [\n",
    "        c for c in X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
    "        if c not in exclude\n",
    "    ]\n",
    "    categorical_features = [\n",
    "        c for c in X.select_dtypes(include=['object', 'category']).columns\n",
    "        if c not in exclude\n",
    "    ]\n",
    "\n",
    "    preprocessing = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('yeo', PowerTransformer(method='yeo-johnson'))\n",
    "            ]), numeric_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ],\n",
    "        remainder='passthrough'  # passthrough → тут окажутся сначала все num→cat, а потом time_idx и batch\n",
    "    )\n",
    "    # Возвращаем дополнительные списки для передачи в модель\n",
    "    return X, y, preprocessing, numeric_features, categorical_features\n",
    "\n",
    "def split_features_batch_time(X_array, n_transformed):\n",
    "    \"\"\"\n",
    "    X_array: np.ndarray после преобразований shape=(N, n_transformed + 2)\n",
    "    n_transformed: сколько колонок ушло на num+cat\n",
    "    возвращает (features, time_raw, batch_raw)\n",
    "    \"\"\"\n",
    "    features = X_array[:, :n_transformed]\n",
    "    batch_raw = X_array[:, n_transformed].ravel()\n",
    "    time_raw = X_array[:, n_transformed + 1].ravel()\n",
    "    return features, batch_raw, time_raw\n",
    "\n",
    "\n",
    "def tft_output_transformer(x):\n",
    "    # Больше НЕ клипуем внутри графа. Пусть модель учится выходить за [-1,1],\n",
    "    # а мы ограничим при расчёте метрик и при возврате пользователю.\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_early_stopping_callback(patience=10, min_delta=0.001):\n",
    "    return EarlyStopping(\n",
    "        monitor=\"train_loss\",\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        mode=\"min\",\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "class SequenceTransformerRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Temporal Fusion Transformer для последовательностей.\n",
    "    Интегрируется в Pipeline аналогично LSTM.\n",
    "    Адаптировано для стабильного обучения и алготрейдинга (реального времени).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 seq_len: int = 10,\n",
    "                 pred_len: int = 1,\n",
    "                 hidden_size: int = 64,  # Увеличено для лучшей емкости\n",
    "                 hidden_continuous_size: int = 24,\n",
    "                 epochs: int = 100,  # Увеличено для более долгого обучения\n",
    "                 batch_size: int = 128,  # Увеличено для стабильности\n",
    "                 learning_rate: float = 1e-3,  # Увеличено для более быстрого старта\n",
    "                 patience: int = 15,  # Увеличено для терпимости\n",
    "                 seed: int = 42,\n",
    "                 dropout: float = 0.25,  # Уменьшено для меньшей регуляризации\n",
    "                 weight_decay: float = 1e-4,  # Новый: для регуляризации\n",
    "                 verbose: int = 2,\n",
    "                 mask_prob: float = 0.1,  # Уменьшено, чтобы меньше шумить\n",
    "                 infer_stride: int = 2,\n",
    "                 ckpt_path=None,\n",
    "                 preprocessing=None,\n",
    "                 numeric_features=None,\n",
    "                 categorical_features=None,\n",
    "                 remainder_columns=None,\n",
    "                 min_encoder_length: int = 1):  # Больше логов\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden_continuous_size = hidden_continuous_size\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.patience = patience\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self._model = None\n",
    "        self._trainer = None\n",
    "        self._n_transformed = None\n",
    "        self._n_feat = None\n",
    "        self._train_dataset = None\n",
    "        self._feature_columns = None\n",
    "        self._dropout = dropout\n",
    "        self.norm_eps = 1e-6\n",
    "        self.norm_window = max(10, self.seq_len // 2)\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.weight_decay = weight_decay  # Новый\n",
    "        self.mask_prob = mask_prob\n",
    "        self.infer_stride = infer_stride\n",
    "        self.global_target_mean = 0.0  # Будем вычислять в fit\n",
    "        self.global_target_std = 1.0  # Fallback global scale\n",
    "        self.global_target_min = None\n",
    "        self.global_target_max = None\n",
    "        self.global_range = None\n",
    "        self.soft_clip_scale =  None\n",
    "        self.preprocessing = preprocessing\n",
    "        self.numeric_features = numeric_features or []\n",
    "        self.categorical_features = categorical_features or []\n",
    "        self.remainder_columns = remainder_columns or ['batch', 'time']\n",
    "        self.use_tanh_post = False\n",
    "        self.train_q_lo = None\n",
    "        self.train_q_hi = None\n",
    "        self.clip_scale = 1.5\n",
    "        self.future_fill_mode = \"repeat\" \n",
    "        self.smooth_window = max(5, self.seq_len // 5)  # For savgol\n",
    "        self.smooth_poly = 2\n",
    "        self.ema_alpha = 0.1\n",
    "        self.infer_batch_size = 512  # Новый: большой батч для inference\n",
    "        self.infer_stride = infer_stride if infer_stride is not None else 4 \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'# Добавьте это, если нужно tanh в предикте\n",
    "        self.min_encoder_length = max(1, min_encoder_length)  # Не меньше 1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        set_seeds(self.seed)\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        if self._n_transformed is None:\n",
    "            self._n_transformed = X.shape[1] - 2\n",
    "            self._n_feat = X.shape[1]\n",
    "\n",
    "        feat, batch_raw, time_raw = split_features_batch_time(X, self._n_transformed)\n",
    "\n",
    "        df = pd.DataFrame(feat, columns=[f\"f{i}\" for i in range(feat.shape[1])])\n",
    "        self._feature_columns = df.columns.tolist()\n",
    "\n",
    "        df[\"batch\"] = pd.Series(batch_raw).astype(\"int64\")\n",
    "        df[\"time_raw\"] = pd.to_datetime(time_raw, utc=True, errors=\"coerce\")\n",
    "\n",
    "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "            y = y.reset_index(drop=True)\n",
    "        df[\"target\"] = pd.Series(y, index=df.index).astype(float)\n",
    "\n",
    "        before = len(df)\n",
    "        df = df.dropna(subset=[\"time_raw\", \"target\"]).reset_index(drop=True)\n",
    "        if self.verbose and len(df) < before:\n",
    "            print(f\"Dropped {before - len(df)} rows with invalid time/target\")\n",
    "\n",
    "        df = df.sort_values([\"batch\", \"time_raw\"]).reset_index(drop=True)\n",
    "        df[\"time_idx\"] = df.groupby(\"batch\").cumcount()\n",
    "\n",
    "        min_len = max(1, int(self.pred_len))  # Изменено: позволяем короткие батчи (encoder может быть < seq_len)\n",
    "        gsize = df.groupby(\"batch\").size()\n",
    "        valid_batches = gsize[gsize >= min_len].index\n",
    "        if len(valid_batches) == 0:\n",
    "            raise ValueError(f\"No batches with length >= {min_len}. Reduce pred_len.\")\n",
    "        if self.verbose and len(valid_batches) < gsize.index.nunique():\n",
    "            dropped = sorted(list(set(gsize.index) - set(valid_batches)))\n",
    "            print(f\"Warning: dropped {len(dropped)} short batches: {dropped[:8]}{' ...' if len(dropped) > 8 else ''}\")\n",
    "        df = df[df[\"batch\"].isin(valid_batches)].reset_index(drop=True)\n",
    "\n",
    "        batch_starts = df.groupby(\"batch\")[\"time_raw\"].min().sort_values()\n",
    "        uniq_batches = batch_starts.index.to_numpy()\n",
    "        n_total = len(uniq_batches)\n",
    "        val_frac = 0.3 if n_total >= 10 else 0.1  # Увеличено для лучшего обобщения\n",
    "        n_val = max(1, int(round(n_total * val_frac)))\n",
    "\n",
    "        embargo = 1 if n_total >= 8 else 0\n",
    "\n",
    "        train_end = max(0, n_total - n_val - embargo)\n",
    "        train_batches = uniq_batches[:train_end]\n",
    "        val_batches = uniq_batches[-n_val:]\n",
    "\n",
    "        if len(train_batches) == 0 and n_total > 1:\n",
    "            train_batches = uniq_batches[:-1]\n",
    "            val_batches = uniq_batches[-1:]\n",
    "\n",
    "        train_df = df[df[\"batch\"].isin(train_batches)].copy()\n",
    "        val_df = df[df[\"batch\"].isin(val_batches)].copy()\n",
    "\n",
    "        self.global_target_mean = train_df[\"target\"].mean()\n",
    "        self.global_target_std = train_df[\"target\"].std() + 1e-8\n",
    "        self.global_target_min = train_df[\"target\"].min()\n",
    "        self.global_target_max = train_df[\"target\"].max()\n",
    "        self.global_range = self.global_target_max - self.global_target_min + 1e-8\n",
    "        self.soft_clip_scale = max(1.0, self.global_target_std * 1.5)  # Adaptive soft clip for stable [-1,1] without compression\n",
    "\n",
    "        #self.clip_scale = max(1.0, self.global_target_std * 1.2)   # Adaptive to train variance\n",
    "\n",
    "        train_targets = train_df[\"target\"].values\n",
    "        #if len(train_targets) > 0:\n",
    "        #   self.train_q_lo, self.train_q_hi = np.quantile(train_targets, [0.01, 0.99])\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Train batches: {len(np.unique(train_batches))}\")\n",
    "            print(f\"Val batches: {len(np.unique(val_batches))}\")\n",
    "            print(f\"Train rows: {len(train_df)}, Val rows: {len(val_df)}\")\n",
    "\n",
    "        full_dataset = TimeSeriesDataSet(\n",
    "            df,\n",
    "            time_idx=\"time_idx\",\n",
    "            target=\"target\",\n",
    "            group_ids=[\"batch\"],\n",
    "            max_encoder_length=int(self.seq_len),\n",
    "            min_encoder_length=0,#int(self.min_encoder_length),  # Новый: позволяем короткие encoder\n",
    "            max_prediction_length=int(self.pred_len),\n",
    "            time_varying_unknown_reals=self._feature_columns,\n",
    "            target_normalizer=None,\n",
    "            allow_missing_timesteps=True,\n",
    "            add_relative_time_idx=True,\n",
    "            add_target_scales=False,\n",
    "            add_encoder_length=True,\n",
    "            min_prediction_length=1,\n",
    "        )\n",
    "\n",
    "        train_dataset = TimeSeriesDataSet.from_dataset(\n",
    "            full_dataset, train_df, predict=False, stop_randomization=True\n",
    "        )\n",
    "        val_dataset = TimeSeriesDataSet.from_dataset(\n",
    "            full_dataset, val_df, predict=False, stop_randomization=True\n",
    "        ) if len(val_df) > 0 else None\n",
    "\n",
    "        train_dl = train_dataset.to_dataloader(\n",
    "            train=True,\n",
    "            batch_size=int(self.batch_size),\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            worker_init_fn=None,\n",
    "            drop_last=False,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "        val_dl = None\n",
    "        if val_dataset is not None and len(val_dataset) > 0:\n",
    "            val_dl = val_dataset.to_dataloader(\n",
    "                train=False,\n",
    "                batch_size=int(self.batch_size),\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                worker_init_fn=None,\n",
    "                drop_last=False,\n",
    "                persistent_workers=False,\n",
    "            )\n",
    "\n",
    "        tft = CustomTFT.from_dataset(\n",
    "            train_dataset,\n",
    "            hidden_size=int(self.hidden_size),\n",
    "            output_size=1,\n",
    "            loss=PeakFriendlyHuber(\n",
    "                delta=0.5,\n",
    "                peak_thr=0.85,  # можно затем подвинуть 0.8..0.9\n",
    "                peak_weight=1.3,  # аккуратно: 1.3..1.6\n",
    "                contrast_weight=0.03,  # очень маленькая добавка\n",
    "                center_band=0.3,  # что считать «центром»\n",
    "                clip_scale=1.5\n",
    "            ),\n",
    "            optimizer=\"adam\",\n",
    "            learning_rate=float(self.learning_rate),  # оставьте тот, на котором MSE был лучше (у вас 1e-4 давал ~0.186)\n",
    "            lstm_layers=3,\n",
    "            hidden_continuous_size=self.hidden_continuous_size,\n",
    "            attention_head_size=4,\n",
    "            dropout=float(self._dropout),\n",
    "            reduce_on_plateau_patience=5,\n",
    "            reduce_on_plateau_min_lr=1e-6,\n",
    "            weight_decay=float(self.weight_decay),\n",
    "            mask_prob=float(self.mask_prob),\n",
    "            output_transformer=tft_output_transformer,\n",
    "        )\n",
    "\n",
    "        class GCCallback(pl.Callback):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.last_mem = psutil.Process().memory_info().rss / 1e6\n",
    "                \n",
    "            def _check_gc(self):\n",
    "                current_mem = psutil.Process().memory_info().rss / 1e6\n",
    "                if current_mem - self.last_mem > 50:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                self.last_mem = current_mem\n",
    "                \n",
    "            def on_train_epoch_end(self, trainer, pl_module):\n",
    "                self._check_gc()\n",
    "                \n",
    "            def on_validation_epoch_end(self, trainer, pl_module):\n",
    "                self._check_gc()\n",
    "            \n",
    "            def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "                if batch_idx % 10 == 0:\n",
    "                    self._check_gc()\n",
    "            \n",
    "            def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "                if batch_idx % 10 == 0:\n",
    "                    self._check_gc()\n",
    "\n",
    "        callbacks = []\n",
    "        if val_dl is not None:\n",
    "            callbacks.append(\n",
    "                get_early_stopping_callback(patience=self.patience, min_delta=1e-3))  # Новый: больше patience\n",
    "            checkpoint_callback = ModelCheckpoint(monitor=\"train_loss\", mode=\"min\", save_top_k=1, verbose=True)\n",
    "            callbacks.append(checkpoint_callback)\n",
    "        else:\n",
    "            checkpoint_callback = None\n",
    "\n",
    "        logger = TensorBoardLogger(save_dir=\"lightning_logs/\", name=\"my_model\") if self.verbose > 0 else False\n",
    "        if self.verbose > 0:\n",
    "            callbacks.append(LearningRateMonitor(logging_interval=\"step\"))\n",
    "            callbacks.append(NoValidationBar(refresh_rate=20))\n",
    "        callbacks.append(GCCallback())\n",
    "\n",
    "        self._model = TFTAdapter(tft)\n",
    "        self._trainer = pl.Trainer(\n",
    "            max_epochs=int(self.epochs),\n",
    "            enable_checkpointing=(checkpoint_callback is not None),\n",
    "            callbacks=callbacks,\n",
    "            logger=logger,\n",
    "            enable_model_summary=True,\n",
    "            gradient_clip_val=1.0,\n",
    "            gradient_clip_algorithm=\"norm\",\n",
    "            deterministic=True,\n",
    "            benchmark=False,\n",
    "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "            precision=32,\n",
    "            limit_val_batches=1.0 if val_dl is not None else 0.0,\n",
    "            enable_progress_bar=self.verbose > 0,\n",
    "            log_every_n_steps=50,\n",
    "            num_sanity_val_steps=0,\n",
    "        )\n",
    "\n",
    "        self._trainer.fit(\n",
    "            self._model,\n",
    "            train_dataloaders=train_dl,\n",
    "            val_dataloaders=val_dl if val_dl is not None else None,\n",
    "            ckpt_path=self.ckpt_path if self.ckpt_path and self.epochs > 0 else None,\n",
    "        )\n",
    "\n",
    "        if checkpoint_callback is not None and checkpoint_callback.best_model_path:\n",
    "            best_path = checkpoint_callback.best_model_path\n",
    "            if self.verbose:\n",
    "                print(f\"Loaded best model from {best_path} with val_loss={checkpoint_callback.best_model_score}\")\n",
    "            self._model = TFTAdapter.load_from_checkpoint(best_path, tft=tft)\n",
    "\n",
    "        self._train_dataset = full_dataset\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        #return self\n",
    "        try:\n",
    "            y_train = train_df[\"target\"].to_numpy(dtype=float)\n",
    "            # robust percentiles — перестрахуемся от выносов: 1% и 99%\n",
    "            self._cal_p_low = float(np.nanpercentile(y_train, 1))\n",
    "            self._cal_p_high = float(np.nanpercentile(y_train, 99))\n",
    "            # амплитуды «типичных пиков»\n",
    "            top_mask = y_train >= self._cal_p_high\n",
    "            bot_mask = y_train <= self._cal_p_low\n",
    "            self._cal_mean_top = float(np.nanmean(y_train[top_mask])) if np.any(top_mask) else float(self.global_target_max)\n",
    "            self._cal_mean_bot = float(np.nanmean(y_train[bot_mask])) if np.any(bot_mask) else float(self.global_target_min)\n",
    "            # защита от вырождения\n",
    "            if not np.isfinite(self._cal_mean_top): self._cal_mean_top = float(self.global_target_max)\n",
    "            if not np.isfinite(self._cal_mean_bot): self._cal_mean_bot = float(self.global_target_min)\n",
    "        except Exception:\n",
    "            # безопасные фолбэки\n",
    "            self._cal_p_low, self._cal_p_high = self.global_target_min, self.global_target_max\n",
    "            self._cal_mean_top, self._cal_mean_bot = self.global_target_max, self.global_target_min\n",
    "        # ----------------------------------------------\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self._train_dataset is None or self._model is None:\n",
    "            raise RuntimeError(\"Model is not fitted yet\")\n",
    "    \n",
    "        # Full suppress context (no logs/output, including PL/Torch/PF/Seed/GPU/TPU/HPU messages)\n",
    "        @contextlib.contextmanager\n",
    "        def suppress_all():\n",
    "            with open(os.devnull, \"w\") as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n",
    "                # Подавление всех возможных логгеров\n",
    "                root_logger = logging.getLogger()\n",
    "                old_level = root_logger.level\n",
    "                root_logger.setLevel(logging.CRITICAL + 1)  # Выше CRITICAL, чтобы ничего не логировалось\n",
    "                \n",
    "                # Специфические логгеры (расширенный список)\n",
    "                for logger_name in [\n",
    "                    \"pytorch_lightning\", \"lightning.pytorch\", \"lightning\", \n",
    "                    \"torch\", \"pytorch_forecasting\", \"optuna\", \n",
    "                    \"sklearn\", \"joblib\", \"numpy\", \"pandas\", \n",
    "                    \"scipy\", \"matplotlib\", \"seaborn\", \"plotly\", \n",
    "                    \"shap\", \"statsmodels\", \"torchmetrics\",\n",
    "                    \"lightning.pytorch.utilities.migration.utils\",  # Для Attribute 'loss' warnings\n",
    "                    \"lightning.pytorch.utilities.migration\", \n",
    "                    \"lightning.pytorch.utilities\", \n",
    "                    \"lightning.pytorch\"\n",
    "                ]:\n",
    "                    logging.getLogger(logger_name).setLevel(logging.CRITICAL + 1)\n",
    "                \n",
    "                # Подавление всех предупреждений (расширенный список)\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*Attribute 'loss' is an instance of nn.Module.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*Attribute 'logging_metrics' is an instance of nn.Module.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*GPU available.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*TPU available.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*HPU available.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*This Pipeline instance is not fitted yet.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*Using an existing study with name.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*A value is trying to be set on a copy of a slice.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*The behavior of DataFrame concatenation.*\")\n",
    "                warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: the use_reentrant parameter.*\")\n",
    "                \n",
    "                # Окружение (расширенное)\n",
    "                os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "                os.environ['LITMODELS_DISABLE_TIP'] = '1'\n",
    "                os.environ['HYDRA_FULL_ERROR'] = '1'\n",
    "                os.environ['TQDM_DISABLE'] = '0'  # Не подавлять tqdm (если вызван снаружи)\n",
    "                os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Для синхронизации CUDA, но без логов\n",
    "                os.environ['TORCH_USE_DETERMINISTIC_ALGORITHMS'] = '1'  # Без логов\n",
    "                \n",
    "                try:\n",
    "                    yield\n",
    "                finally:\n",
    "                    root_logger.setLevel(old_level)\n",
    "                    warnings.resetwarnings()\n",
    "    \n",
    "        with suppress_all():\n",
    "            set_seeds(self.seed)\n",
    "            pl.seed_everything(self.seed, verbose=False, workers=True)\n",
    "    \n",
    "            if self.preprocessing is not None and not isinstance(X, np.ndarray):\n",
    "                X = self.preprocessing.transform(X)\n",
    "    \n",
    "            X = np.asarray(X)\n",
    "            N = X.shape[0]\n",
    "            if N < self.min_encoder_length:\n",
    "                return [float(np.nan)] * N\n",
    "    \n",
    "            feat, _, time_raw = split_features_batch_time(X, self._n_transformed)\n",
    "            df = pd.DataFrame(feat, columns=self._feature_columns)\n",
    "            df[\"__row_id\"] = np.arange(N)\n",
    "            df[\"time_raw\"] = pd.to_datetime(time_raw, utc=True, errors=\"coerce\")\n",
    "            df[\"batch\"] = np.int64(0)\n",
    "            df[\"target\"] = self.global_target_mean\n",
    "    \n",
    "            df = df.dropna(subset=[\"time_raw\"]).reset_index(drop=True)\n",
    "            df_sorted = df.sort_values(\"time_raw\").reset_index(drop=True)\n",
    "            df_sorted[\"time_idx\"] = np.arange(len(df_sorted), dtype=np.int64)\n",
    "    \n",
    "            eff_N = len(df_sorted)\n",
    "            if eff_N < self.min_encoder_length:\n",
    "                out = np.full(N, np.nan, dtype=float)\n",
    "                return out.tolist()\n",
    "    \n",
    "            step_ns = int(60 * 1e9)\n",
    "            if eff_N >= 2:\n",
    "                diffs = df_sorted[\"time_raw\"].view(\"int64\").astype(\"int64\").to_numpy()\n",
    "                diffs = np.diff(diffs)\n",
    "                step_ns = int(np.nan_to_num(np.median(diffs), nan=step_ns))\n",
    "                if step_ns <= 0:\n",
    "                    step_ns = int(60 * 1e9)\n",
    "    \n",
    "            out_raw = np.full(eff_N, np.nan, dtype=float)\n",
    "    \n",
    "            tft = self._model.tft\n",
    "            orig_log = tft.log\n",
    "            tft.log = lambda *args, **kwargs: None\n",
    "    \n",
    "            try:\n",
    "                eff_encoder_len = min(self.seq_len, eff_N)\n",
    "                M = max(0, eff_N - eff_encoder_len + 1)\n",
    "                stride = self.pred_len  # Keep original stride to preserve logic\n",
    "                window_starts = np.arange(0, M, stride)\n",
    "                K = len(window_starts)\n",
    "    \n",
    "                if K == 0:\n",
    "                    K = 1\n",
    "                    window_starts = np.array([0])\n",
    "                    eff_encoder_len = eff_N\n",
    "    \n",
    "                num_feat_cols = len(self._feature_columns)\n",
    "    \n",
    "                # Vectorized construction of all_feats\n",
    "                enc_indices = window_starts[:, np.newaxis] + np.arange(eff_encoder_len)\n",
    "                all_enc_feats = feat[enc_indices]  # (K, eff_encoder_len, num_feat_cols)\n",
    "                last_enc_feats = all_enc_feats[:, -1, :]\n",
    "                all_fut_feats = np.repeat(last_enc_feats[:, np.newaxis, :], self.pred_len, axis=1)  # (K, pred_len, num_feat_cols)\n",
    "                all_feats = np.concatenate([all_enc_feats, all_fut_feats], axis=1).reshape(-1, num_feat_cols)\n",
    "    \n",
    "                # Vectorized all_time_idx\n",
    "                orig_time_idx = df_sorted[\"time_idx\"].values\n",
    "                all_enc_time_idx = orig_time_idx[enc_indices]  # (K, eff_encoder_len)\n",
    "                last_time_idx = all_enc_time_idx[:, -1]\n",
    "                fut_offsets = np.arange(1, self.pred_len + 1)\n",
    "                all_fut_time_idx = last_time_idx[:, np.newaxis] + fut_offsets  # (K, pred_len)\n",
    "                all_time_idx = np.concatenate([all_enc_time_idx, all_fut_time_idx], axis=1).reshape(-1)\n",
    "    \n",
    "                # Vectorized all_batch\n",
    "                batch_per_window = np.arange(K)[:, np.newaxis]\n",
    "                all_enc_batch = np.repeat(batch_per_window, eff_encoder_len, axis=1)  # (K, eff_encoder_len)\n",
    "                all_fut_batch = np.repeat(batch_per_window, self.pred_len, axis=1)  # (K, pred_len)\n",
    "                all_batch = np.concatenate([all_enc_batch, all_fut_batch], axis=1).reshape(-1)\n",
    "    \n",
    "                # all_target (constant)\n",
    "                all_target = np.full(K * (eff_encoder_len + self.pred_len), self.global_target_mean, dtype=np.float32)\n",
    "    \n",
    "                # Vectorized all_time_raw using int64 ns\n",
    "                orig_time_raw_int = df_sorted[\"time_raw\"].view(\"int64\").values\n",
    "                all_enc_time_int = orig_time_raw_int[enc_indices]  # (K, eff_encoder_len)\n",
    "                last_time_int = all_enc_time_int[:, -1]\n",
    "                fut_offsets_ns = fut_offsets * step_ns\n",
    "                all_fut_time_int = last_time_int[:, np.newaxis] + fut_offsets_ns  # (K, pred_len)\n",
    "                all_time_int_flat = np.concatenate([all_enc_time_int.reshape(-1), all_fut_time_int.reshape(-1)])\n",
    "                all_time_raw = pd.to_datetime(all_time_int_flat, unit='ns', utc=True).values  # object array of Timestamp\n",
    "    \n",
    "                pred_df = pd.DataFrame(all_feats, columns=self._feature_columns)\n",
    "                pred_df[\"time_idx\"] = all_time_idx\n",
    "                pred_df[\"batch\"] = all_batch\n",
    "                pred_df[\"target\"] = all_target\n",
    "                pred_df[\"time_raw\"] = all_time_raw\n",
    "    \n",
    "                pred_df[\"batch\"] = pred_df[\"batch\"].astype(\"int64\")\n",
    "                pred_df[\"time_idx\"] = pred_df[\"time_idx\"].astype(\"int64\")\n",
    "                pred_df[\"target\"] = pred_df[\"target\"].astype(\"float32\")\n",
    "    \n",
    "                sliding_dataset = TimeSeriesDataSet.from_dataset(\n",
    "                    self._train_dataset,\n",
    "                    pred_df,\n",
    "                    predict=True,\n",
    "                    stop_randomization=True,\n",
    "                    min_prediction_length=self.pred_len,\n",
    "                    max_prediction_length=self.pred_len,\n",
    "                )\n",
    "    \n",
    "                test_dl = sliding_dataset.to_dataloader(\n",
    "                    train=False,\n",
    "                    batch_size=K,  # Full batch for max speed (K small due to stride=pred_len)\n",
    "                    num_workers=0,\n",
    "                    persistent_workers=False,\n",
    "                    pin_memory=False,\n",
    "                )\n",
    "    \n",
    "                preds = tft.predict(\n",
    "                    test_dl,\n",
    "                    mode=\"prediction\",\n",
    "                    return_x=False,\n",
    "                    trainer_kwargs={\n",
    "                        \"logger\": False, \n",
    "                        \"enable_progress_bar\": False, \n",
    "                        \"enable_model_summary\": False, \n",
    "                        \"enable_checkpointing\": False, \n",
    "                        \"accelerator\": \"gpu\" if self.device == \"cuda\" else \"cpu\"\n",
    "                    },\n",
    "                )\n",
    "    \n",
    "                if isinstance(preds, torch.Tensor):\n",
    "                    preds = preds.detach().cpu()\n",
    "                    if preds.dim() == 3 and preds.size(-1) == 1:\n",
    "                        preds = preds.squeeze(-1)\n",
    "                    if preds.dim() == 2:\n",
    "                        yhat_stride = preds[:, 0].numpy()  # First step per window\n",
    "                    elif preds.dim() == 1:\n",
    "                        yhat_stride = preds.numpy()\n",
    "                    elif preds.dim() == 0:\n",
    "                        yhat_stride = np.array([preds.item()])\n",
    "                    else:\n",
    "                        yhat_stride = np.full(K, self.global_target_mean)\n",
    "                else:\n",
    "                    yhat_stride = np.full(K, self.global_target_mean)\n",
    "    \n",
    "                # Fill out_raw at window ends\n",
    "                for j, i in enumerate(window_starts):\n",
    "                    out_raw[i + eff_encoder_len - 1] = yhat_stride[j]\n",
    "    \n",
    "                # Causal interpolation for missed points (linear from past, no future look)\n",
    "                s = pd.Series(out_raw)\n",
    "                s = s.interpolate(method='linear', limit_direction='forward')  # Only forward for causality\n",
    "                s = s.ffill()  # Fill initial with first pred\n",
    "                out_raw = s.values\n",
    "    \n",
    "                # Global map to [-1,1] (consistent scale, preserves relative peaks)\n",
    "                out = (out_raw - self.global_target_min) / self.global_range * 2 - 1\n",
    "                out = np.tanh(out * self.soft_clip_scale) / np.tanh(self.soft_clip_scale)\n",
    "                out = np.clip(out, -1.0, 1.0)\n",
    "    \n",
    "                # Fill any remaining nans (unlikely) with mapped mean\n",
    "                nan_mask = np.isnan(out)\n",
    "                mapped_mean = (self.global_target_mean - self.global_target_min) / self.global_range * 2 - 1\n",
    "                out[nan_mask] = mapped_mean\n",
    "    \n",
    "            finally:\n",
    "                tft.log = orig_log\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "            original_pos = df_sorted[\"__row_id\"].to_numpy()\n",
    "            out_final = np.full(N, np.nan, dtype=float)\n",
    "            out_final[original_pos] = out\n",
    "    \n",
    "            return out_final.tolist()\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # Чтобы sklearn корректно передавал параметры в Pipeline\n",
    "        return {\n",
    "            \"seq_len\": self.seq_len,\n",
    "            \"pred_len\": self.pred_len,\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"patience\": self.patience,\n",
    "            \"seed\": self.seed,\n",
    "            \"dropout\": self._dropout,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"ckpt_path\": self.ckpt_path\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "def _tensor_state_dict_to(dtype: torch.dtype, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    out = {}\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            # важно: сохраняем в CPU и ровно в dtype (float32 для идентичности)\n",
    "            out[k] = v.detach().to('cpu', dtype=dtype)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "def _save_bytes_compressed_zip(file_path: str, bytes_map: Dict[str, bytes], compression=zipfile.ZIP_DEFLATED, compresslevel=9):\n",
    "    with zipfile.ZipFile(file_path, mode='w', compression=compression, compresslevel=compresslevel) as zf:\n",
    "        for name, b in bytes_map.items():\n",
    "            zf.writestr(name, b)\n",
    "\n",
    "\n",
    "def _load_bytes_from_zip(file_path: str) -> Dict[str, bytes]:\n",
    "    out = {}\n",
    "    with zipfile.ZipFile(file_path, mode='r') as zf:\n",
    "        for name in zf.namelist():\n",
    "            out[name] = zf.read(name)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _safe_json_dump(obj: Dict[str, Any]) -> bytes:\n",
    "    def to_builtin(x):\n",
    "        import numpy as _np\n",
    "        import pandas as _pd\n",
    "        if isinstance(x, (_np.integer,)):\n",
    "            return int(x)\n",
    "        if isinstance(x, (_np.floating,)):\n",
    "            return float(x)\n",
    "        if isinstance(x, (_np.ndarray,)):\n",
    "            return x.tolist()\n",
    "        if isinstance(x, (_pd.Timestamp,)):\n",
    "            return x.isoformat()\n",
    "        return x\n",
    "\n",
    "    def convert(v):\n",
    "        if isinstance(v, dict):\n",
    "            return {k: convert(val) for k, val in v.items()}\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            return [convert(i) for i in v]\n",
    "        return to_builtin(v)\n",
    "\n",
    "    return json.dumps(convert(obj), ensure_ascii=False, separators=(\",\", \":\")).encode(\"utf-8\")\n",
    "\n",
    "\n",
    "def _safe_json_load(b: bytes) -> Dict[str, Any]:\n",
    "    return json.loads(b.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def _extract_tsd_feature_lists(tds) -> Dict[str, Any]:\n",
    "    def g(name, default=None):\n",
    "        return getattr(tds, name, default)\n",
    "\n",
    "    lists = {}\n",
    "    for name in [\n",
    "        \"time_varying_known_reals\",\n",
    "        \"time_varying_unknown_reals\",\n",
    "        \"static_reals\",\n",
    "        \"time_varying_known_categoricals\",\n",
    "        \"time_varying_unknown_categoricals\",\n",
    "        \"static_categoricals\",\n",
    "        \"target_categoricals\",\n",
    "        \"known_reals\",\n",
    "        \"unknown_reals\",\n",
    "        \"known_categoricals\",\n",
    "        \"unknown_categoricals\",\n",
    "        \"reals\",\n",
    "        \"categoricals\",\n",
    "    ]:\n",
    "        val = g(name, None)\n",
    "        if val is not None:\n",
    "            try:\n",
    "                lists[name] = list(val)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return lists\n",
    "\n",
    "\n",
    "def save_transformer(\n",
    "    pipeline: Pipeline,\n",
    "    path: str,\n",
    "    *,\n",
    "    # ЖЁСТКО: float32 по умолчанию для идентичности. Не выставляйте True, пока не сравните предикты.\n",
    "    float16_weights: bool = False,\n",
    "    preprocessing_filename: str = \"preprocessing.joblib.lzma\",\n",
    "    model_zip_filename: str = \"model_weights.zip\",\n",
    "    meta_json_filename: str = \"meta.json\"\n",
    "):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    if not isinstance(pipeline, Pipeline):\n",
    "        raise TypeError(\"Expected sklearn Pipeline\")\n",
    "\n",
    "    steps_dict = dict(pipeline.named_steps)\n",
    "    if \"preprocessing\" not in steps_dict or \"model\" not in steps_dict:\n",
    "        raise ValueError(\"Pipeline must have 'preprocessing' and 'model' steps\")\n",
    "\n",
    "    preprocessing = steps_dict[\"preprocessing\"]\n",
    "    model: SequenceTransformerRegressor = steps_dict[\"model\"]\n",
    "\n",
    "    if model._model is None or model._train_dataset is None:\n",
    "        raise RuntimeError(\"Model must be fitted before saving\")\n",
    "\n",
    "    # 1) preprocessing\n",
    "    prep_path = os.path.join(path, preprocessing_filename)\n",
    "    with lzma.open(prep_path, \"wb\", preset=9) as f:\n",
    "        joblib.dump(preprocessing, f)\n",
    "\n",
    "    # 2) параметры TDS\n",
    "    tds = model._train_dataset\n",
    "    def g(name, default=None):\n",
    "        return getattr(tds, name, default)\n",
    "\n",
    "    train_dataset_params = {\n",
    "        \"time_idx\": g(\"time_idx\", \"time_idx\"),\n",
    "        \"target\": g(\"target\", \"target\"),\n",
    "        \"group_ids\": list(g(\"group_ids\", [\"batch\"])),\n",
    "        \"max_encoder_length\": int(g(\"max_encoder_length\", model.seq_len)),\n",
    "        \"max_prediction_length\": int(g(\"max_prediction_length\", model.pred_len)),\n",
    "        \"target_normalizer\": None,\n",
    "        \"allow_missing_timesteps\": bool(g(\"allow_missing_timesteps\", True)),\n",
    "        \"add_relative_time_idx\": bool(g(\"add_relative_time_idx\", True)),\n",
    "        \"add_target_scales\": bool(g(\"add_target_scales\", False)),\n",
    "        \"add_encoder_length\": bool(g(\"add_encoder_length\", True)),\n",
    "        \"min_prediction_length\": int(g(\"min_prediction_length\", 1)),\n",
    "        \"min_encoder_length\": int(g(\"min_encoder_length\", 0)),\n",
    "    }\n",
    "    feature_lists = _extract_tsd_feature_lists(tds)\n",
    "\n",
    "    # 3) веса TFT — строго в float32 (если float16_weights=False)\n",
    "    tft = model._model.tft\n",
    "    state = tft.state_dict()\n",
    "    dtype = torch.float16 if float16_weights else torch.float32\n",
    "    state = _tensor_state_dict_to(dtype, state)\n",
    "    buf = io.BytesIO()\n",
    "    torch.save(state, buf, _use_new_zipfile_serialization=True)\n",
    "    buf.seek(0)\n",
    "    weights_zip_path = os.path.join(path, model_zip_filename)\n",
    "    _save_bytes_compressed_zip(weights_zip_path, {\"tft_state_dict.pt\": buf.getvalue()})\n",
    "\n",
    "    # 4) метаданные\n",
    "    meta = {\n",
    "        \"class\": \"SequenceTransformerRegressor\",\n",
    "        \"params\": {\n",
    "            \"seq_len\": model.seq_len,\n",
    "            \"pred_len\": model.pred_len,\n",
    "            \"hidden_size\": model.hidden_size,\n",
    "            \"hidden_continuous_size\": model.hidden_continuous_size,\n",
    "            \"epochs\": model.epochs,\n",
    "            \"batch_size\": model.batch_size,\n",
    "            \"learning_rate\": model.learning_rate,\n",
    "            \"patience\": model.patience,\n",
    "            \"seed\": model.seed,\n",
    "            \"dropout\": model._dropout,\n",
    "            \"weight_decay\": model.weight_decay,\n",
    "            \"verbose\": model.verbose,\n",
    "            \"mask_prob\": model.mask_prob,\n",
    "            \"infer_stride\": model.infer_stride,\n",
    "            \"ckpt_path\": None,\n",
    "            \"min_encoder_length\": model.min_encoder_length,\n",
    "            \"lstm_layers\": 3,\n",
    "            \"attention_head_size\": 4,\n",
    "            \"output_size\": 1,\n",
    "        },\n",
    "        \"feature_columns\": model._feature_columns,\n",
    "        \"n_transformed\": model._n_transformed,\n",
    "        \"n_feat\": model._n_feat,\n",
    "        \"global_stats\": {\n",
    "            \"global_target_mean\": model.global_target_mean,\n",
    "            \"global_target_std\": model.global_target_std,\n",
    "            \"global_target_min\": model.global_target_min,\n",
    "            \"global_target_max\": model.global_target_max,\n",
    "            \"global_range\": model.global_range,\n",
    "            \"soft_clip_scale\": model.soft_clip_scale,\n",
    "            \"clip_scale\": model.clip_scale,\n",
    "        },\n",
    "        \"train_dataset_params\": train_dataset_params,\n",
    "        \"feature_lists\": feature_lists,\n",
    "        \"torch_dtype\": \"float16\" if float16_weights else \"float32\",\n",
    "        \"preproc_feature_names_out\": list(getattr(preprocessing, \"get_feature_names_out\", lambda: [])()),\n",
    "    }\n",
    "\n",
    "    meta_path = os.path.join(path, meta_json_filename)\n",
    "    with open(meta_path, \"wb\") as f:\n",
    "        f.write(_safe_json_dump(meta))\n",
    "\n",
    "    weights_size_mb = os.path.getsize(weights_zip_path) / (1024 * 1024)\n",
    "    prep_size_mb = os.path.getsize(prep_path) / (1024 * 1024)\n",
    "    meta_size_kb = os.path.getsize(meta_path) / 1024.0\n",
    "    print(f\"Saved: weights={weights_size_mb:.2f} MB, preprocessing={prep_size_mb:.2f} MB, meta={meta_size_kb:.1f} KB → dir={path}\")\n",
    "\n",
    "# Требуется наличие в окружении:\n",
    "# - SequenceTransformerRegressor\n",
    "# - TimeSeriesDataSet\n",
    "# - CustomTFT, TFTAdapter\n",
    "# - tft_output_transformer\n",
    "# - PeakFriendlyHuber\n",
    "# - to_dense\n",
    "# - вспомогательные функции _load_bytes_from_zip, _safe_json_load из предыдущей версии\n",
    "\n",
    "\n",
    "def load_transformer_exact(\n",
    "    path: str,\n",
    "    *,\n",
    "    preprocessing_filename: str = \"preprocessing.joblib.lzma\",\n",
    "    model_zip_filename: str = \"model_weights.zip\",\n",
    "    meta_json_filename: str = \"meta.json\",\n",
    "    device: Optional[str] = None,\n",
    ") -> Pipeline:\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Attribute 'loss' is an instance of `nn.Module`.*\")\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Attribute 'logging_metrics' is an instance of `nn.Module`.*\")\n",
    "\n",
    "    # 1) preprocessing\n",
    "    prep_path = os.path.join(path, preprocessing_filename)\n",
    "    with lzma.open(prep_path, \"rb\") as f:\n",
    "        preprocessing = joblib.load(f)\n",
    "\n",
    "    # 2) meta\n",
    "    meta_path = os.path.join(path, meta_json_filename)\n",
    "    if not os.path.isfile(meta_path):\n",
    "        raise FileNotFoundError(meta_path)\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = json.loads(f.read().decode(\"utf-8\"))\n",
    "\n",
    "    params = meta[\"params\"]\n",
    "    feature_columns = meta[\"feature_columns\"]\n",
    "    n_transformed = meta[\"n_transformed\"]\n",
    "    n_feat = meta[\"n_feat\"]\n",
    "    global_stats = meta[\"global_stats\"]\n",
    "    tds_params = meta[\"train_dataset_params\"]\n",
    "    feature_lists = meta.get(\"feature_lists\", {})\n",
    "    saved_torch_dtype = meta.get(\"torch_dtype\", \"float32\")\n",
    "\n",
    "    # 3) регрессор\n",
    "    model = SequenceTransformerRegressor(\n",
    "        seq_len=params[\"seq_len\"],\n",
    "        pred_len=params[\"pred_len\"],\n",
    "        hidden_size=params[\"hidden_size\"],\n",
    "        hidden_continuous_size=params[\"hidden_continuous_size\"],\n",
    "        epochs=params[\"epochs\"],\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        patience=params[\"patience\"],\n",
    "        seed=params[\"seed\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        verbose=params[\"verbose\"],\n",
    "        mask_prob=params[\"mask_prob\"],\n",
    "        infer_stride=params[\"infer_stride\"],\n",
    "        ckpt_path=None,\n",
    "        preprocessing=preprocessing,\n",
    "        min_encoder_length=params.get(\"min_encoder_length\", 1),\n",
    "    )\n",
    "\n",
    "    model._feature_columns = feature_columns\n",
    "    model._n_transformed = n_transformed\n",
    "    model._n_feat = n_feat\n",
    "    model.global_target_mean = float(global_stats[\"global_target_mean\"])\n",
    "    model.global_target_std = float(global_stats[\"global_target_std\"])\n",
    "    model.global_target_min = float(global_stats[\"global_target_min\"])\n",
    "    model.global_target_max = float(global_stats[\"global_target_max\"])\n",
    "    model.global_range = float(global_stats[\"global_range\"])\n",
    "    model.soft_clip_scale = float(global_stats[\"soft_clip_scale\"])\n",
    "    model.clip_scale = float(global_stats.get(\"clip_scale\", 1.5))\n",
    "    model.device = device\n",
    "\n",
    "    # 4) синтетический df\n",
    "    max_enc = int(tds_params[\"max_encoder_length\"])\n",
    "    max_pred = int(tds_params[\"max_prediction_length\"])\n",
    "    num_feat_cols = len(feature_columns)\n",
    "    rows = max_enc + max_pred\n",
    "\n",
    "    synth_df_base = pd.DataFrame(\n",
    "        np.zeros((rows, num_feat_cols), dtype=np.float32),\n",
    "        columns=feature_columns,\n",
    "    )\n",
    "    synth_df_base[\"time_idx\"] = np.arange(rows, dtype=np.int64)\n",
    "    synth_df_base[\"target\"] = np.zeros(rows, dtype=np.float32)\n",
    "    synth_df_base[\"batch\"] = 0\n",
    "\n",
    "    # Попытка №1: «обычная» конструкция TDS как при обучении\n",
    "    tds_kwargs = dict(\n",
    "        time_idx=tds_params[\"time_idx\"],\n",
    "        target=tds_params[\"target\"],\n",
    "        group_ids=tds_params[\"group_ids\"],\n",
    "        max_encoder_length=tds_params[\"max_encoder_length\"],\n",
    "        max_prediction_length=tds_params[\"max_prediction_length\"],\n",
    "        target_normalizer=None,\n",
    "        allow_missing_timesteps=tds_params[\"allow_missing_timesteps\"],\n",
    "        add_relative_time_idx=tds_params[\"add_relative_time_idx\"],\n",
    "        add_target_scales=tds_params[\"add_target_scales\"],\n",
    "        add_encoder_length=tds_params[\"add_encoder_length\"],\n",
    "        min_prediction_length=tds_params[\"min_prediction_length\"],\n",
    "        min_encoder_length=tds_params.get(\"min_encoder_length\", 0),\n",
    "    )\n",
    "\n",
    "    if \"time_varying_unknown_reals\" in feature_lists:\n",
    "        tds_kwargs[\"time_varying_unknown_reals\"] = list(feature_lists[\"time_varying_unknown_reals\"])\n",
    "    else:\n",
    "        tds_kwargs[\"time_varying_unknown_reals\"] = list(feature_columns)\n",
    "\n",
    "    for key in [\n",
    "        \"time_varying_known_reals\",\n",
    "        \"static_reals\",\n",
    "        \"time_varying_known_categoricals\",\n",
    "        \"time_varying_unknown_categoricals\",\n",
    "        \"static_categoricals\",\n",
    "        \"known_reals\",\n",
    "        \"unknown_reals\",\n",
    "        \"known_categoricals\",\n",
    "        \"unknown_categoricals\",\n",
    "    ]:\n",
    "        if key in feature_lists:\n",
    "            tds_kwargs[key] = list(feature_lists[key])\n",
    "\n",
    "    dataset = TimeSeriesDataSet(synth_df_base.copy(), **tds_kwargs)\n",
    "\n",
    "    # Проверяем порядок reals\n",
    "    saved_reals = feature_lists.get(\"reals\")\n",
    "    rebuild_forced = False\n",
    "    if saved_reals is not None:\n",
    "        # В PF список dataset.reals может быть кортежами/объектами -- приводим к строкам\n",
    "        ds_reals = list(getattr(dataset, \"reals\", []))\n",
    "        ds_reals = [str(x) for x in ds_reals]\n",
    "        saved_reals_str = [str(x) for x in saved_reals]\n",
    "        if ds_reals != saved_reals_str:\n",
    "            rebuild_forced = True\n",
    "\n",
    "    if rebuild_forced:\n",
    "        # Попытка №2: Форсируем порядок каналов reals.\n",
    "        # Для этого отключим автоматические добавления и сами сгенерируем служебные колонки\n",
    "        # encoder_length и relative_time_idx в synth_df.\n",
    "        synth_df = synth_df_base.copy()\n",
    "        # relative_time_idx: от 0 до rows-1\n",
    "        synth_df[\"relative_time_idx\"] = np.arange(rows, dtype=np.int64)\n",
    "        # encoder_length: длина encoder для каждой позиции; для синтетики можно установить константу max_enc\n",
    "        # PF ожидает целочисленную encoder_length, соответствующую длине энкодера на каждом шаге.\n",
    "        # Для инициализации архитектуры достаточно положить валидные числа.\n",
    "        enc_len = np.zeros(rows, dtype=np.int64)\n",
    "        enc_len[:max_enc] = np.arange(1, max_enc + 1, dtype=np.int64)\n",
    "        enc_len[max_enc:] = max_enc\n",
    "        synth_df[\"encoder_length\"] = enc_len\n",
    "\n",
    "        # Теперь задаём TDS без add_* фичей, и передаём time_varying_unknown_reals в порядке saved_reals.\n",
    "        # saved_reals начинается с [\"encoder_length\",\"relative_time_idx\", ... f0..fN]\n",
    "        tds_kwargs_forced = dict(\n",
    "            time_idx=tds_params[\"time_idx\"],\n",
    "            target=tds_params[\"target\"],\n",
    "            group_ids=tds_params[\"group_ids\"],\n",
    "            max_encoder_length=tds_params[\"max_encoder_length\"],\n",
    "            max_prediction_length=tds_params[\"max_prediction_length\"],\n",
    "            target_normalizer=None,\n",
    "            allow_missing_timesteps=tds_params[\"allow_missing_timesteps\"],\n",
    "            add_relative_time_idx=False,\n",
    "            add_target_scales=tds_params[\"add_target_scales\"],\n",
    "            add_encoder_length=False,\n",
    "            min_prediction_length=tds_params[\"min_prediction_length\"],\n",
    "            min_encoder_length=tds_params.get(\"min_encoder_length\", 0),\n",
    "            time_varying_unknown_reals=list(saved_reals),  # порядок каналов фиксируем здесь\n",
    "            # Не задаём known/unknown cats/reals дополнительно, чтобы не нарушить порядок\n",
    "        )\n",
    "        dataset = TimeSeriesDataSet(synth_df, **tds_kwargs_forced)\n",
    "\n",
    "        # Контроль: проверим снова порядок\n",
    "        ds_reals2 = [str(x) for x in list(getattr(dataset, \"reals\", []))]\n",
    "        if ds_reals2 != [str(x) for x in saved_reals]:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to force reals order. Got {ds_reals2[:8]}..., expected {list(saved_reals)[:8]}...\"\n",
    "            )\n",
    "\n",
    "    # 5) TFT 1-в-1\n",
    "    tft = CustomTFT.from_dataset(\n",
    "        dataset,\n",
    "        hidden_size=int(model.hidden_size),\n",
    "        output_size=int(params.get(\"output_size\", 1)),\n",
    "        loss=PeakFriendlyHuber(\n",
    "            delta=0.5,\n",
    "            peak_thr=0.85,\n",
    "            peak_weight=1.3,\n",
    "            contrast_weight=0.03,\n",
    "            center_band=0.3,\n",
    "            clip_scale=1.5,\n",
    "        ),\n",
    "        optimizer=\"adam\",\n",
    "        learning_rate=float(model.learning_rate),\n",
    "        lstm_layers=int(params.get(\"lstm_layers\", 3)),\n",
    "        hidden_continuous_size=int(model.hidden_continuous_size),\n",
    "        attention_head_size=int(params.get(\"attention_head_size\", 4)),\n",
    "        dropout=float(model._dropout),\n",
    "        reduce_on_plateau_patience=5,\n",
    "        reduce_on_plateau_min_lr=1e-6,\n",
    "        weight_decay=float(model.weight_decay),\n",
    "        mask_prob=float(model.mask_prob),\n",
    "        output_transformer=tft_output_transformer,\n",
    "    )\n",
    "\n",
    "    model._model = TFTAdapter(tft)\n",
    "    model._train_dataset = dataset\n",
    "\n",
    "    # 6) загрузка весов\n",
    "    weights_zip_path = os.path.join(path, model_zip_filename)\n",
    "    with zipfile.ZipFile(weights_zip_path, mode='r') as zf:\n",
    "        if \"tft_state_dict.pt\" not in zf.namelist():\n",
    "            raise RuntimeError(\"tft_state_dict.pt not found in weights zip\")\n",
    "        state = torch.load(io.BytesIO(zf.read(\"tft_state_dict.pt\")), map_location=\"cpu\")\n",
    "\n",
    "    missing, unexpected = tft.load_state_dict(state, strict=False)\n",
    "    if missing or unexpected:\n",
    "        warnings.warn(f\"load_state_dict: missing={missing}, unexpected={unexpected}\")\n",
    "\n",
    "    # 7) eval + dropout off\n",
    "    tft.eval()\n",
    "    for m in tft.modules():\n",
    "        if isinstance(m, torch.nn.Dropout):\n",
    "            m.p = 0.0\n",
    "\n",
    "    model._model.to(device)\n",
    "\n",
    "    restored = Pipeline([\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\"to_dense\", FunctionTransformer(to_dense)),\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "    return restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea1b5ff-8944-47a1-8342-a21c285ad505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study_phase = optuna.create_study(study_name=f\\'feature_hyperparam_search_{ticker}\\', directions=[\\'minimize\\', \\'minimize\\', \\'minimize\\', \\'maximize\\', \\'minimize\\'], storage=db_path_phase, load_if_exists=True)\\ndirections = [\\'minimize\\', \\'minimize\\', \\'minimize\\', \\'maximize\\', \\'minimize\\']\\n\\nbest_score, best_num, best_params, best_values, best_norm =     find_best_trial_equal_importance(study_phase.trials, directions)\\ncombined_ml_df, timings = calculate_indicators(df_phase, final_cols[ticker], params=build_feature_params(final_params[ticker]), multy=False)\\nTGT_COL = [\\'normalized_target\\']\\nunique_batches = combined_ml_df[\\'batch\\'].unique()\\ncolumns_for_model = final_cols[ticker] + TGT_COL + [\\'batch\\', \\'regime\\'] + [\\'time\\']\\ncombined_ml_df[\\'predd\\'] = np.nan\\nfor i in tqdm(combined_ml_df.batch.unique()):\\n    index = combined_ml_df[combined_ml_df[\\'batch\\']==i].index\\n    gh_test = combined_ml_df[columns_for_model][combined_ml_df[columns_for_model][\\'batch\\'] == i]\\n    X_test, _, _, _, _ = prepare_data_transformer(gh_test, \\'normalized_target\\')\\n    \\n    # Локальный перехват предупреждений от pipeline\\n    with warnings.catch_warnings(record=True) as caught_warnings:\\n        warnings.simplefilter(\"always\")  # Чтобы catch_warnings перехватывал их\\n        combined_ml_df.loc[index, \\'predd\\'] = pipeline_trans.predict(X_test)\\n    \\n    # Игнорируем только релевантные предупреждения (опционально, для чистоты)\\n    for warning in caught_warnings:\\n        if \"Pipeline instance is not fitted yet\" in str(warning.message):\\n            continue  # Просто пропускаем (не показываем)\\n    \\n    # Остальной код (shift, pct_change и т.д.) без изменений\\n    combined_ml_df.loc[index, \\'predd_shift_5\\'] = combined_ml_df.loc[index][\\'predd\\'].shift(5)\\n    combined_ml_df.loc[index, \\'predd_pct\\'] = combined_ml_df.loc[index][\\'predd\\'].pct_change(3)\\n    combined_ml_df.loc[index, \\'predd_var\\'] = combined_ml_df.loc[index][\\'predd\\'].rolling(10).var()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker = 'OZPH'\n",
    "\n",
    "with open('final_cols.pkl', 'rb') as f:\n",
    "    final_cols = pickle.load(f)\n",
    "\n",
    "with open('final_params.pkl', 'rb') as f:\n",
    "    final_params = pickle.load(f)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DataLoader will create.*\")\n",
    "    \n",
    "path = r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\data_long' #test_files_15_2  data_long\n",
    "file_name = [i for i in os.listdir(r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\data_long') if ticker == i.split('_')[0]][0]\n",
    "with open('phase_ful_tickers_params.txt', 'r') as file:\n",
    "    phase_df = json.load(file)\n",
    "df_phase = pd.read_parquet(os.path.join(path, file_name))\n",
    "window = int(phase_df[ticker]['params']['moving_average_length']*9.5)\n",
    "features = extract_features(df_phase, window=window)\n",
    "scaled = joblib.load(\"scaler_global.pkl\").transform(features)\n",
    "labels = joblib.load(\"kmeans_global.pkl\").predict(scaled)\n",
    "\n",
    "regime_series = pd.Series(labels, index=df_phase.index)\n",
    "window_size = int(phase_df[ticker]['params']['atr_period']*5.5)\n",
    "\n",
    "smoother = FastRollingMode(window_size=window_size)\n",
    "smoothed = [smoother.update(x) for x in labels]\n",
    "smoothed_regime = pd.Series(smoothed, index=df_phase.index)\n",
    "\n",
    "regime_params = prepare_regime_params(phase_df[ticker]['params'])\n",
    "\n",
    "CV = AdaptiveTradingSystem(regime_params['base_params'])\n",
    "\n",
    "pipeline_trans = load_transformer_exact(f'C:/Users/aleksandrovva1/Desktop/data science/0-trade/t/transformers/{ticker}', device='cpu')    \n",
    "\n",
    "df_phase = CV.generate_adaptive_signals(df_phase, regime_series=smoothed_regime)\n",
    "\n",
    "buy_signals = df_phase[df_phase['buy_signal']]\n",
    "sell_signals = df_phase[df_phase['sell_signal']]\n",
    "for _, buy in buy_signals.iterrows():\n",
    "    sell = sell_signals[sell_signals.time > buy.time].head(1)\n",
    "    if not sell.empty:\n",
    "        df_phase.loc[buy.name, \"event_time\"] = buy.time\n",
    "        df_phase.loc[buy.name, \"event_price\"] = buy.close\n",
    "        df_phase.loc[buy.name, \"event_sell_time\"] = sell.time.values[0]\n",
    "        df_phase.loc[buy.name, \"event_sell_price\"] = sell.close.values[0]\n",
    "\n",
    "df_phase['pnl'] = ((df_phase['event_sell_price'] * (1 - 0.003)) / (df_phase['event_price'] * (1 + 0.003)) - 1) * 100\n",
    "df_phase['regime'] = smoothed_regime\n",
    "df_phase = calculate_target(df_phase, threshold=1.9)\n",
    "df_phase = calculate_smoothed_target_qnorm(df_phase, \n",
    "                                       smooth_method='whittaker', whittaker_lambda=10, savgol_window=15, savgol_poly=3, \n",
    "                                       per_batch_equalize=True, per_batch_q=0.01\n",
    "                                      )\n",
    "db_path_phase = f\"sqlite:///C:/Users/aleksandrovva1/Desktop/data science/0-trade/t/out_try/{ticker}.db\"\n",
    "'''study_phase = optuna.create_study(study_name=f'feature_hyperparam_search_{ticker}', directions=['minimize', 'minimize', 'minimize', 'maximize', 'minimize'], storage=db_path_phase, load_if_exists=True)\n",
    "directions = ['minimize', 'minimize', 'minimize', 'maximize', 'minimize']\n",
    "\n",
    "best_score, best_num, best_params, best_values, best_norm = \\\n",
    "    find_best_trial_equal_importance(study_phase.trials, directions)\n",
    "combined_ml_df, timings = calculate_indicators(df_phase, final_cols[ticker], params=build_feature_params(final_params[ticker]), multy=False)\n",
    "TGT_COL = ['normalized_target']\n",
    "unique_batches = combined_ml_df['batch'].unique()\n",
    "columns_for_model = final_cols[ticker] + TGT_COL + ['batch', 'regime'] + ['time']\n",
    "combined_ml_df['predd'] = np.nan\n",
    "for i in tqdm(combined_ml_df.batch.unique()):\n",
    "    index = combined_ml_df[combined_ml_df['batch']==i].index\n",
    "    gh_test = combined_ml_df[columns_for_model][combined_ml_df[columns_for_model]['batch'] == i]\n",
    "    X_test, _, _, _, _ = prepare_data_transformer(gh_test, 'normalized_target')\n",
    "    \n",
    "    # Локальный перехват предупреждений от pipeline\n",
    "    with warnings.catch_warnings(record=True) as caught_warnings:\n",
    "        warnings.simplefilter(\"always\")  # Чтобы catch_warnings перехватывал их\n",
    "        combined_ml_df.loc[index, 'predd'] = pipeline_trans.predict(X_test)\n",
    "    \n",
    "    # Игнорируем только релевантные предупреждения (опционально, для чистоты)\n",
    "    for warning in caught_warnings:\n",
    "        if \"Pipeline instance is not fitted yet\" in str(warning.message):\n",
    "            continue  # Просто пропускаем (не показываем)\n",
    "    \n",
    "    # Остальной код (shift, pct_change и т.д.) без изменений\n",
    "    combined_ml_df.loc[index, 'predd_shift_5'] = combined_ml_df.loc[index]['predd'].shift(5)\n",
    "    combined_ml_df.loc[index, 'predd_pct'] = combined_ml_df.loc[index]['predd'].pct_change(3)\n",
    "    combined_ml_df.loc[index, 'predd_var'] = combined_ml_df.loc[index]['predd'].rolling(10).var()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f855bf-924b-46a1-af6b-70a9a5e15022",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_cols.pkl', 'rb') as f:\n",
    "    final_cols = pickle.load(f)\n",
    "\n",
    "with open('final_params.pkl', 'rb') as f:\n",
    "    final_params = pickle.load(f)\n",
    "\n",
    "def optimize_for_ticker(df_path, db_path, study_name, ticker):\n",
    "  \n",
    "    path = r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\data_long' #test_files_15_2  data_long\n",
    "    with open('phase_ful_tickers_params.txt', 'r') as file:\n",
    "        phase_df = json.load(file)\n",
    "    df_phase = pd.read_parquet(os.path.join(path, df_path))\n",
    "    window = int(phase_df[ticker]['params']['moving_average_length']*9.5)\n",
    "    features = extract_features(df_phase, window=window)\n",
    "    scaled = joblib.load(\"scaler_global.pkl\").transform(features)\n",
    "    labels = joblib.load(\"kmeans_global.pkl\").predict(scaled)\n",
    "    \n",
    "    regime_series = pd.Series(labels, index=df_phase.index)\n",
    "    window_size = int(phase_df[ticker]['params']['atr_period']*5.5)\n",
    "    \n",
    "    smoother = FastRollingMode(window_size=window_size)\n",
    "    smoothed = [smoother.update(x) for x in labels]\n",
    "    smoothed_regime = pd.Series(smoothed, index=df_phase.index)\n",
    "    \n",
    "    regime_params = prepare_regime_params(phase_df[ticker]['params'])\n",
    "    \n",
    "    CV = AdaptiveTradingSystem(regime_params['base_params'])\n",
    "\n",
    "    pipeline_trans = load_transformer_exact(f'C:/Users/aleksandrovva1/Desktop/data science/0-trade/t/transformers/{ticker}', device='cpu')    \n",
    "    \n",
    "    df_phase = CV.generate_adaptive_signals(df_phase, regime_series=smoothed_regime)\n",
    "    \n",
    "    buy_signals = df_phase[df_phase['buy_signal']]\n",
    "    sell_signals = df_phase[df_phase['sell_signal']]\n",
    "    for _, buy in buy_signals.iterrows():\n",
    "        sell = sell_signals[sell_signals.time > buy.time].head(1)\n",
    "        if not sell.empty:\n",
    "            df_phase.loc[buy.name, \"event_time\"] = buy.time\n",
    "            df_phase.loc[buy.name, \"event_price\"] = buy.close\n",
    "            df_phase.loc[buy.name, \"event_sell_time\"] = sell.time.values[0]\n",
    "            df_phase.loc[buy.name, \"event_sell_price\"] = sell.close.values[0]\n",
    "    \n",
    "    df_phase['pnl'] = ((df_phase['event_sell_price'] * (1 - 0.003)) / (df_phase['event_price'] * (1 + 0.003)) - 1) * 100\n",
    "    df_phase['regime'] = smoothed_regime\n",
    "    df_phase = calculate_target(df_phase, threshold=1.9)\n",
    "    df_phase = calculate_smoothed_target_qnorm(df_phase, \n",
    "                                           smooth_method='whittaker', whittaker_lambda=10, savgol_window=15, savgol_poly=3, \n",
    "                                           per_batch_equalize=True, per_batch_q=0.01\n",
    "                                          )\n",
    "    db_path_phase = f\"sqlite:///C:/Users/aleksandrovva1/Desktop/data science/0-trade/t/out_try/{ticker}.db\"\n",
    "    study_phase = optuna.create_study(study_name=f'feature_hyperparam_search_{ticker}', directions=['minimize', 'minimize', 'minimize', 'maximize', 'minimize'], storage=db_path_phase, load_if_exists=True)\n",
    "    directions = ['minimize', 'minimize', 'minimize', 'maximize', 'minimize']\n",
    "\n",
    "    best_score, best_num, best_params, best_values, best_norm = \\\n",
    "        find_best_trial_equal_importance(study_phase.trials, directions)\n",
    "    combined_ml_df, timings = calculate_indicators(df_phase, final_cols[ticker], params=build_feature_params(final_params[ticker]), multy=False)\n",
    "\n",
    "    TGT_COL = ['normalized_target']\n",
    "    unique_batches = combined_ml_df['batch'].unique()\n",
    "    columns_for_model = final_cols[ticker] + TGT_COL + ['batch', 'regime'] + ['time']\n",
    "\n",
    "    if combined_ml_df.isna().sum().any() == True:\n",
    "        nan_batch = combined_ml_df[combined_ml_df.isna().any(axis=1)]['batch'].unique()\n",
    "        combined_ml_df = combined_ml_df[~combined_ml_df['batch'].isin(nan_batch)]\n",
    "    combined_ml_df['predd'] = np.nan\n",
    "            \n",
    "    for i in combined_ml_df.batch.unique():\n",
    "        index = combined_ml_df[combined_ml_df['batch']==i].index\n",
    "        gh_test = combined_ml_df[columns_for_model][combined_ml_df[columns_for_model]['batch'] == i]\n",
    "        X_test, _, _, _, _ = prepare_data_transformer(gh_test, 'normalized_target')\n",
    "        \n",
    "        # Локальный перехват предупреждений от pipeline\n",
    "        with warnings.catch_warnings(record=True) as caught_warnings:\n",
    "            warnings.simplefilter(\"always\")  # Чтобы catch_warnings перехватывал их\n",
    "            combined_ml_df.loc[index, 'predd'] = pipeline_trans.predict(X_test)\n",
    "        \n",
    "        # Игнорируем только релевантные предупреждения (опционально, для чистоты)\n",
    "        for warning in caught_warnings:\n",
    "            if \"Pipeline instance is not fitted yet\" in str(warning.message):\n",
    "                continue  # Просто пропускаем (не показываем)\n",
    "        \n",
    "        # Остальной код (shift, pct_change и т.д.) без изменений\n",
    "        combined_ml_df.loc[index, 'predd_shift_5'] = combined_ml_df.loc[index]['predd'].shift(5)\n",
    "        combined_ml_df.loc[index, 'predd_pct'] = combined_ml_df.loc[index]['predd'].pct_change(3)\n",
    "        combined_ml_df.loc[index, 'predd_var'] = combined_ml_df.loc[index]['predd'].rolling(10).var()\n",
    "\n",
    "    def make_batch_bins(df, target_col, batch_col, K):\n",
    "        \"\"\"\n",
    "        Для каждого batch отдельно строим K бинов от min до max и возвращаем Series[0..K-1].\n",
    "        \"\"\"\n",
    "        def bin_series(s):\n",
    "            if s.max()==s.min():\n",
    "                return pd.Series(np.zeros(len(s),dtype=int), index=s.index)\n",
    "            # K равных интервалов между [min,max]\n",
    "            bins = np.linspace(s.min(), s.max(), K+1)\n",
    "            return pd.cut(s, bins=bins, labels=False, include_lowest=True)\n",
    "        return df.groupby(batch_col)[target_col].apply(bin_series)\n",
    "    \n",
    "    def make_group(batches):\n",
    "        vals, idx = np.unique(batches, return_index=True)\n",
    "        order = np.argsort(idx)\n",
    "        vals  = vals[order]\n",
    "        return batches.value_counts().reindex(vals).to_numpy()\n",
    "    \n",
    "    def pred_converter(y_vl_bins):\n",
    "        indices = y_vl_bins.index.get_level_values(1).values\n",
    "        values = y_vl_bins.values\n",
    "        return pd.Series(values, index=indices)\n",
    "    \n",
    "    def objective(trial):\n",
    "        K = 15  # Фиксированное K для бинов\n",
    "    \n",
    "        # Гиперпараметры для LGBMRanker\n",
    "        params = {\n",
    "            'objective': 'lambdarank',\n",
    "            'metric': 'ndcg',\n",
    "            'ndcg_eval_at': [3],\n",
    "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 3000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 13),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 1000),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('lgbm_reg_alpha', 0.0, 10.0),\n",
    "            'reg_lambda': trial.suggest_float('lgbm_reg_lambda', 0.0, 10.0),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 6000),\n",
    "            'random_state': 42,\n",
    "            'verbosity': -1\n",
    "        }\n",
    "    \n",
    "        # Кросс-валидация\n",
    "        ndcg_scores, r2_scores, corr_scores = [], [], []\n",
    "    \n",
    "        TGT_COLS = ['normalized_target']\n",
    "        unique_batches = combined_ml_df['batch'].unique()\n",
    "        columns_for_model = final_cols[ticker] + TGT_COLS + ['batch', 'regime', 'predd', 'predd_var', 'predd_shift_5', 'predd_pct']\n",
    "        combined_ml_opt = combined_ml_df[columns_for_model].copy()\n",
    "    \n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        for train_idx, test_idx in kf.split(combined_ml_df['batch'].unique()):\n",
    "            train_batches = combined_ml_opt['batch'].unique()[train_idx]\n",
    "            test_batches = combined_ml_opt['batch'].unique()[test_idx]\n",
    "    \n",
    "            train_data = combined_ml_opt[combined_ml_opt['batch'].isin(train_batches)].reset_index(drop=True)\n",
    "            test_data = combined_ml_opt[combined_ml_opt['batch'].isin(test_batches)].reset_index(drop=True)\n",
    "    \n",
    "            # Подготовка данных\n",
    "            X_train, y_train, preprocessing = prepare_data(train_data, 'normalized_target')\n",
    "            X_test, y_test, _ = prepare_data(test_data, 'normalized_target')\n",
    "    \n",
    "            # Бины для ранжирования\n",
    "            y_tr_bins = make_batch_bins(train_data, 'normalized_target', 'batch', K)\n",
    "            y_vl_bins = make_batch_bins(test_data, 'normalized_target', 'batch', K)\n",
    "    \n",
    "            # Группы для LGBMRanker\n",
    "            grp_tr = make_group(train_data['batch'])\n",
    "            grp_vl = make_group(test_data['batch'])\n",
    "    \n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessing', preprocessing),\n",
    "                ('to_dense', FunctionTransformer(to_dense)),  # Преобразование в плотную матрицу\n",
    "                ('model', LGBMRanker(**params))\n",
    "            ])\n",
    "    \n",
    "            # Предварительно трансформируем X_train и X_test\n",
    "            X_tr_t = pipeline.named_steps['to_dense'].transform(pipeline.named_steps['preprocessing'].fit_transform(X_train, y_tr_bins))\n",
    "            X_vl_t = pipeline.named_steps['to_dense'].transform(pipeline.named_steps['preprocessing'].transform(X_test))\n",
    "    \n",
    "            # Фитим ranker вручную\n",
    "            pipeline.named_steps['model'].fit(\n",
    "                X_tr_t, y_tr_bins,\n",
    "                group=grp_tr,\n",
    "                eval_set=[(X_vl_t, y_vl_bins)],\n",
    "                eval_group=[grp_vl],\n",
    "                eval_at=[3],\n",
    "            )\n",
    "    \n",
    "            # Предсказание\n",
    "            y_pred_val = pipeline.named_steps['model'].predict(X_vl_t)\n",
    "            y_pred_s = pd.Series(y_pred_val, index=test_data.index)\n",
    "    \n",
    "            # Вычисление NDCG@3 по батчам\n",
    "            ndcgs = []\n",
    "            for b in test_batches:\n",
    "                mask = (test_data['batch'] == b)\n",
    "                true_labels = pred_converter(y_vl_bins)[mask].to_numpy()\n",
    "                preds = y_pred_s[mask].to_numpy()\n",
    "                k = min(3, len(preds))\n",
    "                #print(preds, k)\n",
    "                \n",
    "                if k >= 1:  # Add this check to skip empty batches\n",
    "                    ndcgs.append(ndcg_score([true_labels], [preds], k=k))\n",
    "                else:\n",
    "                    # Optional: Append a default value like 0 if you want to penalize empty batches explicitly\n",
    "                    # ndcgs.append(0.0)\n",
    "                    # Or log a warning: print(f\"Skipping empty batch {b} with {len(preds)} points\")\n",
    "                    pass  # Skipping is fine; average will ignore it\n",
    "            \n",
    "            avg_ndcg = np.mean(ndcgs) if ndcgs else 0.0  # Fallback to 0 if all batches are empty (or keep your inf logic)\n",
    "    \n",
    "            # Для сравнения: R2 и корреляция на bin-labels vs scores\n",
    "            true_bins_all = pred_converter(y_vl_bins).to_numpy()\n",
    "            r2_scores.append(r2_score(true_bins_all, y_pred_val))\n",
    "            corr = pearsonr(true_bins_all, y_pred_val)[0]\n",
    "            corr_scores.append(corr if not np.isnan(corr) else 0.0)\n",
    "    \n",
    "            ndcg_scores.append(avg_ndcg)\n",
    "    \n",
    "        if not ndcg_scores:\n",
    "            return float('inf'), float('inf'), float('inf'), float('inf'), float('inf')\n",
    "    \n",
    "        if corr_scores == []:\n",
    "            return float('inf'), float('inf'), float('inf'), float('inf'), float('inf')\n",
    "    \n",
    "        avg_ndcg = float(np.mean(ndcg_scores))\n",
    "        avg_r2 = float(np.mean(r2_scores))      # we want it *higher*\n",
    "        std_r2 = float(np.std(r2_scores))\n",
    "        corr_mean = float(np.mean(corr_scores))\n",
    "        corr_std = float(np.std(corr_scores))\n",
    "    \n",
    "        if np.isfinite([avg_ndcg, avg_r2, std_r2, corr_mean, corr_std]).all() == True:\n",
    "            return -avg_ndcg, -avg_r2, std_r2, corr_mean, corr_std  # Primary: maximize NDCG (negative for minimize)\n",
    "        else:\n",
    "            return float('inf'), float('inf'), float('inf'), float('inf'), float('inf')\n",
    "\n",
    "          \n",
    "    study = optuna.create_study(\n",
    "          study_name=study_name,\n",
    "          directions=['minimize', 'minimize', 'minimize', 'maximize', 'minimize'],\n",
    "          storage=db_path,\n",
    "          load_if_exists=True\n",
    "      )\n",
    "    study.optimize(\n",
    "          objective,\n",
    "          n_trials=200 - len([trial for trial in study.trials if trial.values is not None]),\n",
    "          n_jobs=3,\n",
    "          show_progress_bar=False\n",
    "      )\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2244c8a3-350b-4b6f-824b-dc7a8f0035f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_index = list(phase_df.keys())\n",
    "canceled_keys = [i for i in filtered_index if i not in [f.split('.')[0] for f in os.listdir(r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\lgbmranker_params')]]\n",
    "\n",
    "# Преобразуем в список и перемешиваем для более равномерного распределения\n",
    "tickers_list = np.random.permutation(canceled_keys)\n",
    "\n",
    "# Разделяем на 3 части\n",
    "split_indices = np.array_split(tickers_list, 4)\n",
    "\n",
    "# Получаем три группы\n",
    "group1 = split_indices[0].tolist()\n",
    "group2 = split_indices[1].tolist()\n",
    "group3 = split_indices[2].tolist()\n",
    "group4 = split_indices[3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d958570-df38-4448-9077-0860c6e98f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f8934dea0646a3919a6ea77357ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1c687b9f8541ba8ff0568608eecb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-01 14:07:53,111] Using an existing study with name 'feature_hyperparam_search_IRKT' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "group1 = ['VKCO',\n",
    " 'LNZL',\n",
    " 'NMTP',\n",
    " 'SVAV',\n",
    " #'CNTLP',\n",
    " 'MRKC',\n",
    " #'FESH',\n",
    " #'BANEP',\n",
    " 'RBCM',\n",
    " 'RENI',\n",
    " 'IRKT',\n",
    " 'PRFN',\n",
    " 'PLZL',\n",
    " 'TGKB',\n",
    " #'HEAD',\n",
    " 'X5',\n",
    " 'NTZL',\n",
    " 'MAGN',\n",
    " 'KZIZ',\n",
    " 'KLSB',\n",
    " 'PMSBP',\n",
    " 'MDMG']\n",
    "\n",
    "tickers = []\n",
    "for i in tqdm([i for i in os.listdir(r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\data_long')]): #test_files_15_2  data_long\n",
    "    if i.split('_')[0] in group1:\n",
    "        tickers.append((i.replace(\"'\", \"\").split('_')[0], i.replace(\"'\", \"\").replace(\",\", \"\"), i.replace(\"'\", \"\").split('_')[0]+'.db'))\n",
    "for ticks in tqdm(tickers):\n",
    "    db_path = f\"sqlite:///C:/Users/aleksandrovva1/Desktop/data science/0-trade/t/lgbmranker_params/{ticks[2]}\"\n",
    "    study_name=f'feature_hyperparam_search_{ticks[0]}'\n",
    "    optimize_for_ticker(ticks[1], db_path, study_name, ticks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65ba158-5b5a-469e-a302-7c08d09b90fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IRKT', 'IRKT_industrials_CANDLE_INTERVAL_15_MIN.pq', 'IRKT.db')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "263698d5-638a-4860-b6c9-c1ce72b58f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNGSP.db\n",
      "SPBE.db\n",
      "TATN.db\n",
      "TGKBP.db\n",
      "TGKJ.db\n",
      "TGKN.db\n",
      "TRMK.db\n",
      "TRNFP.db\n",
      "TTLK.db\n",
      "UGLD.db\n",
      "UNAC.db\n",
      "UPRO.db\n",
      "UWGN.db\n",
      "VRSB.db\n",
      "YDEX.db\n"
     ]
    }
   ],
   "source": [
    "group1 = ['NVTK',\n",
    " 'TATN',\n",
    " 'BSPB',\n",
    " 'MRKU',\n",
    " 'SELG',\n",
    " 'OZPH',\n",
    " 'ETLN',\n",
    " 'LKOH',\n",
    " 'MGTSP',\n",
    " 'FRHC',\n",
    " 'UWGN',\n",
    " 'ABRD',\n",
    " 'RKKE',\n",
    " 'AFKS',\n",
    " #'OBNEP',\n",
    " 'YDEX',\n",
    " 'CNRU',\n",
    " 'SBER',\n",
    " 'UGLD',\n",
    " 'KROT',\n",
    " 'PMSB',\n",
    " 'TGKBP',\n",
    " 'LSRG']\n",
    "\n",
    "group2 = ['NLMK',\n",
    " #'GECO',\n",
    " 'MGNT',\n",
    " 'NKHP',\n",
    " 'SBERP',\n",
    " 'RNFT',\n",
    " 'LNZLP',\n",
    " 'LSNG',\n",
    " #'KZIZP',\n",
    " 'OBNE',\n",
    " 'TTLK',\n",
    " 'TGKJ',\n",
    " 'BANE',\n",
    " 'SNGSP',\n",
    " 'LSNGP',\n",
    " #'MRKZ',\n",
    " 'VRSB',\n",
    " 'MRKV',\n",
    " 'SPBE',\n",
    " 'TRNFP',\n",
    " 'MSTT',\n",
    " 'DVEC']\n",
    "\n",
    "group3 = ['OKEY',\n",
    " 'MTLRP',\n",
    " 'KZOSP',\n",
    " 'LIFE',\n",
    " 'UNAC',\n",
    " 'UPRO',\n",
    " #'FLOT',\n",
    " #'GEMC',\n",
    " 'MRKP',\n",
    " 'TGKN',\n",
    " 'NOMP',\n",
    " 'NSVZ',\n",
    " 'RAGR',\n",
    " 'LENT',\n",
    " 'TATNP',\n",
    " 'AFLT',\n",
    " 'TRMK',\n",
    " 'ROSN',\n",
    " 'MTLR',\n",
    " 'MOEX',\n",
    " #'GTRK',\n",
    " 'CHMK']\n",
    "\n",
    "grop = group1 + group2 + group3\n",
    "\n",
    "\n",
    "for i in os.listdir(r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\lgbmregressor_params'):\n",
    "    \n",
    "    if i in os.listdir(r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\lgbmranker_params'):\n",
    "        pass\n",
    "    elif i.split('.')[0] in grop:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8774d-2d4a-4741-babf-fd239c055581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def prepare_data(df, target_col):\n",
    "    \"\"\"\n",
    "    Подготавливает данные: разделяет на числовые и категориальные признаки, создает конвейер преобразования.\n",
    "    \"\"\"\n",
    "    if type(target_col) == str:\n",
    "        df.dropna(inplace=True)\n",
    "        X = df.drop([target_col, 'batch'], axis=1)\n",
    "        y = df[target_col]\n",
    "    elif type(target_col) == list:\n",
    "        df.dropna(inplace=True)\n",
    "        X = df.drop(target_col+['batch'], axis=1)\n",
    "        y = df[target_col]\n",
    "\n",
    "    # Разделение на числовые и категориальные признаки\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Создание конвейера преобразования\n",
    "    preprocessing = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('scaler', RobustScaler()),\n",
    "                ('normalize', PowerTransformer(method='yeo-johnson')),\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "            ]), categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return X, y, preprocessing\n",
    "#TGT_COLS = ['normalized_target', 'multi_target_5', 'multi_target_10', 'multi_target_20']\n",
    "TGT_COLS = ['normalized_target']\n",
    "unique_batches = combined_ml_df['batch'].unique()\n",
    "columns_for_model = final_cols[ticker] + TGT_COLS + ['batch', 'regime', 'predd', 'predd_var', 'predd_shift_5', 'predd_pct']\n",
    "gh_t = combined_ml_df[columns_for_model].copy()\n",
    "\n",
    "mse_scores, r2_scores, corr_scores = [], [], []\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "if len(TGT_COLS)!=1:\n",
    "    for train_idx, test_idx in kf.split(gh_t['batch'].unique()):\n",
    "        train_batches = gh_t['batch'].unique()[train_idx]\n",
    "        test_batches = gh_t['batch'].unique()[test_idx]\n",
    "    \n",
    "        train_data = gh_t[gh_t['batch'].isin(train_batches)]\n",
    "        test_data = gh_t[gh_t['batch'].isin(test_batches)]\n",
    "    \n",
    "        # Подготовка данных (как в вашем коде)\n",
    "        X_train, y_train, preprocessing = prepare_data(train_data, TGT_COLS)\n",
    "        X_test, y_test, _ = prepare_data(test_data, TGT_COLS)\n",
    "    \n",
    "        # Определение модели (как в вашем коде)\n",
    "        base_model = LGBMRegressor(**study_model_base.trials[best_num_model].params, random_state=42, verbosity=-1)  # objective='quantile', alpha=0.25 если нужно\n",
    "        #multi_model = MultiOutputRegressor(base_model, n_jobs=-1)\n",
    "    \n",
    "        # Пайплайн (как в вашем коде)\n",
    "        pipeline_reg = Pipeline([\n",
    "            ('preprocessing', preprocessing),\n",
    "            ('to_dense', FunctionTransformer(to_dense)),  # Преобразование в плотную матрицу\n",
    "            ('model', base_model)\n",
    "        ])\n",
    "    \n",
    "        # Обучение\n",
    "        pipeline_reg.fit(X_train, y_train)\n",
    "    \n",
    "        # Предсказания: y_pred — (n_samples, n_targets)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "        # Вычисление метрик (MSE и R2 — как в вашем коде, средние по горизонтам)\n",
    "        mse_per_horizon = [mean_squared_error(y_test.to_numpy()[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n",
    "        r2_per_horizon = [r2_score(y_test.to_numpy()[:, i], y_pred[:, i]) for i in range(y_test.shape[1])]\n",
    "        mse_scores.append(np.mean(mse_per_horizon))\n",
    "        r2_scores.append(np.mean(r2_per_horizon))\n",
    "    \n",
    "        # Вычисление корреляции: per-horizon (между y_test[:,i] и y_pred[:,i])\n",
    "        corr_per_horizon = []\n",
    "        for i in range(y_test.shape[1]):\n",
    "            # Используем pearsonr для обработки NaN/infs; берем [0] для коэффициента\n",
    "            corr_i, _ = pearsonr(y_test.to_numpy()[:, i], y_pred[:, i])\n",
    "            corr_per_horizon.append(corr_i if not np.isnan(corr_i) else 0.0)  # Обработка NaN\n",
    "        corr_scores.append(np.mean(corr_per_horizon))  # Средняя корреляция по горизонтам\n",
    "    \n",
    "        # Опционально: Аггрегированная корреляция (среднее предсказание vs original normalized_target)\n",
    "        # Это для вашего \"corr\" в коде: коррелируем mean(y_pred, axis=1) с 'normalized_target'\n",
    "        y_pred_mean = np.mean(y_pred, axis=1)\n",
    "        original_target = df_phase.loc[test_data.index, 'normalized_target']  # Или test_data['normalized_target'] если доступно\n",
    "        corr_mean_pred, _ = pearsonr(original_target, y_pred_mean)\n",
    "        print(f\"Fold corr_mean_pred (mean pred vs normalized_target): {corr_mean_pred}\")  # Для логирования, не добавляем в scores\n",
    "    \n",
    "        # Ваш закомментированный код для missed_pnl (можно раскомментировать и адаптировать)\n",
    "        # missed_pnl = []  # % missed from max\n",
    "        # for batch in test_data['batch'].unique():\n",
    "        #     mask = test_data['batch'] == batch\n",
    "        #     max_high = test_data.loc[mask, 'high'].max()\n",
    "        #     pred = y_pred[mask]  # (n_bars_in_batch, n_targets)\n",
    "        #     pred_mean_batch = np.mean(pred, axis=1)  # Среднее по горизонтам для сигнала\n",
    "        #     sell_idx = np.argmin(pred_mean_batch)  # sell at min pred (low confidence)\n",
    "        #     sell_price = test_data.loc[mask].iloc[sell_idx]['close']\n",
    "        #     missed = (max_high - sell_price) / (max_high - test_data.loc[mask].iloc[0]['close'])  # % missed\n",
    "        #     missed_pnl.append(missed)\n",
    "        # avg_missed = np.mean(missed_pnl)\n",
    "        # print(f\"Fold avg_missed: {avg_missed}\")\n",
    "    \n",
    "    # Финальные метрики (как в вашем коде)\n",
    "    avg_mse = float(np.mean(mse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))      # we want it *higher*\n",
    "    std_r2 = float(np.std(r2_scores))\n",
    "    corr_mean = float(np.mean(corr_scores))  # Теперь это средняя per-horizon corr\n",
    "    corr_std = float(np.std(corr_scores))\n",
    "else:\n",
    "    for train_idx, test_idx in kf.split(gh_t['batch'].unique()):\n",
    "        train_batches = gh_t['batch'].unique()[train_idx]\n",
    "        test_batches = gh_t['batch'].unique()[test_idx]\n",
    "    \n",
    "        train_data = gh_t[gh_t['batch'].isin(train_batches)]\n",
    "        test_data = gh_t[gh_t['batch'].isin(test_batches)]\n",
    "    \n",
    "        # Подготовка данных\n",
    "        X_train, y_train, preprocessing = prepare_data(train_data, 'normalized_target')\n",
    "        X_test, y_test, _ = prepare_data(test_data, 'normalized_target')\n",
    "    \n",
    "        pipeline_reg = Pipeline([\n",
    "                    ('preprocessing', preprocessing),\n",
    "                    ('to_dense', FunctionTransformer(to_dense)),  # Преобразование в плотную матрицу\n",
    "                    ('model', LGBMRegressor(max_depth = 13,\n",
    "                                            n_estimators = 140, \n",
    "                                            random_state=42, \n",
    "                                            verbosity=-1))#study_model.trials[best_num_model].params |\n",
    "                ])\n",
    "    \n",
    "        pipeline_reg.fit(X_train, y_train)\n",
    "        y_pred = pipeline_reg.predict(X_test)\n",
    "    \n",
    "        #missed_pnl = []  # % missed from max\n",
    "        #for batch in test_data['batch'].unique():\n",
    "        #    mask = test_data['batch'] == batch\n",
    "        #    max_high = test_data.loc[mask, 'high'].max()\n",
    "        #    pred = y_pred[mask]  # assume pred ~ normalized_target\n",
    "        #    sell_idx = np.argmin(pred)  # sell at min pred (low confidence)\n",
    "        #    sell_price = test_data.loc[mask].iloc[sell_idx]['close']\n",
    "        #    missed = (max_high - sell_price) / (max_high - test_data.loc[mask].iloc[0]['close'])  # % missed\n",
    "        #    missed_pnl.append(missed)\n",
    "        #avg_missed = np.mean(missed_pnl)\n",
    "        #corr_score = df_phase.loc[gh[gh['batch'].isin(test_batches)].index]['normalized_target'].corr(pd.Series(y_pred, index=X_test.index))\n",
    "        corr = df_phase.loc[test_data.index, 'normalized_target']\\\n",
    "                               .corr(pd.Series(y_pred, index=X_test.index))\n",
    "        mse_scores.append(mean_squared_error(y_test, y_pred))\n",
    "        r2_scores.append(r2_score(y_test, y_pred))\n",
    "        corr_scores.append(corr)\n",
    "    avg_mse = float(np.mean(mse_scores))\n",
    "    avg_r2 = float(np.mean(r2_scores))      # we want it *higher*\n",
    "    std_r2 = float(np.std(r2_scores))\n",
    "    corr_mean = float(np.mean(corr_scores))  # Теперь это средняя per-horizon corr\n",
    "    corr_std = float(np.std(corr_scores))\n",
    "\n",
    "print(f\"Avg MSE: {avg_mse}\")\n",
    "print(f\"Avg R2: {avg_r2}\")\n",
    "print(f\"Std R2: {std_r2}\")\n",
    "print(f\"Mean Corr (per-horizon): {corr_mean}\")\n",
    "print(f\"Std Corr: {corr_std}\")\n",
    "\n",
    "from lightgbm import LGBMRanker\n",
    "from sklearn.metrics import ndcg_score\n",
    "n_splits = 3\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "TGT_COLS = ['normalized_target']\n",
    "unique_batches = combined_ml_df['batch'].unique()\n",
    "columns_for_model = final_cols[ticker] + TGT_COLS + ['batch', 'regime', 'predd', 'predd_var', 'predd_shift_5', 'predd_pct']\n",
    "gh_t = combined_ml_df[columns_for_model].copy()\n",
    "\n",
    "ranker_params = {\n",
    "    'objective':'lambdarank',#lambdarank\n",
    "    'metric':'ndcg', #ndcg\n",
    "    'ndcg_eval_at':[3],\n",
    "    'learning_rate':0.15,\n",
    "    'num_leaves':128,\n",
    "    'n_estimators':1000,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.8,\n",
    "    'random_state':42\n",
    "}\n",
    "\n",
    "K = 15  \n",
    "global_y_min = gh_t['normalized_target'].min()\n",
    "global_y_max = gh_t['normalized_target'].max()\n",
    "\n",
    "mse_list, r2_list, corr_list, ndcg_list = [], [], [], []\n",
    "\n",
    "def make_batch_bins(df, target_col, batch_col, K):\n",
    "    \"\"\"\n",
    "    Для каждого batch отдельно строим K бинов от min до max и возвращаем Series[0..K-1].\n",
    "    \"\"\"\n",
    "    def bin_series(s):\n",
    "        if s.max()==s.min():\n",
    "            return pd.Series(np.zeros(len(s),dtype=int), index=s.index)\n",
    "        # K равных интервалов между [min,max]\n",
    "        bins = np.linspace(s.min(), s.max(), K+1)\n",
    "        return pd.cut(s, bins=bins, labels=False, include_lowest=True)\n",
    "    return df.groupby(batch_col)[target_col].apply(bin_series)\n",
    "\n",
    "def make_group(batches):\n",
    "        vals, idx = np.unique(batches, return_index=True)\n",
    "        order = np.argsort(idx)\n",
    "        vals  = vals[order]\n",
    "        return batches.value_counts().reindex(vals).to_numpy()\n",
    "\n",
    "# уникальные батчи\n",
    "all_batches = gh_t['batch'].unique()\n",
    "\n",
    "for train_idx, test_idx in kf.split(all_batches):\n",
    "    train_batches = gh_t['batch'].unique()[train_idx]\n",
    "    test_batches = gh_t['batch'].unique()[test_idx]\n",
    "    \n",
    "    train_df = gh_t[gh_t['batch'].isin(train_batches)].reset_index(drop=True)\n",
    "    val_df   = gh_t[gh_t['batch'].isin(test_batches)] .reset_index(drop=True)\n",
    "    \n",
    "    # готовим X,y и препроцессор\n",
    "    \n",
    "    X_tr, y_tr, preprocessing = prepare_data(train_df, 'normalized_target')\n",
    "    X_vl, y_vl, _            = prepare_data(val_df,  'normalized_target')\n",
    "\n",
    "    K = 15  \n",
    "    y_tr_bins = make_batch_bins(train_df, 'normalized_target', 'batch', K)\n",
    "    y_vl_bins = make_batch_bins(val_df,  'normalized_target', 'batch', K)\n",
    "\n",
    "    \n",
    "    # группы для LGBMRanker\n",
    "    # group = число строк в каждом batch (по порядку уникальных batch'ей в train_df)\n",
    "    grp_tr = make_group(train_df['batch'])\n",
    "    grp_vl = make_group(val_df ['batch'])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preproc',  preprocessing),\n",
    "        ('todense',  FunctionTransformer(to_dense, feature_names_out=\"one-to-one\")),\n",
    "        ('ranker',   LGBMRanker(**ranker_params)),\n",
    "    ])\n",
    "\n",
    "    # 5) предварительно трансформируем X_tr и X_vl\n",
    "    X_tr_t = pipeline.named_steps['todense']\\\n",
    "                      .transform(pipeline.named_steps['preproc']\\\n",
    "                      .fit_transform(X_tr, y_tr_bins))\n",
    "    X_vl_t = pipeline.named_steps['todense']\\\n",
    "                      .transform(pipeline.named_steps['preproc']\\\n",
    "                      .transform   (X_vl))\n",
    "\n",
    "    # 6) фитим ranker вручную\n",
    "    pipeline.named_steps['ranker'].fit(\n",
    "        X_tr_t, y_tr_bins,\n",
    "        group      = grp_tr,\n",
    "        eval_set   = [(X_vl_t, y_vl_bins)],\n",
    "        eval_group = [grp_vl],\n",
    "        eval_at    = [3],\n",
    "        #verbose    = False,\n",
    "    )\n",
    "    \n",
    "    # предсказание\n",
    "    y_pred_val = pipeline.predict(X_vl)\n",
    "    \n",
    "    y_pred_s = pd.Series(y_pred_val, index=val_df.index)\n",
    "\n",
    "    ndcgs = []\n",
    "    \n",
    "    def pred_converter(df):\n",
    "        indices = y_vl_bins.index.get_level_values(1).values\n",
    "        values = y_vl_bins.values  # или df['название_столбца'].values\n",
    "        return pd.Series(values, index=indices)\n",
    "\n",
    "    \n",
    "    for b in val_df['batch'].unique():\n",
    "        mask = (val_df['batch'] == b)\n",
    "        # true_rank должен быть Series или numpy того же размера\n",
    "        true_labels = pred_converter(y_vl_bins)[mask].to_numpy()\n",
    "        preds       = y_pred_s[mask].to_numpy()\n",
    "        k = min(3, len(preds))\n",
    "        ndcgs.append(ndcg_score([true_labels], [preds], k=k))\n",
    "    \n",
    "    avg_ndcg = np.mean(ndcgs)\n",
    "    \n",
    "    # 2) для сравнения: MSE, R2, корреляция\n",
    "    mse_list.append(mean_squared_error(pred_converter(y_vl_bins), y_pred_val))\n",
    "    r2_list.append(r2_score(pred_converter(y_vl_bins), y_pred_val))\n",
    "    corr = pearsonr(pred_converter(y_vl_bins), y_pred_val)[0]\n",
    "    corr_list.append(corr if not np.isnan(corr) else 0.0)\n",
    "    ndcg_list.append(avg_ndcg)\n",
    "\n",
    "# итог\n",
    "print(f\"CV results over {n_splits} folds:\")\n",
    "print(f\" avg NDCG@3 = {np.mean(ndcg_list):.4f} ± {np.std(ndcg_list):.4f}\")\n",
    "print(f\" avg Corr   = {np.mean(corr_list):.4f} ± {np.std(corr_list):.4f}\")\n",
    "print(f\" avg R2     = {np.mean(r2_list):.4f} ± {np.std(r2_list):.4f}\")\n",
    "print(f\" avg MSE    = {np.mean(mse_list):.4e} ± {np.std(mse_list):.4e}\")\n",
    "\n",
    "predict = pipeline_reg.predict(combined_ml_df[columns_for_model].drop(TGT_COLS+['batch', 'normalized_target'], axis=1))#.drop('normalized_target', axis=1))\n",
    "if len(TGT_COLS)!=1:\n",
    "    predict_series = pd.Series(np.mean(predict, axis=1), index=combined_ml_df.index)\n",
    "    predict_series = predict_series\n",
    "else:\n",
    "    predict_series = pd.Series(predict, index=combined_ml_df.index)\n",
    "reg_pred= predict_series.reindex(df_phase.index)\n",
    "\n",
    "predict = pipeline.predict(combined_ml_df[columns_for_model].drop(TGT_COLS+['batch', 'normalized_target'], axis=1))#.drop('normalized_target', axis=1))\n",
    "if len(TGT_COLS)!=1:\n",
    "    predict_series = pd.Series(np.mean(predict, axis=1), index=combined_ml_df.index)\n",
    "    predict_series = predict_series\n",
    "else:\n",
    "    predict_series = pd.Series(predict, index=combined_ml_df.index)\n",
    "    predict_series = 2* ((predict_series +15) / (5 + 15))-1\n",
    "rank_pred = predict_series.reindex(df_phase.index)\n",
    "w_reg =  0.940107707282538\n",
    "df_phase['predicted_p'] = (reg_pred * w_reg) + (rank_pred * (1.0-w_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5657f395-7e13-4a6e-8fa5-6d8b269e3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(df_phase.loc[combined_ml_df[combined_ml_df['batch'].isin(test_batches)].index]['normalized_target'], df_phase.loc[combined_ml_df[combined_ml_df['batch'].isin(test_batches)].index]['predicted_p']))\n",
    "pred11s = df_phase['predicted_p'].dropna()\n",
    "print(mean_squared_error(df_phase[df_phase.index.isin(pred11s.index)]['normalized_target'], pred11s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
