{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Necrotox/Necrotox/blob/main/bot_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w1nf_P3XJ6K"
      },
      "outputs": [],
      "source": [
        "!pip install -q --no-warn-script-location --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torch==2.7.1+cpu > /dev/null 2>&1\n",
        "\n",
        "!pip install -q --no-warn-script-location --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torchvision==0.22.0+cpu > /dev/null 2>&1\n",
        "!pip install -q --no-warn-script-location --index-url https://download.pytorch.org/whl/cpu \\\n",
        "  torchaudio==2.7.1+cpu > /dev/null 2>&1\n",
        "\n",
        "!pip install -q --no-warn-script-location \\\n",
        "  pytorch-lightning==2.5.2 pytorch-forecasting==1.4.0 > /dev/null 2>&1\n",
        "\n",
        "!pip install -q tinkoff-investments dill telebot --upgrade mplfinance > /dev/null 2>&1\n",
        "\n",
        "import asyncio\n",
        "import contextlib\n",
        "import functools\n",
        "import statsmodels.api as sm\n",
        "import inspect\n",
        "import zipfile\n",
        "import pickle\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "import time\n",
        "import pytz\n",
        "import dill\n",
        "import lzma\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "import hashlib\n",
        "import random\n",
        "import joblib\n",
        "import math\n",
        "import psutil\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import scipy.sparse.linalg as spla\n",
        "\n",
        "\n",
        "from numba import jit, njit\n",
        "from grpc import StatusCode\n",
        "from grpc.aio import AioRpcError\n",
        "from contextlib import redirect_stderr, redirect_stdout\n",
        "from tinkoff.invest import AsyncClient, CandleInterval\n",
        "from tinkoff.invest.exceptions import RequestError\n",
        "from tinkoff.invest import (\n",
        "    AsyncClient,\n",
        "    CandleInstrument,\n",
        "    MarketDataRequest,\n",
        "    SubscribeCandlesRequest,\n",
        "    SubscriptionAction,\n",
        "    SubscriptionInterval,\n",
        ")\n",
        "from tinkoff.invest.schemas import Candle\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from scipy.stats import norm\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from collections import defaultdict, deque\n",
        "from multiprocessing import Process\n",
        "from multiprocessing import Process\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import mplfinance as mpf\n",
        "import telebot\n",
        "import datetime\n",
        "import matplotlib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.signal import lfilter\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.ndimage import gaussian_filter1d\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.stats import norm, pearsonr\n",
        "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.model_selection import BaseCrossValidator\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, PowerTransformer, QuantileTransformer, RobustScaler, StandardScaler\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data.encoders import EncoderNormalizer\n",
        "from pytorch_forecasting.metrics import Metric, QuantileLoss\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.callbacks.progress import RichProgressBar, TQDMProgressBar\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import Any, Dict, Iterable, Iterator, List, Literal, Mapping, Optional, Tuple\n",
        "import os, gc, warnings, typing as tp\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "matplotlib.use(\"agg\")\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import psutil\n",
        "import os.path\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print('Загруженны библиотеки', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "# nest_asyncio.apply()\n",
        "\n",
        "#-1002681271607 Чат с общей статой\n",
        "#-1002619839070 Чат по прошедшим проверку алертам\n",
        "#-1002555215009 Чат со статистикой\n",
        "#-1002596481496 Чат с ошибками\n",
        "\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "path_to_save = '/content/drive/MyDrive/t_ml/data/'\n",
        "tg_bot_token = \"7734846848:AAHjM7r9ZInzC59GYkUfQUZr7a6ynIaTRAg\"\n",
        "bot = telebot.TeleBot(tg_bot_token)\n",
        "\n",
        "#with open('/content/drive/MyDrive/t_ml/ful_tickers_params.txt', 'r') as file:\n",
        "#    full_ticker_params = json.load(file)\n",
        "\n",
        "\n",
        "MAX_CANDLES = 5000\n",
        "\n",
        "TOKEN = 't.bWjxPlU6vz77qoqS754OKy0QorLDaZP-CE091dhGl56v7GHrqgF-mQAdWaeRg2kDRJmmxzvaaOwKUTxW6dnOKg'\n",
        "sectors = ['consumer', 'ecomaterials', 'electrocars', 'energy', 'financial', 'green_buildings',\n",
        "           'green_energy', 'health_care', 'industrials', 'it', 'materials',\n",
        "           'other', 'real_estate', 'telecom', 'utilities']\n",
        "\n",
        "ban_list = ['KLVZ', 'APTK', 'OKEY', 'VSEH', 'KRKNP', 'UDMN', 'YAKG', 'AFKS', 'CARM', 'MGKL', 'MBNK', 'ZAYM', 'GLTR@GS',\n",
        "            'DELI', 'DIAS', 'IVAT', 'SOFL', 'AKRN', 'GMKN', 'KAZT', 'KAZTP',\n",
        "            'KZOS', 'KZOSP', 'NKNCP', 'OBNE', 'OBNEP', 'PHOR', 'RUAL', 'UFOSP', 'UNKL', 'VSMO', 'LEAS', 'CNTL', 'MGTSP',\n",
        "            'HYDR', 'IRAO', 'MRKY', 'OGKB']\n",
        "\n",
        "countrys = ['AR', 'AU', 'BE', 'BM', 'BR', 'CA', 'CH', 'CN',\n",
        "            'DE', 'FI', 'FR', 'GB', 'HK', 'IE', 'IL', 'IN', 'IT', 'JP',\n",
        "            'KR', 'KZ', 'NL', 'PE', 'RU', 'SE', 'SG', 'TW', 'US', 'UY']\n",
        "\n",
        "#TICKERS = ['LIFE','UNAC','VKCO','MGNT','SVCB','ETLN','WUSH','HHRU','LNZL','SELG',\n",
        "# 'TATNP','PRFN','MAGN','VTBR','NKHP','ALRS','MRKP','FLOT','TATN','SPBE','DVEC','RTKM','MTSS',\n",
        "# 'TGKN','TRNFP','FEES','RBCM','ZILLP','MSRS','NSVZ','GCHE','SNGSP','NVTK','NKNC',\n",
        "# 'AQUA','VRSB','MOEX','ROLO','SNGS','CBOM','TGKBP','ABRD','PIKK','ROSN','TRMK','MRKU',\n",
        "# 'CHMF','ENPG','MRKV','LSRG','SVAV','RTKMP','KMAZ','VEON-RX','BANE','AFLT','SBER',\n",
        "# 'MVID','MSTT','MTLR','GAZP','SBERP','LENT','BSPB','RKKE','LSNG','MRKC','POSI','KROT','SIBN',\n",
        "# 'TGKB','KLSB','RNFT','MRKZ','QIWI','GTRK','NLMK','BELU','LNZLP','SFIN','RASP','SGZH',\n",
        "# 'UPRO','MRKS','MSNG','PLZL','LKOH','NMTP','TGKA','MTLRP']\n",
        "\n",
        "TICKERS =  ['RAGR', 'MGNT', 'MSTT', 'VRSB', 'PRFN', 'MTLR', 'LIFE', 'UPRO', 'GECO',\n",
        "       'BANE', 'MTLRP', 'GEMC', 'NVTK', 'TRNFP', 'TRMK', 'LSNGP', 'OBNEP',\n",
        "       'SNGSP', 'UWGN', 'MRKP', 'KZOSP', 'YDEX', 'NLMK', 'IRKT', 'CNTLP',\n",
        "       'LENT', 'KLSB', 'SELG', 'NMTP', 'UNAC', 'VKCO', 'MRKU', 'UGLD', 'NTZL',\n",
        "       'BANEP', 'FLOT', 'TGKJ', 'MAGN', 'ROSN', 'TGKB', 'AFKS', 'TTLK', 'HEAD',\n",
        "       'KZIZ', 'NOMP', 'OKEY', 'ABRD', 'NSVZ', 'MRKV', 'LSNG', 'MRKC', 'SVAV',\n",
        "       'ETLN', 'MRKZ', 'LNZL', 'CNRU', 'BSPB', 'RBCM', 'PMSB', 'LSRG', 'RNFT',\n",
        "       'MOEX', 'GTRK', 'NKHP', 'LKOH', 'SBERP', 'SBER', 'PLZL', 'RENI', 'MDMG',\n",
        "       'AFLT', 'FESH', 'OBNE', 'X5', 'MGTSP', 'DVEC', 'KROT', 'TATNP', 'OZPH',\n",
        "       'TGKN', 'TATN', 'PMSBP', 'TGKBP', 'SPBE', 'LNZLP', 'CHMK', 'KZIZP',\n",
        "       'RKKE', 'FRHC']\n",
        "\n",
        "#TICKERS = ['NKNC', 'PIKK']\n",
        "\n",
        "\n",
        "SECTOR = sectors[4]\n",
        "COUNTRY = countrys[22]\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}open_price.txt', 'r') as f:\n",
        "        open_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}close_price.txt', 'r') as f:\n",
        "        close_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}high_price.txt', 'r') as f:\n",
        "        high_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}low_price.txt', 'r') as f:\n",
        "        low_price = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}volume.txt', 'r') as f:\n",
        "        volume = json.loads(f.read())\n",
        "\n",
        "    print(\"Данные 1 успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке данных: {e}\")\n",
        "    open_price, close_price, high_price, low_price, volume = {}, {}, {}, {}, {}\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}time_last_kline_start.txt', 'r') as f:\n",
        "        time_last_kline_start = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}time_last_kline_end.txt', 'r') as f:\n",
        "        time_last_kline_end = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}ma.txt', 'r') as f:\n",
        "        ma = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}open_trades.txt', 'r') as f:\n",
        "        open_trades = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}trading_data.txt', 'r') as f:\n",
        "        trading_data = json.loads(f.read())\n",
        "\n",
        "    print(\"Данные 2 успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке данных: {e}\")\n",
        "    time_last_kline_start, time_last_kline_end, ma, open_trades, trading_data = {}, {}, {}, {}, {}\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "\n",
        "    with open(f'{path_to_save}pmax.txt', 'r') as f:\n",
        "        pmax = json.loads(f.read())\n",
        "\n",
        "    with open(f'{path_to_save}signals.txt', 'r') as f:\n",
        "        signals = json.loads(f.read())\n",
        "\n",
        "    print(\"Данные 3 успешно загружены.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке данных: {e}\")\n",
        "    pmax, signals = {}, {}\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "print('Загрузка Моделей', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "try:\n",
        "    kmeans_global = joblib.load(f'{path_to_save}models/kmeans_global.pkl')\n",
        "    print('Модель MiniBatchKMeans успешно загруженна', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели MiniBatchKMeans: {e}\")\n",
        "    kmeans_global = joblib.load(f'{path_to_save}models/kmeans_global.pkl')\n",
        "    print('Модель MiniBatchKMeans загруженна со второй попытки', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    scaler_global = joblib.load(f'{path_to_save}models/scaler_global.pkl')\n",
        "    print('Скалер успешно загружен', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели MiniBatchKMeans: {e}\")\n",
        "    scaler_global = joblib.load(f'{path_to_save}models/scaler_global.pkl')\n",
        "    print('Скалер загружен со второй попытки', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "'''\n",
        "try:\n",
        "    with open(f'{path_to_save}/models/regression_model_general.dill', 'rb') as file:\n",
        "        regression_model = dill.load(file)\n",
        "        print('Модель выхода из сделки загруженна', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели регрессии: {e}\")\n",
        "    with open(f'{path_to_save}/models/regression_model_general.dill', 'rb') as file:\n",
        "        regression_model = dill.load(file)\n",
        "        print('Модель выхода из сделки загруженна с второйпопытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "try:\n",
        "    with open(f'{path_to_save}/models/global_model.dill', 'rb') as file:\n",
        "        classifier_model = dill.load(file)\n",
        "        print('Глобальная модель поиска входа для всех кластеров заргуженна',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели поиска входа для 0 кластера: {e}\")\n",
        "    with open(f'{path_to_save}/ful_tickers_params.txt', 'rb') as file:\n",
        "        classifier_model = dill.load(file)\n",
        "        print('Глобальная модель поиска входа для всех кластеров загруженна с второй попытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "'''\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}/ful_tickers_params_new.txt', 'r') as file:\n",
        "      ticker_params = json.load(file)\n",
        "      print('Параметры для тикетов загруженны',\n",
        "            f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке параметров тикетов: {e}\")\n",
        "    with open(f'{path_to_save}/ful_tickers_params_new.txt', 'r') as file:\n",
        "        ticker_params = json.load(file)\n",
        "        print('Параметры для тикетов загруженны со второй попытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    transformers_path = f'{path_to_save}/models/neuros/'\n",
        "    models_path = f'{path_to_save}/models/final_models/'\n",
        "\n",
        "    with open(f'{path_to_save}/models/final_cols.pkl', 'rb') as f:\n",
        "        final_cols = pickle.load(f)\n",
        "\n",
        "    with open(f'{path_to_save}/models/final_params.pkl', 'rb') as f:\n",
        "        final_params = pickle.load(f)\n",
        "\n",
        "    with open(f'{path_to_save}/models/best_methods.pkl', 'rb') as f:\n",
        "        methods = pickle.load(f)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при определении пути на модели и трансформеры: {e}\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "try:\n",
        "    with open(f'{path_to_save}/phase_ful_tickers_params.txt', 'r') as file:\n",
        "      phase_ticker_params = json.load(file)\n",
        "      print('Параметры для тикетов с фазами загруженны',\n",
        "            f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке параметров тикетов с фазами: {e}\")\n",
        "    with open(f'{path_to_save}/phase_ful_tickers_params.txt', 'r') as file:\n",
        "        phase_ticker_params = json.load(file)\n",
        "        print('Параметры для тикетов с фазами загруженны со второй попытки',\n",
        "              f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "    await asyncio.sleep(60)\n",
        "\n",
        "print('Память после загрузки всех модулей', f\"Память: {psutil.Process().memory_info().rss / 1024 ** 2:.2f} МБ\")\n",
        "\n",
        "\n",
        "# open_price, close_price, high_price, low_price, volume, time_last_kline_start, time_last_kline_end, atr, pmax, signals, var = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
        "\n",
        "def to_dense(X):\n",
        "    \"\"\"Преобразует разреженную матрицу в плотную.\"\"\"\n",
        "    if issparse(X):\n",
        "        return X.toarray()\n",
        "    return X\n",
        "\n",
        "class ToDenseTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Преобразует разреженную матрицу в плотную numpy-массив.\"\"\"\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        if issparse(X):\n",
        "            return X.toarray()\n",
        "        return X\n",
        "    def get_feature_names_out(self, input_features: tp.Sequence[str] | None = None):\n",
        "        return np.asarray(input_features) if input_features is not None else np.array([])\n",
        "\n",
        "def deep_elbow(imp: np.ndarray, win: int = 5, eps: float = 0.02) -> int:\n",
        "    \"\"\"\n",
        "    Берём окно длиной win, считаем средний относительный спад.\n",
        "    Первое место, где спад < eps, считаем плато.\n",
        "    \"\"\"\n",
        "    if len(imp) <= win:\n",
        "        return len(imp)\n",
        "    dif = np.abs(np.diff(imp) / (imp[:-1] + 1e-9))\n",
        "    # скользящее среднее\n",
        "    m = np.convolve(dif, np.ones(win) / win, mode=\"valid\")\n",
        "    flat = np.nonzero(m < eps)[0]\n",
        "    return int(flat[0] + win) if flat.size else len(imp)\n",
        "\n",
        "# ───────────────────────────────────────────────────────\n",
        "# 2.  корреляционная чистка (быстрая, исправленная)\n",
        "# ───────────────────────────────────────────────────────\n",
        "def corr_prune(df: pd.DataFrame, feats: list[str], thr=.95) -> list[str]:\n",
        "    if len(feats) < 2 or thr >= 1:\n",
        "        return feats\n",
        "    X  = df[feats].apply(pd.to_numeric, errors='ignore')\n",
        "    C  = X.corr().abs().to_numpy()\n",
        "    keep = []\n",
        "    for i in range(len(feats)):\n",
        "        if not keep or C[i, keep].max() < thr:\n",
        "            keep.append(i)\n",
        "    return [feats[i] for i in keep]\n",
        "\n",
        "# ───────────────────────────────────────────────────────\n",
        "# 3.  универсальный быстрый селектор\n",
        "# ───────────────────────────────────────────────────────\n",
        "def fast_feature_select(\n",
        "        res           : pd.DataFrame,      # feature / importance\n",
        "        df_full       : pd.DataFrame,      # датасет для corr-prune\n",
        "        target_col    : str = \"target\",\n",
        "        *,\n",
        "        method        : str = \"elbow\",     # elbow | deep_elbow | percentile | quantile | top_k\n",
        "        top_k         : int = 150,         # для method=\"top_k\"\n",
        "        perc_limit    : float = .90,       # для method=\"percentile\"\n",
        "        quantile_q    : float = .10,       # для method=\"quantile\"\n",
        "        elbow_eps     : float = .05,       # (> flat %) для (shallow) elbow\n",
        "        deep_win      : int = 5,           # окно для deep_elbow\n",
        "        deep_eps      : float = .02,       # порог для deep_elbow\n",
        "        corr_thr      : float = .95        # корреляционный порог\n",
        ") -> list[str]:\n",
        "\n",
        "    ranked = res.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    feats  = ranked.feature.to_numpy()\n",
        "    imps   = ranked.importance.to_numpy()\n",
        "\n",
        "    # ---------- 1) сколько оставить  ----------\n",
        "    if method == \"elbow\":                 # одношаговое колено\n",
        "        k = np.argmax(np.abs(np.diff(imps) / (imps[:-1] + 1e-9)) < elbow_eps) + 1\n",
        "        if k == 1:        # колено не найдено\n",
        "            k = len(imps)\n",
        "    elif method == \"deep_elbow\":\n",
        "        k = deep_elbow(imps, win=deep_win, eps=deep_eps)\n",
        "    elif method == \"percentile\":          # кумулятивная доля\n",
        "        cum = np.cumsum(imps)\n",
        "        k   = np.searchsorted(cum / cum[-1], perc_limit) + 1\n",
        "    elif method == \"quantile\":\n",
        "        thr = np.quantile(imps, 1 - quantile_q)\n",
        "        k   = int((imps >= thr).sum())\n",
        "    elif method == \"top_k\":\n",
        "        k = min(top_k, len(feats))\n",
        "    else:\n",
        "        raise ValueError(\"unknown method\")\n",
        "\n",
        "    selected = feats[:k].tolist()\n",
        "\n",
        "    # ---------- 2) корреляционная чистка ----------\n",
        "    selected = corr_prune(\n",
        "        df_full.drop(columns=[target_col], errors='ignore'),\n",
        "        selected,\n",
        "        thr=corr_thr\n",
        "    )\n",
        "\n",
        "    return selected\n",
        "\n",
        "def patch_feature_timings(cls):\n",
        "    \"\"\"\n",
        "    Оборачивает все методы cls, начинающиеся на _feat_,\n",
        "    и складывает затраченное время в self._timings[method_name].\n",
        "    Вызывать сразу после объявления класса.\n",
        "    \"\"\"\n",
        "    def timed(func):\n",
        "        def wrapper(self, *args, **kwargs):\n",
        "            t0 = time.perf_counter()\n",
        "            result = func(self, *args, **kwargs)\n",
        "            dt = time.perf_counter() - t0\n",
        "            # заводим словарь при первом же вызове\n",
        "            if not hasattr(self, \"_timings\"):\n",
        "                self._timings = {}\n",
        "            self._timings[func.__name__] = dt\n",
        "            return result\n",
        "        return wrapper\n",
        "\n",
        "    for name, method in inspect.getmembers(cls, inspect.isfunction):\n",
        "        if name.startswith(\"_feat_\"):             # ← только расчётные функции\n",
        "            setattr(cls, name, timed(method))\n",
        "\n",
        "    return cls\n",
        "\n",
        "def _slope(y):\n",
        "        x = np.arange(len(y))\n",
        "        # линейная регрессия «по формуле»\n",
        "        xm, ym = x.mean(), y.mean()\n",
        "        beta = ((x - xm) * (y - ym)).sum() / ((x - xm)**2).sum()\n",
        "        return beta\n",
        "\n",
        "@njit\n",
        "def _rolling_entropy_exact_numba(x, window):\n",
        "    \"\"\" Точная реализация rolling entropy с bins='auto' для каждого окна. \"\"\"\n",
        "    n = len(x)\n",
        "    res = np.empty(n, dtype=np.float32)\n",
        "    res[:] = np.nan\n",
        "    if window < 2:\n",
        "        return res\n",
        "    for end in range(window - 1, n):\n",
        "        win = x[end - window + 1 : end + 1]\n",
        "        a_min = np.min(win)\n",
        "        a_max = np.max(win)\n",
        "        if a_min == a_max:\n",
        "            res[end] = 0.0\n",
        "            continue\n",
        "        sorted_win = np.sort(win)\n",
        "        idx25 = int(0.25 * window)\n",
        "        idx75 = int(0.75 * window)\n",
        "        q25 = sorted_win[idx25]\n",
        "        q75 = sorted_win[idx75]\n",
        "        iqr = q75 - q25\n",
        "        sturges = int(np.ceil(np.log2(window) + 1))\n",
        "        if iqr > 0:\n",
        "            bin_width = 2.0 * iqr / (window ** (1.0 / 3.0))\n",
        "            fd = int(np.ceil((a_max - a_min) / bin_width))\n",
        "        else:\n",
        "            fd = 1\n",
        "        nbins = max(sturges, fd, 1)\n",
        "        edges = np.empty(nbins + 1, dtype=np.float32)\n",
        "        step = (a_max - a_min) / nbins\n",
        "        edges[0] = a_min\n",
        "        for i in range(1, nbins):\n",
        "            edges[i] = a_min + i * step\n",
        "        edges[nbins] = a_max\n",
        "        counts = np.zeros(nbins, dtype=np.int32)  # int32 достаточно для window<=1e9\n",
        "        for val in win:\n",
        "            idx = np.searchsorted(edges, val, side='right') - 1\n",
        "            if 0 <= idx < nbins:\n",
        "                counts[idx] += 1\n",
        "        ent = 0.0\n",
        "        total = float(window)\n",
        "        for c in counts:\n",
        "            if c > 0:\n",
        "                p = c / total\n",
        "                ent -= p * np.log(p + 1e-10)\n",
        "        res[end] = ent\n",
        "    return res\n",
        "\n",
        "@jit(nopython=True)\n",
        "def rolling_autocorr(arr, window):\n",
        "    n = len(arr)\n",
        "    result = np.full(n, 0.0)\n",
        "    for i in range(n):\n",
        "        start = max(0, i - window + 1)\n",
        "        w = i - start + 1\n",
        "        if w <= 1:\n",
        "            continue\n",
        "        s = arr[start: i + 1]\n",
        "        a = s[1:]\n",
        "        b = s[:-1]\n",
        "        n_pts = w - 1\n",
        "        mean_a = np.sum(a) / n_pts\n",
        "        mean_b = np.sum(b) / n_pts\n",
        "        cov = np.dot(a, b) / n_pts - mean_a * mean_b\n",
        "        var_a = np.dot(a, a) / n_pts - mean_a * mean_a\n",
        "        var_b = np.dot(b, b) / n_pts - mean_b * mean_b\n",
        "        if var_a <= 0 or var_b <= 0:\n",
        "            result[i] = np.nan\n",
        "        else:\n",
        "            std_a = np.sqrt(var_a)\n",
        "            std_b = np.sqrt(var_b)\n",
        "            result[i] = cov / (std_a * std_b)\n",
        "    return result\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "@patch_feature_timings\n",
        "class FeatureCalculatorForRegression:\n",
        "    \"\"\"\n",
        "    df  -- исходный OHLCV-DataFrame.\n",
        "    required_features -- список имён колонок, которые нужны модели.\n",
        "    params -- { primitive_name: {... гиперпараметры ...}, 'stat_window': int }.\n",
        "    \"\"\"\n",
        "\n",
        "    _PRIMITIVES = {\n",
        "        \"MEDPRICE\":               \"_feat_base\",\n",
        "        \"MACD\":                   \"_feat_macd\",\n",
        "        \"MACD_Hist\":              \"_feat_macd\",\n",
        "        \"Overbought_Oversold\":    \"_feat_overbought\",\n",
        "        \"Overbought_Oversold_Index_mean\": \"_feat_overbought\",\n",
        "        \"Price_MADist%\":          \"_feat_madist\",\n",
        "        \"Mean_Reversion\":         \"_feat_mean_reversion\",\n",
        "        \"Fear_Greed\":             \"_feat_fear_greed\",\n",
        "        \"perc_var_open_close\":    \"_feat_price_variation\",\n",
        "        \"pmax_norm\":              \"_feat_pmax_ma\",\n",
        "        \"ma_norm\":                \"_feat_pmax_ma\",\n",
        "        \"ma_pmax_norm_rage\":      \"_feat_pmax_ma\",\n",
        "        \"ma_pmax_norm_rage_pct\":  \"_feat_pmax_ma\",\n",
        "        \"slope_trend\":            \"_feat_slope\",\n",
        "        \"ema_trend\":              \"_feat_ema_trend\",\n",
        "        \"hp_trend\":               \"_feat_hp_trend\",\n",
        "        \"trade_bars_counter\":     \"_feat_trade_duration\",\n",
        "        \"ROC\":                    \"_feat_roc\",\n",
        "        \"ATR_norm\":               \"_feat_atr\",\n",
        "        \"BB_Width\":               \"_feat_bb_width\",\n",
        "        \"Asset_Growth\":           \"_feat_asset_growth\",\n",
        "        \"ema_acceleration\":       \"_feat_ema_acceleration\",\n",
        "        \"price_change\":           \"_feat_price_change\",\n",
        "        \"Asset_To_Equity_Ratio\":  \"_feat_asset_to_equity_ratio\",\n",
        "        \"volume_ratio\":           \"_feat_fear_greed_index\",\n",
        "        \"WILLR\":                  \"_feat_willr\",\n",
        "        \"kf_trend\":               \"_feat_kf_trend\",\n",
        "        \"Fractal_Dim\":            \"_feat_fractal_dim\",\n",
        "        \"Peak_Exhaustion_Score\":  \"_feat_peak_exhaustion\",\n",
        "        \"%B_BB\":                  \"_feat_bb_percent\",\n",
        "        \"Kurtosis_roll\":          \"_feat_kurtosis_roll\",\n",
        "        \"OBV_div\":                \"_feat_obv_div\",\n",
        "        \"RSI_slope\":              \"_feat_rsi_slope\",\n",
        "        \"Vol_Decay\":              \"_feat_vol_decay\",\n",
        "        \"Accel_Decay\":            \"_feat_accel_decay\",\n",
        "        \"Entropy_roll\":           \"_feat_entropy_roll\",\n",
        "        \"Wavelet_Var_Ratio\":      \"_feat_wavelet_var\",\n",
        "        \"Autocorr_Lag1\":          \"_feat_autocorr\",\n",
        "        \"Beta_Market\":            \"_feat_beta\",\n",
        "        \"PSC\":                    \"_feat_peak_squeeze_curvature\",\n",
        "        \"PSC_raw\":                \"_feat_peak_squeeze_curvature\",\n",
        "        \"PSC_z\":                  \"_feat_peak_squeeze_curvature\",\n",
        "        \"PSC_sigmoid\":            \"_feat_peak_squeeze_curvature\",\n",
        "    }\n",
        "\n",
        "    # СТАРАЯ: r\"^ago_(\\d+)_\"\n",
        "    # НОВАЯ: умеет и \"ago50_\", и \"ago_50_\"\n",
        "    _LAG_RE  = re.compile(r\"^ago_?(\\d+)_\")\n",
        "    _STAT_RE = re.compile(r\"_(mean|min|max|std|skew|kurt|quantile(\\d{2}))$\")\n",
        "    _LOGSF   = \"_logsf\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self.df = df.copy()\n",
        "        f64 = self.df.select_dtypes(\"float64\").columns\n",
        "        self.df[f64] = self.df[f64].astype(np.float32)\n",
        "        if \"time\" in self.df:\n",
        "            ts = pd.to_datetime(self.df[\"time\"], utc=True, errors=\"coerce\")\n",
        "            self.df[\"hour\"]        = ts.dt.hour.astype(\"int8\")\n",
        "            self.df[\"day_of_week\"] = ts.dt.day_of_week.astype(\"int8\")\n",
        "\n",
        "    def calculate_features(\n",
        "        self,\n",
        "        required_features: Iterable[str],\n",
        "        params: Mapping[str, Mapping[str, Any]] | None = None\n",
        "    ) -> pd.DataFrame:\n",
        "        saved_cols = ['regime', 'normalized_target', 'batch', 'time', 'open', 'close', 'high', 'low', 'volume', 'buy_signal',\n",
        "                      'sell_signal', 'event_sell_time', 'event_sell_price', 'event_time', 'event_price', 'event_sell_time',\n",
        "                      'event_sell_price', 'target', 'pnl', 'ma', 'pmax']\n",
        "        self._params      = defaultdict(dict, params or {})\n",
        "        self._stat_window = self._params.get(\"stat_window\", 50)\n",
        "        for col in required_features:\n",
        "            self._ensure_column(col)\n",
        "        out = self.df[list(required_features)].copy()\n",
        "        f64 = out.select_dtypes(\"float64\").columns\n",
        "        out[f64] = out[f64].astype(np.float32)\n",
        "\n",
        "        for mandatory_col in saved_cols:\n",
        "            if mandatory_col in self.df.columns:\n",
        "                out[mandatory_col] = self.df[mandatory_col]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def calculate_all_possible_features(self, params: Mapping[str, Mapping[str, Any]] | None = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Вычисляет все возможные фичи, исключая lag-версии для указанных колонок.\n",
        "        Все бесконечные значения (np.inf/-np.inf) заменяются на 0.\n",
        "        Порядок вычислений:\n",
        "        1. Все базовые примитивы\n",
        "        2. Lag-версии фич (кроме исключенных)\n",
        "        3. Статистики для всех фич\n",
        "        \"\"\"\n",
        "        # Инициализация параметров\n",
        "        if not hasattr(self, '_params'):\n",
        "            self._params = defaultdict(dict, params or {})\n",
        "        self._stat_window = self._params.get(\"stat_window\", 50)\n",
        "\n",
        "        # Колонки, для которых не нужно создавать lag-версии\n",
        "        EXCLUDE_FROM_LAGS = {\n",
        "            'time', 'open', 'close', 'high', 'low', 'volume',\n",
        "            'ma', 'pmax', 'buy_signal', 'sell_signal', 'regime',\n",
        "            'event_time', 'event_price', 'event_sell_time',\n",
        "            'event_sell_price', 'pnl', 'target', 'normalized_target',\n",
        "            'batch', 'hour', 'day_of_week', 'trade_bars_counter'\n",
        "        }\n",
        "\n",
        "        # 1. Вычисляем все базовые примитивы\n",
        "        all_primitives = list(self._PRIMITIVES.keys())\n",
        "        for primitive in all_primitives:\n",
        "            method_name = self._PRIMITIVES[primitive]\n",
        "            primitive_params = self._params.get(primitive, {})\n",
        "            try:\n",
        "                getattr(self, method_name)(**primitive_params)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при вычислении примитива {primitive}: {str(e)}\")\n",
        "\n",
        "        # 2. Добавляем lag-версии только для разрешенных фич\n",
        "        numeric_cols = [\n",
        "            col for col in self.df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
        "            if col not in EXCLUDE_FROM_LAGS and  # Исключаем указанные колонки\n",
        "            not self._LAG_RE.match(col) and      # Исключаем уже lag-фичи\n",
        "            not col.endswith(self._LOGSF) and    # Исключаем logsf-фичи\n",
        "            not self._STAT_RE.search(col)        # Исключаем статистики\n",
        "        ]\n",
        "\n",
        "        lag_periods = [1, 2, 3, 5, 10, 20, 50]  # Стандартные лаги\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            for lag in lag_periods:\n",
        "                lag_col = f\"ago_{lag}_{col}\"\n",
        "                if lag_col not in self.df.columns:\n",
        "                    self.df[lag_col] = self.df[col].shift(lag)\n",
        "\n",
        "        # 3. Добавляем статистики для всех фич (кроме исключенных)\n",
        "        all_cols_for_stats = [\n",
        "            col for col in self.df.columns\n",
        "            if col not in EXCLUDE_FROM_LAGS and\n",
        "            not col.endswith(self._LOGSF) and\n",
        "            not self._STAT_RE.search(col)\n",
        "        ]\n",
        "\n",
        "        stats = ['mean', 'std', 'min', 'max', 'skew', 'kurt']\n",
        "\n",
        "        for col in all_cols_for_stats:\n",
        "            for stat in stats:\n",
        "                stat_col = f\"{col}_{stat}\"\n",
        "                if stat_col not in self.df.columns:\n",
        "                    try:\n",
        "                        self._add_stat(col, stat)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Ошибка при вычислении статистики {stat} для {col}: {str(e)}\")\n",
        "\n",
        "        # 4. Добавляем logsf-версии только для разрешенных фич\n",
        "        main_cols_for_logsf = [\n",
        "            col for col in numeric_cols\n",
        "            if not col.startswith('ago_') and\n",
        "            not col.endswith(self._LOGSF) and\n",
        "            col not in EXCLUDE_FROM_LAGS\n",
        "        ]\n",
        "\n",
        "        for col in main_cols_for_logsf:\n",
        "            logsf_col = f\"{col}{self._LOGSF}\"\n",
        "            if logsf_col not in self.df.columns:\n",
        "                try:\n",
        "                    self.df[logsf_col] = norm.logsf(self.df[col])\n",
        "                except Exception as e:\n",
        "                    print(f\"Ошибка при вычислении logsf для {col}: {str(e)}\")\n",
        "\n",
        "        # 5. Заменяем бесконечные значения на 0\n",
        "        numeric_cols_all = self.df.select_dtypes(include=['float32', 'float64', 'int32', 'int64']).columns\n",
        "        self.df[numeric_cols_all] = self.df[numeric_cols_all].replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        # Сохраняем все оригинальные колонки\n",
        "        for col in EXCLUDE_FROM_LAGS:\n",
        "            if col in self.df.columns and col not in self.df:\n",
        "                self.df[col] = self.df[col]\n",
        "\n",
        "        return self.df.copy()\n",
        "\n",
        "    def _ensure_column(self, name: str):\n",
        "        if name in self.df:\n",
        "            return\n",
        "\n",
        "        # 1) lag-префикс \"ago50_\" или \"ago_50_\"\n",
        "        m = self._LAG_RE.match(name)\n",
        "        if m:\n",
        "            lag  = int(m.group(1))\n",
        "            base = name[m.end():]\n",
        "            self._ensure_column(base)\n",
        "            self.df[name] = self.df[base].shift(lag)\n",
        "            return\n",
        "\n",
        "        # 2) _logsf\n",
        "        if name.endswith(self._LOGSF):\n",
        "            base = name[:-len(self._LOGSF)]\n",
        "            self._ensure_column(base)\n",
        "            self.df[name] = norm.logsf(self.df[base])\n",
        "            return\n",
        "\n",
        "        # 3) статистический суффикс\n",
        "        m = self._STAT_RE.search(name)\n",
        "        if m:\n",
        "            stat = m.group(1)\n",
        "            base = name[:m.start()]\n",
        "            self._ensure_column(base)\n",
        "            self._add_stat(base, stat)\n",
        "            return\n",
        "\n",
        "        # 4) примитив\n",
        "        prim = name\n",
        "        if prim.startswith(\"Overbought_Oversold\"):\n",
        "            prim = \"Overbought_Oversold\"\n",
        "        if prim.startswith(\"Fear_Greed\"):\n",
        "            prim = \"Fear_Greed\"\n",
        "        if prim not in self._PRIMITIVES:\n",
        "            raise KeyError(f\"Не знаю, как получить примитив «{prim}» для «{name}»\")\n",
        "        getattr(self, self._PRIMITIVES[prim])(**self._params.get(prim, {}))\n",
        "        if name not in self.df:\n",
        "            raise RuntimeError(f\"После _feat_{prim}() нет колонки «{name}»\")\n",
        "\n",
        "    def _add_stat(self, base: str, stat: str):\n",
        "        col = f\"{base}_{stat}\"\n",
        "        if col in self.df:\n",
        "            return\n",
        "        s = self.df[base]; w = self._stat_window\n",
        "        if stat == \"mean\":\n",
        "            self.df[col] = s.rolling(w).mean()\n",
        "        elif stat == \"std\":\n",
        "            self.df[col] = s.rolling(w).std()\n",
        "        elif stat == \"min\":\n",
        "            self.df[col] = s.rolling(w).min()\n",
        "        elif stat == \"max\":\n",
        "            self.df[col] = s.rolling(w).max()\n",
        "        elif stat == \"skew\":\n",
        "            self.df[col] = s.rolling(w).skew()\n",
        "        elif stat == \"kurt\":\n",
        "            self.df[col] = s.rolling(w).kurt()\n",
        "        elif stat.startswith(\"quantile\"):\n",
        "            q = int(stat[-2:]) / 100\n",
        "            self.df[col] = s.rolling(w).quantile(q)\n",
        "        else:\n",
        "            raise ValueError(f\"Неизвестная stat «{stat}»\")\n",
        "\n",
        "    # ---------------------- ПРИМИТИВЫ ----------------------\n",
        "\n",
        "    def _feat_base(self, medprice: int = 50):\n",
        "        if \"MEDPRICE\" in self.df:\n",
        "            return\n",
        "        self.df[\"MEDPRICE\"]      = (self.df[\"high\"] + self.df[\"low\"]) / 2\n",
        "        self.df[\"MEDPRICE_std\"] = self.df[\"MEDPRICE\"].rolling(medprice).std()\n",
        "\n",
        "    def _feat_macd(self, fast: int = 12, slow: int = 26, signal: int = 9):\n",
        "        \"\"\"\n",
        "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
        "        \"\"\"\n",
        "        if {\"MACD\",\"MACD_Hist\"}.issubset(self.df.columns):\n",
        "            return\n",
        "\n",
        "        close = self.df['close']\n",
        "        # Создаем множества для уникальных периодов\n",
        "        ema_cache_fp = close.ewm(span=fast, adjust=False).mean()\n",
        "\n",
        "        ema_cache_sp = close.ewm(span=slow, adjust=False).mean()\n",
        "        rolling_cache = close.rolling(window=slow).mean()\n",
        "\n",
        "        # Основной цикл вычислений\n",
        "        ema_fast = ema_cache_fp\n",
        "        ema_slow = ema_cache_sp\n",
        "        rolling_mean = rolling_cache\n",
        "        macd = ema_fast - ema_slow\n",
        "        macd_norm = macd / rolling_mean\n",
        "        self.df[f'MACD'] = macd_norm\n",
        "        signal = macd.ewm(span=signal, adjust=False).mean()\n",
        "        signal_norm = signal / rolling_mean\n",
        "\n",
        "        # Сохраняем результаты\n",
        "        self.df[f'MACD_Hist'] = macd_norm - signal_norm\n",
        "\n",
        "    '''def _feat_macd(self, fast: int = 12, slow: int = 26, signal: int = 9):\n",
        "        if {\"MACD\",\"MACD_Hist\"}.issubset(self.df.columns):\n",
        "            return\n",
        "        c  = self.df[\"close\"]\n",
        "        ef = c.ewm(span=fast, adjust=False).mean()\n",
        "        es = c.ewm(span=slow, adjust=False).mean()\n",
        "        macd = (ef - es) / (c.rolling(slow).mean().add(1e-10))\n",
        "        sig  = macd.ewm(span=signal, adjust=False).mean()\n",
        "        self.df[\"MACD\"]      = macd\n",
        "        self.df[\"MACD_Hist\"] = macd - sig'''\n",
        "\n",
        "    def _feat_overbought(self, rsi_p: int = 14, stoch_p: int = 14):\n",
        "        name = \"Overbought_Oversold_Index\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        c   = self.df[\"close\"]; d = c.diff()\n",
        "        g   = d.clip(lower=0); l = (-d).clip(lower=0)\n",
        "        rs  = g.rolling(rsi_p).mean() / (l.rolling(rsi_p).mean().add(1e-10))\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        lo  = self.df[\"low\"].rolling(stoch_p).min()\n",
        "        hi  = self.df[\"high\"].rolling(stoch_p).max()\n",
        "        st  = 100*(c - lo)/(hi - lo + 1e-10)\n",
        "        self.df[name] = (rsi + st)/2\n",
        "\n",
        "    def _feat_madist(self, span_lenght: int = 200):\n",
        "        name = \"Price_MADist%\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ema = self.df[\"close\"].ewm(span=span_lenght, adjust=False).mean()\n",
        "        self.df[name] = (self.df[\"close\"]/ema - 1)*100\n",
        "\n",
        "    def _feat_mean_reversion(self, window: int = 20):\n",
        "        name = \"Mean_Reversion\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ma = self.df[\"close\"].rolling(window).mean()\n",
        "        self.df[name] = self.df[\"close\"] - ma\n",
        "\n",
        "    def _feat_fear_greed(self, window: int = 14):\n",
        "        name = \"Fear_Greed_Index\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        v  = self.df[\"close\"].pct_change().rolling(window).std()\n",
        "        vc = self.df[\"volume\"].pct_change().rolling(window).mean()\n",
        "        tr = self.df[\"close\"]/self.df[\"close\"].rolling(window).mean()\n",
        "        self.df[name] = (v + vc + tr)/3*100\n",
        "\n",
        "    def _feat_price_variation(self):\n",
        "        name = \"perc_var_open_close\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        eps = 1e-10\n",
        "        self.df[name] = (self.df[\"close\"]-self.df[\"open\"])/(self.df[\"open\"]+eps)*100\n",
        "\n",
        "    def _feat_pmax_ma(self,\n",
        "        pmax_ma_length: int = 10,\n",
        "        pmax_ma_length_roll: int = 50,\n",
        "        pct_window: int = 5\n",
        "    ):\n",
        "        need = {\n",
        "            \"pmax_norm\", \"ma_norm\",\n",
        "            \"ma_pmax_norm_rage\", \"ma_pmax_norm_rage_pct\"\n",
        "        }\n",
        "        if need.issubset(self.df.columns):\n",
        "            return\n",
        "        if {\"pmax\",\"ma\"}.difference(self.df.columns):\n",
        "            raise ValueError(\"Нужны 'pmax' и 'ma'\")\n",
        "        c = self.df[\"close\"]\n",
        "        self.df[\"pmax_norm\"]             = (c-self.df[\"pmax\"])/self.df[\"pmax\"]\n",
        "        self.df[\"ma_norm\"]               = (c-self.df[\"ma\"])/self.df[\"ma\"]\n",
        "        self.df[\"ma_pmax_norm_rage\"]     = self.df[\"ma_norm\"] - self.df[\"pmax_norm\"]\n",
        "        # новый примитив — pct-динамика\n",
        "        self.df[\"ma_pmax_norm_rage_pct\"] = \\\n",
        "          self.df[\"ma_pmax_norm_rage\"].pct_change(pct_window).fillna(0)\n",
        "\n",
        "    def _feat_slope(self, slope_lag: int = 300, pct_window: int = 6):\n",
        "        name = \"slope_trend\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
        "        self.df[name] = r.rolling(slope_lag, min_periods=slope_lag)\\\n",
        "                         .apply(_slope, raw=True)\n",
        "\n",
        "    def _feat_ema_trend(self, span: int = 300, pct_window: int = 6):\n",
        "        name = \"ema_trend\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
        "        e = r.ewm(span=span, adjust=False).mean()\n",
        "        self.df[name] = e.diff().fillna(0)\n",
        "\n",
        "    def _feat_asset_to_equity_ratio(self):\n",
        "        \"\"\"\n",
        "        Вычисление коэффициента соотношения активов и собственного капитала.\n",
        "        \"\"\"\n",
        "        name = \"Asset_To_Equity_Ratio\"\n",
        "        asset = self.df['close']\n",
        "        equity = self.df['low']\n",
        "        # Добавляем в DataFrame\n",
        "        self.df[name] = asset / (equity + 1e-10)\n",
        "\n",
        "    def _feat_hp_trend(self, lamb: float = 1600):\n",
        "        name = \"hp_trend\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        y    = np.log(self.df[\"close\"]).fillna(method=\"ffill\")\n",
        "        coef = lamb/(1+lamb)\n",
        "        tr   = np.empty(len(y), dtype=float)\n",
        "        tr[0] = y.iloc[0]\n",
        "        for i in range(1, len(y)):\n",
        "            tr[i] = coef*y.iloc[i] + (1-coef)*tr[i-1]\n",
        "        self.df[name] = np.append([0], np.diff(tr))\n",
        "\n",
        "    def _feat_kf_trend(self,\n",
        "        pct_window: int = 6,\n",
        "        obs_var: float = 1e-4, # σ² ε_t (шум наблюдения)\n",
        "        level_var: float = 1e-5 # σ² η_t (шум уровня)\n",
        "        ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Добавляет к DataFrame колонки:\n",
        "        kf_trend — one-sided Калман-оценка тренда доходностей\n",
        "        kf_trend_logsf — лог-survival-function (z-score) тренда\n",
        "        Полностью каузально, обновляется тик-за-тиком.\n",
        "        \"\"\"\n",
        "\n",
        "        name = 'kf_trend'\n",
        "        # 1. Доходности\n",
        "        r = self.df['close'].pct_change(pct_window).fillna(0)\n",
        "        # 2. Local-level модель: y_t = μ_t + ε_t ;  μ_t = μ_{t-1} + η_t\n",
        "        mod = sm.tsa.UnobservedComponents(r, level='llevel')\n",
        "\n",
        "        # 3. Параметры модели в log-шкале (требование statsmodels)\n",
        "        params = np.log([obs_var, level_var])\n",
        "\n",
        "        # 4. Только forward-filter → нет look-ahead bias\n",
        "        res = mod.filter(params)                       # <— односторонний Калман\n",
        "        trend = pd.Series(res.filtered_state[0], index=self.df.index)\n",
        "\n",
        "        # 5. Запись результата\n",
        "        self.df['kf_trend'] = trend\n",
        "\n",
        "    def _feat_willr(self, window=14):\n",
        "        \"\"\"\n",
        "        Вычисление %R по методу Уильямса (WILLR).\n",
        "        \"\"\"\n",
        "        name = 'WILLR'\n",
        "        high = self.df['high']\n",
        "        low = self.df['low']\n",
        "        close = self.df['close']\n",
        "\n",
        "        highest_high = high.rolling(window).max()\n",
        "        lowest_low = low.rolling(window).min()\n",
        "\n",
        "        willr = ((highest_high - close) / (highest_high - lowest_low)) * -100\n",
        "\n",
        "        # Добавляем в DataFrame\n",
        "        self.df[name] = willr\n",
        "\n",
        "    def _feat_fear_greed_index(self, window: int = 14):\n",
        "        \"\"\"\n",
        "        Расчет объема как отношение последнего объема к скользящему среднему.\n",
        "        \"\"\"\n",
        "        name = \"volume_ratio\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        s = self.df[\"volume\"]\n",
        "        self.df[name] = s / s.rolling(window).mean()\n",
        "\n",
        "    def _feat_trade_duration(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Добавляет в self.df колонку 'trade_bars_counter' — количество прошедших баров\n",
        "        с момента входа в текущую открытую сделку до каждой строки self.df (включая строку).\n",
        "        Если открытой сделки нет — NaN.\n",
        "\n",
        "        Требования к self.df:\n",
        "          - self.df['time'] — время начала свечи (строки в ISO формате, совпадают со шкалой time_last_kline_start[ticker])\n",
        "          - self.df отсортирован по времени по возрастанию\n",
        "        \"\"\"\n",
        "        def _parse_to_timestamp(value):\n",
        "            # Используем pd.to_datetime для унифицированного парсинга ISO строк в UTC Timestamp\n",
        "            return pd.to_datetime(value, utc=True)\n",
        "\n",
        "        name = \"trade_bars_counter\"\n",
        "        if name in self.df.columns:\n",
        "            return self.df\n",
        "\n",
        "        n = len(self.df)\n",
        "        if n == 0:\n",
        "            self.df[name] = np.empty(0, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Получаем текущую открытую сделку (предполагаем open_trades и ticker доступны)\n",
        "        od = open_trades.get(ticker)\n",
        "        has_open = bool(od and od.get(\"status\") == \"open\" and od.get(\"buy_time\"))\n",
        "        if not has_open:\n",
        "            self.df[name] = np.full(n, np.nan, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Парсим buy_time в Timestamp\n",
        "        try:\n",
        "            buy_ts = _parse_to_timestamp(od[\"buy_time\"])\n",
        "        except ValueError:\n",
        "            # Если ошибка парсинга, установить NaN\n",
        "            self.df[name] = np.full(n, np.nan, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Получаем эталонную шкалу времени (ISO строки)\n",
        "        times_iso = time_last_kline_start.get(ticker, [])\n",
        "        if not times_iso:\n",
        "            self.df[name] = np.full(n, np.nan, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Парсим шкалу в список Timestamp\n",
        "        try:\n",
        "            dt_scale = [_parse_to_timestamp(s) for s in times_iso]\n",
        "        except ValueError:\n",
        "            self.df[name] = np.full(n, np.nan, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Найти индекс входа: первая свеча с ts >= buy_ts (bisect_left работает с Timestamp)\n",
        "        entry_idx_scale = bisect_left(dt_scale, buy_ts)\n",
        "\n",
        "        if entry_idx_scale >= len(dt_scale):\n",
        "            self.df[name] = np.full(n, np.nan, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Парсим времена из df (строки ISO)\n",
        "        try:\n",
        "            df_times = [_parse_to_timestamp(t) for t in self.df['time']]\n",
        "        except ValueError:\n",
        "            self.df[name] = np.full(n, np.nan, dtype=np.float32)\n",
        "            return self.df\n",
        "\n",
        "        # Построим маппинг: время -> индекс в общей шкале\n",
        "        scale_index_by_time = {dt_scale[i]: i for i in range(len(dt_scale))}\n",
        "\n",
        "        # Для каждой строки df найдём её индекс в шкале и вычислим счётчик\n",
        "        out = np.full(n, np.nan, dtype=np.float32)\n",
        "\n",
        "        for i, t in enumerate(df_times):\n",
        "            idx_in_scale = scale_index_by_time.get(t)\n",
        "            if idx_in_scale is None:\n",
        "                continue\n",
        "            delta = idx_in_scale - entry_idx_scale + 1\n",
        "            if delta >= 1:\n",
        "                out[i] = float(delta)\n",
        "\n",
        "        self.df[name] = out\n",
        "\n",
        "    def _feat_roc(self, window: int = 5):\n",
        "        name = \"ROC\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = self.df[\"close\"].pct_change(window)\n",
        "\n",
        "    def _feat_atr(self, atr_window: int = 14):\n",
        "        name = \"ATR_norm\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        h,l,c = self.df[\"high\"], self.df[\"low\"], self.df[\"close\"]\n",
        "        tr1 = h-l\n",
        "        tr2 = (h-c.shift()).abs()\n",
        "        tr3 = (l-c.shift()).abs()\n",
        "        tr  = pd.concat([tr1,tr2,tr3], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_window).mean()\n",
        "        self.df[name] = atr/c\n",
        "\n",
        "    def _feat_bb_width(self, bb_window: int = 20):\n",
        "        name = \"BB_Width\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        c   = self.df[\"close\"]\n",
        "        ma  = c.rolling(bb_window).mean()\n",
        "        std = c.rolling(bb_window).std()\n",
        "        self.df[name] = 2*std/ma\n",
        "\n",
        "    def _feat_asset_growth(self, window: int = 3):\n",
        "        name = \"Asset_Growth\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = self.df[\"close\"].pct_change(window).fillna(0)*100\n",
        "\n",
        "    def _feat_ema_acceleration(self, pct_window: int = 3, ema_window: int = 300):\n",
        "        name = \"ema_acceleration\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        r = self.df[\"close\"].pct_change(pct_window).fillna(0)\n",
        "        e = r.ewm(span=ema_window).mean()\n",
        "        self.df[name] = e.diff(4)\n",
        "\n",
        "    def _feat_price_change(self, window: int = 1):\n",
        "        name = \"price_change\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        self.df[name] = self.df[\"close\"].pct_change(window).fillna(0)\n",
        "\n",
        "    def _feat_peak_exhaustion(\n",
        "        self,\n",
        "        price_win: int = 60,    # окно \"локального максимума\"\n",
        "        mom_win:   int = 10,    # окно для momentum\n",
        "        vol_win:   int = 20,\n",
        "        atr_win:   int = 14,\n",
        "        z_win:     int = 100    # z-score нормализация\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Peak-Exhaustion Score  ~ 0…1\n",
        "        1 → почти наверху, импульс затух, объём падает, ATR высок.\n",
        "        \"\"\"\n",
        "        name = \"Peak_Exhaustion_Score\"\n",
        "        c = self.df[\"close\"]\n",
        "\n",
        "        # 1) расстояние до локального max\n",
        "        roll_max = c.rolling(price_win).max()\n",
        "        dist_max = (roll_max - c) / roll_max          # 0 — на max, >0 — ниже\n",
        "\n",
        "        # 2) ослабевающий импульс\n",
        "        roc_now  = c.pct_change(mom_win)\n",
        "        roc_hist = roc_now.rolling(price_win).max()   # max импульса в окне\n",
        "        momentum_div = 1 - (roc_now / (roc_hist + 1e-12))   # 0 → свежий high\n",
        "\n",
        "        # 3) сушащийся объём\n",
        "        vol_ratio = self.df[\"volume\"] / \\\n",
        "            self.df[\"volume\"].rolling(vol_win).mean()\n",
        "\n",
        "        # 4) расширенный спред (ATR/price)\n",
        "        tr  = pd.concat([\n",
        "                self.df[\"high\"]  - self.df[\"low\"],\n",
        "                (self.df[\"high\"] - c.shift()).abs(),\n",
        "                (self.df[\"low\"]  - c.shift()).abs()\n",
        "            ], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_win).mean()\n",
        "        atr_norm = atr / c\n",
        "\n",
        "        # 5) агрегируем, переводим в z-score, squash σ → 0…1\n",
        "        raw = (dist_max + momentum_div + (1/vol_ratio) + atr_norm) / 4\n",
        "        z   = (raw - raw.rolling(z_win).mean()) / (raw.rolling(z_win).std() + 1e-9)\n",
        "        self.df[name] = 1 / (1 + np.exp(-z))   # σ(z)\n",
        "\n",
        "    def _feat_fractal_dim(self, short_win=20, long_win=40):\n",
        "        \"\"\"Вычисляет фрактальную размерность на основе отношения ATR разных периодов\"\"\"\n",
        "        name = \"Fractal_Dim\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "\n",
        "        # Вычисляем ATR для короткого периода\n",
        "        h, l, c = self.df['high'], self.df['low'], self.df['close']\n",
        "        tr1 = h - l\n",
        "        tr2 = (h - c.shift()).abs()\n",
        "        tr3 = (l - c.shift()).abs()\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        atr_short = tr.rolling(short_win).mean()\n",
        "\n",
        "        # Вычисляем ATR для длинного периода\n",
        "        atr_long = tr.rolling(long_win).mean()\n",
        "\n",
        "        # Вычисляем фрактальную размерность\n",
        "        ratio = atr_long / (atr_short + 1e-10)  # Добавляем небольшое значение для избежания деления на 0\n",
        "        self.df[name] = np.log(ratio) / np.log(2)\n",
        "\n",
        "    def _feat_bb_percent(self, window=20, std_mult=2):\n",
        "        name = \"%B_BB\"\n",
        "        if name in self.df: return\n",
        "        ma = self.df[\"close\"].rolling(window).mean()\n",
        "        std = self.df[\"close\"].rolling(window).std()\n",
        "        self.df[name] = (self.df[\"close\"] - (ma - std_mult * std)) / (4 * std)\n",
        "\n",
        "    def _feat_kurtosis_roll(self, window=50):\n",
        "        name = \"Kurtosis_roll\"\n",
        "        if name in self.df: return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        self.df[name] = ret.rolling(window).kurt()\n",
        "\n",
        "    def _feat_obv_div(self, window=10):\n",
        "        name = \"OBV_div\"\n",
        "        if name in self.df: return\n",
        "        sign = np.sign(self.df[\"close\"].diff())\n",
        "        obv = (sign * self.df[\"volume\"]).cumsum()\n",
        "        price_chg = self.df[\"close\"].pct_change(window)\n",
        "        obv_chg = obv.pct_change(window)\n",
        "        self.df[name] = price_chg - obv_chg\n",
        "\n",
        "    def _feat_rsi_slope(self, rsi_p=14, diff_win=5):\n",
        "        name = \"RSI_slope\"\n",
        "        if name in self.df: return\n",
        "        delta = self.df[\"close\"].diff()\n",
        "        gain = delta.clip(lower=0).rolling(rsi_p).mean()\n",
        "        loss = -delta.clip(lower=0).rolling(rsi_p).mean()\n",
        "        rsi = 100 - 100 / (1 + gain / (loss + 1e-10))\n",
        "        self.df[name] = rsi.diff(diff_win)\n",
        "\n",
        "    def _feat_vol_decay(self, window=20):\n",
        "        name = \"Vol_Decay\"\n",
        "        if name in self.df: return\n",
        "        vol_ema = self.df[\"volume\"].ewm(span=window).mean()\n",
        "        self.df[name] = self.df[\"volume\"] / vol_ema - 1\n",
        "\n",
        "    def _feat_accel_decay(self, window=10):\n",
        "        name = \"Accel_Decay\"\n",
        "        if name in self.df: return\n",
        "        vel = self.df[\"close\"].diff(window)\n",
        "        accel = vel.diff(window)\n",
        "        self.df[name] = accel / (vel.abs() + 1e-10)\n",
        "\n",
        "    '''def _feat_entropy_roll(self, window=50):\n",
        "        name = \"Entropy_roll\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "\n",
        "        def _entropy(x):\n",
        "            if len(x) < 2:\n",
        "                return 0\n",
        "            hist = np.histogram(x, bins='auto')[0]\n",
        "            hist = hist / hist.sum()  # Нормализуем\n",
        "            return -np.sum(hist * np.log(hist + 1e-10))\n",
        "\n",
        "        self.df[name] = ret.rolling(window).apply(_entropy, raw=True)'''\n",
        "\n",
        "    def _feat_entropy_roll(self, window: int = 50):\n",
        "        name = \"Entropy_roll\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0.0).to_numpy(dtype=np.float32)\n",
        "        ent = _rolling_entropy_exact_numba(ret, window)\n",
        "        self.df[name] = ent\n",
        "\n",
        "    def _feat_wavelet_var(self, short_win=10, long_win=50):\n",
        "        name = \"Wavelet_Var_Ratio\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        var_short = ret.rolling(short_win).var()\n",
        "        var_long = ret.rolling(long_win).var()\n",
        "        self.df[name] = var_short / (var_long + 1e-10)\n",
        "\n",
        "    '''def _feat_autocorr(self, window=50):\n",
        "        name = \"Autocorr_Lag1\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        self.df[name] = ret.rolling(window).apply(lambda x: x.autocorr(lag=1) if len(x) > 1 else 0, raw=False)'''\n",
        "\n",
        "    def _feat_autocorr(self, window=50):\n",
        "        name = \"Autocorr_Lag1\"\n",
        "        if name in self.df:\n",
        "            return\n",
        "        ret = self.df[\"close\"].pct_change().fillna(0)\n",
        "        arr = ret.to_numpy()  # Use to_numpy() for compatibility\n",
        "        autocorrs = rolling_autocorr(arr, window)\n",
        "        self.df[name] = autocorrs\n",
        "\n",
        "    def _feat_beta(self, window=50):\n",
        "        name = \"Beta_Market\"\n",
        "        if name in self.df or \"market_close\" not in self.df:\n",
        "            return\n",
        "        ret_stock = self.df[\"close\"].pct_change().fillna(0)\n",
        "        ret_market = self.df[\"market_close\"].pct_change().fillna(0)\n",
        "        cov = ret_stock.rolling(window).cov(ret_market)\n",
        "        var_market = ret_market.rolling(window).var()\n",
        "        self.df[name] = cov / (var_market + 1e-10)\n",
        "\n",
        "    def _feat_peak_squeeze_curvature(self,\n",
        "                                vel_win: int = 5,\n",
        "                                acc_win: int = 5,\n",
        "                                vol_win: int = 20,\n",
        "                                atr_win: int = 14,\n",
        "                                z_win : int = 60):\n",
        "        \"\"\"\n",
        "        Возвращает 3 колонки:\n",
        "        PSC_raw, PSC_z, PSC_sigmoid ∈ [0,1]\n",
        "        \"\"\"\n",
        "        name = \"PSC\"\n",
        "        cols_need = {\"PSC_raw\",\"PSC_z\",\"PSC_sigmoid\"}\n",
        "        if cols_need.issubset(self.df.columns): return\n",
        "        c = self.df['close']\n",
        "\n",
        "        # 1) speed & accel\n",
        "        speed  = c.pct_change(vel_win).fillna(0)\n",
        "        accel  = speed.diff(acc_win).fillna(0)\n",
        "        curvature = accel / (speed.abs() + 1e-10)\n",
        "\n",
        "        # 2) squeeze = ATR_norm ↘ & HV_norm ↘\n",
        "        h,l = self.df['high'], self.df['low']\n",
        "        tr = pd.concat([h-l, (h-c.shift()).abs(), (l-c.shift()).abs()], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_win).mean()\n",
        "        hv  = c.pct_change().rolling(vol_win).std()\n",
        "        squeeze = - (atr / (c+1e-10)).diff().clip(upper=0)   # падение ATR\n",
        "        squeeze += - hv.diff().clip(upper=0)                 # падение HV\n",
        "        squeeze /= 2\n",
        "\n",
        "        # 3) агрегируем\n",
        "        raw = 0.6*curvature + 0.4*squeeze\n",
        "\n",
        "        # 4) z-score + σ(z)\n",
        "        mu  = raw.rolling(z_win).mean()\n",
        "        std = raw.rolling(z_win).std()\n",
        "        z = (raw - mu)/(std + 1e-9)\n",
        "        sigm = 1/(1+np.exp(-z))\n",
        "\n",
        "        self.df[f'{name}_raw']     = raw\n",
        "        self.df[f'{name}_z']       = z.clip(-5, 5)\n",
        "        self.df[f'{name}_sigmoid'] = sigm\n",
        "\n",
        "\n",
        "def calculate_metrics(test_data, y_test, y_pred, target_column='normalized_target'):\n",
        "    \"\"\"\n",
        "    Функция для расчета метрик по данным теста и предсказаниям модели.\n",
        "\n",
        "    Параметры:\n",
        "    - test_data: pd.DataFrame — тестовые данные с колонками batch, high, close и другими.\n",
        "    - y_test: pd.Series или np.array — фактические значения целевой переменной.\n",
        "    - y_pred: pd.Series или np.array — предсказанные моделью значения.\n",
        "    - target_column: str — название колонки целевой переменной в test_data.\n",
        "\n",
        "    Возвращает:\n",
        "    - avg_mse: float — среднеквадратическая ошибка.\n",
        "    - avg_r2: float — средняя R².\n",
        "    - std_r2: float — стандартное отклонение R².\n",
        "    - corr_mean: float — средняя корреляция.\n",
        "    - corr_std: float — стандартное отклонение корреляции.\n",
        "    - avg_missed: float — средний процент упущенной прибыли.\n",
        "    \"\"\"\n",
        "    # Инициализация метрик\n",
        "    mse_scores, r2_scores, corr_scores, missed_pnl = [], [], [], []\n",
        "\n",
        "    # Расчет корреляции целевой переменной и предсказаний\n",
        "    y_pred_series = pd.Series(y_pred, index=y_test.index)\n",
        "    corr_score = test_data[target_column].corr(y_pred_series)\n",
        "    corr_scores.append(corr_score)\n",
        "\n",
        "    # MSE и R2\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "    r2_scores.append(r2_score(y_test, y_pred))\n",
        "\n",
        "    # Расчет missed_pnl для каждого batch\n",
        "    \"\"\"for batch in test_data['batch'].unique():\n",
        "        mask = test_data['batch'] == batch\n",
        "        max_high = test_data.loc[mask, 'high'].max()\n",
        "        pred = y_pred[mask]  # предполагается, что y_pred соответствует normalized_target\n",
        "        sell_idx = np.argmin(pred)  # продажа на минимальном предсказанном значении\n",
        "        sell_price = test_data.loc[mask].iloc[sell_idx]['close']\n",
        "        missed = (max_high - sell_price) / (max_high - test_data.loc[mask].iloc[0]['close'])  # % упущенной прибыли\n",
        "        missed_pnl.append(missed)\"\"\"\n",
        "\n",
        "    # Усреднение метрик\n",
        "    avg_mse = float(np.mean(mse_scores))\n",
        "    avg_r2 = float(np.mean(r2_scores))\n",
        "    std_r2 = float(np.std(r2_scores))\n",
        "    corr_mean = float(np.mean(corr_scores))\n",
        "    corr_std = float(np.std(corr_scores))\n",
        "    #avg_missed = float(np.mean(missed_pnl))\n",
        "\n",
        "    # Проверка на корректность результатов\n",
        "    if np.isfinite([avg_mse, avg_r2, std_r2, corr_mean, corr_std]).all(): #avg_missed\n",
        "        return avg_mse, avg_r2, std_r2, corr_mean, corr_std, #avg_missed\n",
        "    else:\n",
        "        return float('inf'), float('inf'), float('inf'), float('inf'), float('inf'), float('inf')\n",
        "\n",
        "def prepare_data(df, target_col):\n",
        "    \"\"\"\n",
        "    Подготавливает данные: разделяет на числовые и категориальные признаки, создает конвейер преобразования.\n",
        "    \"\"\"\n",
        "    if type(target_col) == str:\n",
        "        df.dropna(inplace=True)\n",
        "        X = df.drop([target_col, 'batch'], axis=1)\n",
        "        y = df[target_col]\n",
        "    elif type(target_col) == list:\n",
        "        df.dropna(inplace=True)\n",
        "        X = df.drop(target_col+['batch'], axis=1)\n",
        "        y = df[target_col]\n",
        "\n",
        "    # Разделение на числовые и категориальные признаки\n",
        "    numeric_features = X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
        "    categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Создание конвейера преобразования\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('normalize', PowerTransformer(method='yeo-johnson')),\n",
        "            ]), numeric_features),\n",
        "            ('cat', Pipeline([\n",
        "                ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
        "            ]), categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return X, y, preprocessing\n",
        "\n",
        "\n",
        "def calculate_indicators(df, features, params=None, mode=None, multy=False):\n",
        "\n",
        "    fc = FeatureCalculatorForRegression(df)\n",
        "    if mode==None:\n",
        "        df_features = fc.calculate_features(params=params, required_features=features)\n",
        "    else:\n",
        "        df_features = fc.calculate_all_possible_features()\n",
        "    if multy == False:\n",
        "        df1 = df_features[df_features['normalized_target'].notna()]\n",
        "    else:\n",
        "        df1 = df_features[df_features['multi_target_5'].notna()]\n",
        "    features = ['open', 'close', 'high', 'low', 'volume', 'buy_signal', 'sell_signal','event_sell_time','event_sell_price',\n",
        "                'event_time','event_price','event_sell_time','event_sell_price','target', 'pnl', 'ma', 'pmax'] #time\n",
        "    df1['regime'] = df1['regime'].astype('object')\n",
        "    df1[['batch','trade_bars_counter']] = df1[['batch', 'trade_bars_counter']].astype('int')\n",
        "    df1 = df1.drop(features, axis=1)\n",
        "    #df1 = df1.dropna()\n",
        "    return df1, fc._timings\n",
        "\n",
        "def sample_feature_params(params) -> dict:\n",
        "        \"\"\"\n",
        "        Draws *one* sample of the whole feature-engineering hyper-parameter set.\n",
        "        Rule of thumb for ranges:\n",
        "          • lower bound = ‘sane minimum‘ from domain knowledge\n",
        "          • upper bound = ‘sane maximum’\n",
        "        Adjust them if you feel the search space is too wide or too narrow.\n",
        "        \"\"\"\n",
        "        # ---- helpers for monotone constraints ----------------------------------\n",
        "        fast  = params['macd_fast']\n",
        "        slow  = params['macd_slow']\n",
        "\n",
        "        slope_lag_min = params['slope_lag_min']\n",
        "        slope_lag     = params['slope_lag']\n",
        "\n",
        "        # ---- finally compose the nested dict -----------------------------------\n",
        "        return {\n",
        "            'base': {\n",
        "                'medprice': params['medprice']\n",
        "            },\n",
        "            'macd': {\n",
        "                'fast'      : fast,\n",
        "                'slow'      : slow,\n",
        "                'signal'    : params['macd_signal'],\n",
        "                'macd_roll' : params['macd_roll']\n",
        "            },\n",
        "            'overbought': {\n",
        "                'rsi_p'         : params['rsi_p'],\n",
        "                'stoch_p'       : params['stoch_p'],\n",
        "                'oversold_roll' : params['oversold_roll']\n",
        "            },\n",
        "            'madist': {\n",
        "                'span_lenght'   : params['madist_span'],\n",
        "                'madist_lenght' : params['madist_len']\n",
        "            },\n",
        "            'mean_reversion': {\n",
        "                'window' : params['mr_window']\n",
        "            },\n",
        "            'fear_greed': {\n",
        "                'greed_pct'    : params['fg_greed_pct'],\n",
        "                'volume_ratio_scr' : params['fg_vol_ratio'],\n",
        "                'window'       : params['fg_window'],\n",
        "                'greed_roll'   : params['fg_roll']\n",
        "            },\n",
        "            'price_variation': {\n",
        "                'variation_lenght': params['pv_len']\n",
        "            },\n",
        "            'pmax_ma': {\n",
        "                'pmax_ma_lenght'      : params['pmax_len'],\n",
        "                'pmax_ma_lenght_roll' : params['pmax_roll']\n",
        "            },\n",
        "            'slope': {\n",
        "                'slope_lag'     : slope_lag,\n",
        "                'slope_lag_min' : slope_lag_min,\n",
        "                'sloap_pct'     : params['slope_pct'],\n",
        "                'sloap_roll'    : params['slope_roll']\n",
        "            },\n",
        "            # _trade_duration_features – no params\n",
        "        }\n",
        "\n",
        "def build_feature_params(\n",
        "    flat_params: Dict[str, Any],\n",
        "    extra_alias: Optional[Dict[str, Tuple[str, str | None]]] = None\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Преобразует «плоский» словарь от Optuna в структуру,\n",
        "    которую понимает FeatureCalculatorForRegression.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Базовая явная таблица соответствий\n",
        "    alias: Dict[str, Tuple[str, str | None]] = {\n",
        "        'hp_lamb'          : ('hp_trend'           , 'lamb'),\n",
        "        'ea_pct'           : ('ema_acceleration'   , 'pct_window'),\n",
        "        'ea_ema'           : ('ema_acceleration'   , 'ema_window'),\n",
        "        'mr_window'        : ('Mean_Reversion'     , 'window'),\n",
        "        'ag_window'        : ('Asset_Growth'       , 'window'),\n",
        "        'medprice'         : ('MEDPRICE'           , 'medprice'),\n",
        "        'bb_window'        : ('BB_Width'           , 'bb_window'),\n",
        "        'macd_fast'        : ('MACD'               , 'fast'),\n",
        "        'macd_slow'        : ('MACD'               , 'slow'),\n",
        "        'macd_signal'      : ('MACD'               , 'signal'),\n",
        "        'fg_window'        : ('Fear_Greed'         , 'window'),\n",
        "        'atr_window'       : ('ATR_norm'           , 'atr_window'),\n",
        "        'vr_window'        : ('volume_ratio'       , 'window'),\n",
        "        'madist_span'      : ('Price_MADist%'      , 'span_lenght'),\n",
        "        'slope_lag'        : ('slope_trend'        , 'slope_lag'),\n",
        "        'slope_lag_min'    : ('slope_trend'        , 'slope_lag'),\n",
        "        'rsi_p'            : ('Overbought_Oversold', 'rsi_p'),\n",
        "        'stoch_p'          : ('Overbought_Oversold', 'stoch_p'),\n",
        "        'pmax_len'         : ('pmax_norm'          , 'pmax_ma_length'),\n",
        "        'pmax_roll'        : ('pmax_norm'          , 'pmax_ma_length_roll'),\n",
        "        'pc_window'        : ('pmax_norm'          , 'pct_window'),\n",
        "        'ema_trend_span'   : ('ema_trend'          , 'span'),\n",
        "        'ema_trend_pct'    : ('ema_trend'          , 'pct_window'),\n",
        "        'stat_window'      : ('stat_window', None),\n",
        "        # --- новые алиасы ---\n",
        "        'roc_window'        : ('ROC'          , 'window'),\n",
        "        'willr_window'      : ('WILLR'          , 'window'),\n",
        "        'fractal_short_win' : ('Fractal_Dim', 'short_win'),\n",
        "        'fractal_long_win'  : ('Fractal_Dim', 'long_win'),\n",
        "        'peak_price_win'    : ('Peak_Exhaustion_Score', 'price_win'),\n",
        "        'peak_mom_win'      : ('Peak_Exhaustion_Score', 'mom_win'),\n",
        "        'peak_vol_win'      : ('Peak_Exhaustion_Score', 'vol_win'),\n",
        "        'peak_atr_win'      : ('Peak_Exhaustion_Score', 'atr_win'),\n",
        "        'peak_z_win'        : ('Peak_Exhaustion_Score', 'z_win'),\n",
        "        'bb_window'         : ('%B_BB', 'window'),\n",
        "        'bb_std_mult'       : ('%B_BB', 'std_mult'),\n",
        "        'kurt_window'       : ('Kurtosis_roll', 'window'),\n",
        "        'obv_window'        : ('OBV_div', 'window'),\n",
        "        'rsi_slope_rsi_p'   : ('RSI_slope', 'rsi_p'),\n",
        "        'rsi_diff_win'      : ('RSI_slope', 'diff_win'),\n",
        "        'voldec_window'     : ('Vol_Decay', 'window'),\n",
        "        'acceldec_window'   : ('Accel_Decay', 'window'),\n",
        "        'ent_window'        : ('Entropy_roll', 'window'),\n",
        "        'wlt_short_win'     : ('Wavelet_Var_Ratio', 'short_win'),\n",
        "        'wlt_long_win'      : ('Wavelet_Var_Ratio', 'long_win'),\n",
        "        'acorr_window'      : ('Autocorr_Lag1', 'window'),\n",
        "        'beta_window'       : ('Beta_Market', 'window'),\n",
        "        'psc_vel_win'       : ('PSC', 'vel_win'),\n",
        "        'psc_acc_win'       : ('PSC', 'acc_win'),\n",
        "        'psc_vol_win'       : ('PSC', 'vol_win'),\n",
        "        'psc_atr_win'       : ('PSC', 'atr_win'),\n",
        "        'psc_z_win'         : ('PSC', 'z_win'),\n",
        "    }\n",
        "\n",
        "    # 2. Пользовательские переопределения\n",
        "    if extra_alias:\n",
        "        alias.update(extra_alias)\n",
        "\n",
        "    # 3. Автоматический разбор префиксов (fallback)\n",
        "    prefix_map: Dict[str, str] = {\n",
        "        'macd'        : 'MACD',\n",
        "        'hp'          : 'hp_trend',\n",
        "        'ea'          : 'ema_acceleration',\n",
        "        'mr'          : 'Mean_Reversion',\n",
        "        'ag'          : 'Asset_Growth',\n",
        "        'bb'          : 'BB_Width',\n",
        "        'fg'          : 'Fear_Greed',\n",
        "        'atr'         : 'ATR_norm',\n",
        "        'vr'          : 'volume_ratio',\n",
        "        'madist'      : 'Price_MADist%',\n",
        "        'slope'       : 'slope_trend',\n",
        "        'pmax'        : 'pmax_norm',\n",
        "        'ema_trend'   : 'ema_trend',\n",
        "        'rsi'         : 'Overbought_Oversold',\n",
        "        'stoch'       : 'Overbought_Oversold',\n",
        "        'fractal'     : 'Fractal_Dim',\n",
        "        'peak'        : 'Peak_Exhaustion_Score',\n",
        "        'bb'          : '%B_BB',\n",
        "        'kurt'        : 'Kurtosis_roll',\n",
        "        'obv'         : 'OBV_div',\n",
        "        'rsi_slope'   : 'RSI_slope',\n",
        "        'voldec'      : 'Vol_Decay',\n",
        "        'acceldec'    : 'Accel_Decay',\n",
        "        'entropy'     : 'Entropy_roll',\n",
        "        'wavelet'     : 'Wavelet_Var_Ratio',\n",
        "        'acorr'       : 'Autocorr_Lag1',\n",
        "        'beta'        : 'Beta_Market',\n",
        "        'psc'         : 'PSC',\n",
        "    }\n",
        "\n",
        "    nested: Dict[str, Dict[str, Any]] = defaultdict(dict)\n",
        "\n",
        "    for key, val in flat_params.items():\n",
        "\n",
        "        # 3.1 Явное соответствие\n",
        "        if key in alias:\n",
        "            prim, arg = alias[key]\n",
        "            if prim == 'stat_window' or arg is None:\n",
        "                nested['stat_window'] = val\n",
        "            else:\n",
        "                nested[prim][arg] = val\n",
        "            continue\n",
        "\n",
        "        # 3.2 Игнорируем вспомогательные ключи вида *_min, *_max, если\n",
        "        #     они не нужны никакому примитиву.\n",
        "        if key.endswith('_min') or key.endswith('_max'):\n",
        "            continue\n",
        "\n",
        "        # 3.3 Fallback-разбор _\n",
        "        if '_' in key:\n",
        "            prefix, arg = key.split('_', 1)\n",
        "            if prefix in prefix_map:\n",
        "                nested[prefix_map[prefix]][arg] = val\n",
        "                continue\n",
        "\n",
        "        # 3.4 Неизвестный ключ — игнорируем или логируем\n",
        "        # print(f'Warning: parameter \"{key}\" was not mapped')\n",
        "\n",
        "    return {p: d for p, d in nested.items()}\n",
        "\n",
        "_ORIG_INTERP = F.interpolate\n",
        "\n",
        "\n",
        "def _collapse_pred_to_bt(y_pred: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Приводим предсказание к виду (B, T), считая последнюю ось временем (горизонтом).\n",
        "    Все промежуточные оси (кроме batch=ось 0 и time=последняя ось) усредняем.\n",
        "    Пример: (B, 1, 4, 10) -> mean по осям (1,2) -> (B,10)\n",
        "    (B, 10) -> ок\n",
        "    (B, 1, 10) -> squeeze -> (B,10)\n",
        "    \"\"\"\n",
        "    if not isinstance(y_pred, torch.Tensor):\n",
        "        raise TypeError(f\"y_pred must be a tensor, got {type(y_pred)}\")\n",
        "\n",
        "    # Сначала уберём все единичные оси\n",
        "    if any(s == 1 for s in y_pred.shape[1:-1]):\n",
        "        # squeeze не трогает последнюю ось, если она не равна 1\n",
        "        y_pred = y_pred.squeeze()\n",
        "        # Если squeeze убрал не только единичные, но и привёл к (B, T) — хорошо.\n",
        "\n",
        "    if y_pred.dim() == 1:\n",
        "        # (B,) — интерпретируем как T=1, сделаем (B,1)\n",
        "        y_pred = y_pred.unsqueeze(-1)\n",
        "        return y_pred\n",
        "\n",
        "    if y_pred.dim() == 2:\n",
        "        # (B, T) — уже как надо\n",
        "        return y_pred\n",
        "\n",
        "    # Если размерностей больше 2: считаем last dim = time, batch = 0\n",
        "    # Все промежуточные оси схлопываем усреднением\n",
        "    reduce_dims = tuple(range(1, y_pred.dim() - 1))\n",
        "    if len(reduce_dims) > 0:\n",
        "        y_pred = y_pred.mean(dim=reduce_dims)\n",
        "    # На выходе (B, T)\n",
        "    if y_pred.dim() != 2:\n",
        "        # На всякий случай добьёмся (B, T)\n",
        "        y_pred = y_pred.view(y_pred.size(0), -1)\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def _install_safe_interpolate_patch():\n",
        "    \"\"\"\n",
        "    Патч делает F.interpolate детерминированным при включённом torch.use_deterministic_algorithms(True)\n",
        "    для CUDA и режимов linear/bilinear/bicubic, прогоняя вычисление на CPU.\n",
        "    Идемпотентен и не меняет сигнатуру.\n",
        "    \"\"\"\n",
        "    if getattr(F.interpolate, \"_is_deterministic_wrapper\", False):\n",
        "        return\n",
        "\n",
        "    _orig_interpolate = F.interpolate\n",
        "\n",
        "    # Какие режимы считаем потенциально недетерминируемыми на CUDA\n",
        "    _CUDA_UNSAFE_MODES = {\"linear\", \"bilinear\", \"bicubic\"}  # 1d/2d/2d\n",
        "\n",
        "    def _needs_cpu_fallback(input, mode):\n",
        "        if not torch.is_tensor(input):\n",
        "            return False\n",
        "        if input.is_cuda and mode in _CUDA_UNSAFE_MODES:\n",
        "            # upsample_linear1d_backward_out_cuda и др. — недетерминируемы\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    @functools.wraps(_orig_interpolate)\n",
        "    def _deterministic_interpolate(\n",
        "        input: torch.Tensor,\n",
        "        size=None,\n",
        "        scale_factor=None,\n",
        "        mode=\"nearest\",\n",
        "        align_corners=None,\n",
        "        recompute_scale_factor=None,\n",
        "        antialias=False,\n",
        "    ):\n",
        "        # Если не нужно, просто вызовем оригинал\n",
        "        if not _needs_cpu_fallback(input, mode):\n",
        "            return _orig_interpolate(\n",
        "                input,\n",
        "                size=size,\n",
        "                scale_factor=scale_factor,\n",
        "                mode=mode,\n",
        "                align_corners=align_corners,\n",
        "                recompute_scale_factor=recompute_scale_factor,\n",
        "                antialias=antialias,\n",
        "            )\n",
        "\n",
        "        # CUDA + linear/bilinear/bicubic → CPU fallback\n",
        "        x = input\n",
        "        dev = x.device\n",
        "        orig_dtype = x.dtype\n",
        "\n",
        "        # Для стабильности переводим в float32 на CPU\n",
        "        x_cpu = x.detach().to(\"cpu\", dtype=torch.float32).requires_grad_(x.requires_grad)\n",
        "\n",
        "        y_cpu = _orig_interpolate(\n",
        "            x_cpu,\n",
        "            size=size,\n",
        "            scale_factor=scale_factor,\n",
        "            mode=mode,\n",
        "            align_corners=align_corners,\n",
        "            recompute_scale_factor=recompute_scale_factor,\n",
        "            antialias=antialias,\n",
        "        )\n",
        "\n",
        "        # Возвращаем на исходное устройство и тип\n",
        "        y = y_cpu.to(dev, dtype=orig_dtype)\n",
        "\n",
        "        return y\n",
        "\n",
        "    _deterministic_interpolate._is_deterministic_wrapper = True  # type: ignore[attr-defined]\n",
        "    F.interpolate = _deterministic_interpolate\n",
        "\n",
        "\n",
        "_install_safe_interpolate_patch()\n",
        "\n",
        "\n",
        "def _unpack_pf_batch(batch):\n",
        "    \"\"\"\n",
        "    Унифицированная распаковка батча из TimeSeriesDataSet.to_dataloader(...)\n",
        "    Возвращает: x (dict), y (Tensor|None), weight (Tensor|None)\n",
        "    \"\"\"\n",
        "    if isinstance(batch, (list, tuple)):\n",
        "        if len(batch) == 3:\n",
        "            x, y, weight = batch\n",
        "        elif len(batch) == 2:\n",
        "            x, y = batch\n",
        "            weight = None\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected batch tuple length: {len(batch)}\")\n",
        "    elif isinstance(batch, dict):\n",
        "        # На всякий случай поддержим dict → возьмём таргет из decoder_target, если есть\n",
        "        x = batch\n",
        "        y = batch.get(\"decoder_target\", None)\n",
        "        weight = None\n",
        "    else:\n",
        "        raise TypeError(f\"Unexpected batch type: {type(batch)}\")\n",
        "    return x, y, weight\n",
        "\n",
        "\n",
        "# F.interpolate = _deterministic_interpolate\n",
        "\n",
        "\n",
        "def _extract_pred_tensor(y_pred):\n",
        "    # извлекаем тензор предикта из любых обёрток\n",
        "    if isinstance(y_pred, dict):\n",
        "        for key in (\"prediction\", \"output\", \"decoder_output\"):\n",
        "            if key in y_pred and torch.is_tensor(y_pred[key]):\n",
        "                return y_pred[key]\n",
        "        # если не нашли — попробуем fallback: первый тензор в dict\n",
        "        for v in y_pred.values():\n",
        "            if torch.is_tensor(v):\n",
        "                return v\n",
        "        raise ValueError(\"Could not extract prediction tensor from dict y_pred.\")\n",
        "\n",
        "    if isinstance(y_pred, (list, tuple)):\n",
        "        # обычно y_pred[0] — предсказание\n",
        "        return y_pred[0]\n",
        "\n",
        "    if torch.is_tensor(y_pred):\n",
        "        return y_pred\n",
        "\n",
        "    raise TypeError(f\"Unsupported y_pred type: {type(y_pred)}\")\n",
        "\n",
        "\n",
        "class EventTimeSeriesSplit(BaseCrossValidator):\n",
        "    \"\"\"\n",
        "    Кросс-валидация по событиям (batch), хронологическая, с эмбарго.\n",
        "    groups: массив той же длины, что и df, со значениями batch\n",
        "    times:  массив pd.Timestamp (или sortable), та же длина, что и df\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_splits: int = 5, embargo_events: int = 1, min_train_events: int = 5):\n",
        "        self.n_splits = n_splits\n",
        "        self.embargo_events = embargo_events\n",
        "        self.min_train_events = min_train_events\n",
        "\n",
        "    def get_n_splits(self, X=None, y=None, groups=None):\n",
        "        return self.n_splits\n",
        "\n",
        "    def split(self, X, y=None, groups=None, times: Optional[pd.Series] = None) -> Iterator[\n",
        "        Tuple[np.ndarray, np.ndarray]]:\n",
        "        if groups is None or times is None:\n",
        "            raise ValueError(\"Pass groups=batch and times=time columns\")\n",
        "\n",
        "        groups = np.asarray(groups)\n",
        "        times = pd.to_datetime(times)\n",
        "\n",
        "        # порядок событий по старт-времени\n",
        "        df_tmp = pd.DataFrame({\"group\": groups, \"time\": times}).reset_index(names=\"row_idx\")\n",
        "        first_time = df_tmp.groupby(\"group\")[\"time\"].min().sort_values()\n",
        "        uniq_groups = first_time.index.to_numpy()\n",
        "\n",
        "        n_events = len(uniq_groups)\n",
        "        if n_events < (self.n_splits + self.min_train_events):\n",
        "            # уменьшаем число сплитов, если событий мало\n",
        "            eff_splits = max(1, n_events - self.min_train_events)\n",
        "        else:\n",
        "            eff_splits = self.n_splits\n",
        "\n",
        "        # на каждой итерации расширяем train вправо\n",
        "        for split_idx in range(1, eff_splits + 1):\n",
        "            # доля событий для валидации\n",
        "            val_events = max(1, n_events // (eff_splits + 1))\n",
        "            train_end = n_events - (eff_splits - split_idx + 1) * val_events\n",
        "\n",
        "            if train_end < self.min_train_events:\n",
        "                continue\n",
        "\n",
        "            # эмбарго\n",
        "            embargoed_end = max(0, train_end - self.embargo_events)\n",
        "\n",
        "            train_groups = uniq_groups[:embargoed_end]\n",
        "            val_groups = uniq_groups[train_end: train_end + val_events]\n",
        "\n",
        "            train_idx = df_tmp.index[df_tmp[\"group\"].isin(train_groups)].to_numpy()\n",
        "            val_idx = df_tmp.index[df_tmp[\"group\"].isin(val_groups)].to_numpy()\n",
        "\n",
        "            # индексы исходной X (если это DataFrame — у вас совпадают позиции с row_idx)\n",
        "            yield (train_idx, val_idx)\n",
        "\n",
        "\n",
        "class MinimalRichProgressBar(RichProgressBar):\n",
        "    def on_validation_start(self, trainer, pl_module):\n",
        "        pass\n",
        "\n",
        "    def on_validation_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        pass\n",
        "\n",
        "\n",
        "class NoValidationBar(TQDMProgressBar):\n",
        "    def init_validation_tqdm(self):\n",
        "        # возвращаем полностью отключённый tqdm для валидации\n",
        "        return tqdm_class(disable=True)\n",
        "\n",
        "class CustomTFT(TemporalFusionTransformer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.mask_prob = kwargs.pop(\"mask_prob\", 0.05)\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._val_preds = []\n",
        "        self._val_trues = []\n",
        "        self._val_gids = []\n",
        "        self.scheduled_prob = 0.0\n",
        "\n",
        "        safe_val = torch.tensor(\n",
        "            torch.finfo(torch.float16).min,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        m = self.multihead_attn.attention\n",
        "        if hasattr(m, \"mask_bias\") and not isinstance(m.mask_bias, torch.Tensor):\n",
        "            delattr(m, \"mask_bias\")\n",
        "        m.register_buffer(\"mask_bias\", safe_val)\n",
        "\n",
        "    def on_epoch_start(self, trainer, pl_module):\n",
        "        if trainer.max_epochs > 0:\n",
        "            self.scheduled_prob = min(1.0, trainer.current_epoch / (trainer.max_epochs * 0.8))\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y, weight = _unpack_pf_batch(batch)\n",
        "\n",
        "        if torch.rand(1).item() < self.mask_prob and \"encoder_target\" in x:\n",
        "            enc = x[\"encoder_target\"]\n",
        "            noise = torch.normal(0, 0.15, size=enc.shape, device=enc.device)\n",
        "            x = {**x, \"encoder_target\": noise}\n",
        "            del enc, noise\n",
        "\n",
        "        if torch.rand(1).item() < self.scheduled_prob and y is not None:\n",
        "            with torch.no_grad():\n",
        "                out = self(x)\n",
        "                y_pred = self.loss.to_prediction(out)\n",
        "                y_bt = _collapse_pred_to_bt(y_pred)\n",
        "                dec_tgt = y_bt if y_bt.dim() == 2 else y_bt.unsqueeze(-1)\n",
        "                x['decoder_target'] = dec_tgt.detach()\n",
        "                del out, y_pred, y_bt, dec_tgt\n",
        "\n",
        "        batch = (x, y, weight) if weight is not None else (x, y)\n",
        "        result = super().training_step(batch, batch_idx)\n",
        "\n",
        "        if batch_idx % 50 == 0:  # Rare for speed\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y, weight = _unpack_pf_batch(batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out_temp = self(x)\n",
        "            y_pred_temp = self.loss.to_prediction(out_temp)\n",
        "            y_bt_temp = _collapse_pred_to_bt(y_pred_temp)\n",
        "            del out_temp, y_pred_temp\n",
        "\n",
        "        if \"decoder_target\" in x:\n",
        "            enc_tgt = x.get(\"encoder_target\")\n",
        "            if enc_tgt is not None:\n",
        "                batch_mean = enc_tgt.mean(dim=-1, keepdim=True)\n",
        "                batch_std = enc_tgt.std(dim=-1, keepdim=True) + 1e-8\n",
        "                mean_tensor = batch_mean.expand_as(x[\"decoder_target\"])\n",
        "                std_tensor = (0.1 * batch_std).expand_as(x[\"decoder_target\"])\n",
        "                noise_fill = torch.normal(mean_tensor, std_tensor)\n",
        "                del batch_mean, std_tensor, mean_tensor\n",
        "            else:\n",
        "                dec_tgt = x[\"decoder_target\"]\n",
        "                device = dec_tgt.device\n",
        "                noise_fill = torch.full_like(dec_tgt, self.global_target_mean)\n",
        "                batch_size = dec_tgt.size(0)\n",
        "                batch_std = torch.full((batch_size,), self.global_target_std, device=device).unsqueeze(-1)\n",
        "\n",
        "            if torch.rand(1).item() < self.scheduled_prob:\n",
        "                decoder_fill = y_bt_temp\n",
        "            else:\n",
        "                jitter_size = noise_fill.shape\n",
        "                additional_jitter = torch.randn(jitter_size, device=noise_fill.device) * (0.05 * batch_std.expand_as(noise_fill))\n",
        "                decoder_fill = noise_fill + additional_jitter\n",
        "                del additional_jitter, noise_fill\n",
        "\n",
        "            decoder_fill = torch.clamp(decoder_fill, -1.0, 1.0)\n",
        "            x[\"decoder_target\"] = decoder_fill\n",
        "            del y_bt_temp, batch_std\n",
        "\n",
        "        out = self(x)\n",
        "        y_pred_raw = self.loss.to_prediction(out)\n",
        "        del out\n",
        "\n",
        "        y_pred_metrics = torch.clamp(y_pred_raw, -1.0, 1.0)\n",
        "        del y_pred_raw\n",
        "\n",
        "        y_actual = y if y is not None else x.get(\"decoder_target\")\n",
        "        if y_actual is None:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            y_pred_aligned, y_actual_aligned = _align_pred_target(y_pred_metrics, y_actual)\n",
        "            del y_pred_metrics, y_actual\n",
        "        except Exception:\n",
        "            return\n",
        "\n",
        "        self._val_preds.append(y_pred_aligned.detach())\n",
        "        self._val_trues.append(y_actual_aligned.detach())\n",
        "        del y_pred_aligned, y_actual_aligned\n",
        "\n",
        "        gid = x.get(\"group_ids\")\n",
        "        if gid is not None and isinstance(gid, torch.Tensor):\n",
        "            self._val_gids.append(gid.detach())\n",
        "        else:\n",
        "            self._val_gids.append(None)\n",
        "\n",
        "        if batch_idx % 50 == 0:  # Rare for speed\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        if len(self._val_preds) == 0:\n",
        "            return\n",
        "\n",
        "        yp = torch.cat(self._val_preds, dim=0)\n",
        "        yt = torch.cat(self._val_trues, dim=0)\n",
        "\n",
        "        self._val_preds.clear()\n",
        "        self._val_trues.clear()\n",
        "\n",
        "        se = (yp - yt) ** 2\n",
        "        val_mse = float(se.mean().item())\n",
        "        val_mse_std = float(se.std(unbiased=False).item())\n",
        "        del yp, yt\n",
        "\n",
        "        self.log(\"val_mse\", val_mse, prog_bar=True, on_step=False, on_epoch=True)\n",
        "        self.log(\"val_mse_std\", val_mse_std, prog_bar=False, on_step=False, on_epoch=True)\n",
        "\n",
        "        has_any_gid = any(g is not None for g in self._val_gids)\n",
        "        if has_any_gid:\n",
        "            gid_list = []\n",
        "            valid = True\n",
        "            for g in self._val_gids:\n",
        "                if g is None:\n",
        "                    valid = False\n",
        "                    break\n",
        "                gid_list.append(g)\n",
        "\n",
        "            if valid and len(gid_list) > 0:\n",
        "                gid_all = torch.cat(gid_list, dim=0).numpy().ravel()\n",
        "                se_np = se.numpy().ravel()\n",
        "                del se\n",
        "\n",
        "                if gid_all.shape[0] == se_np.shape[0]:\n",
        "                    uniq = np.unique(gid_all)\n",
        "                    g_mse = [se_np[gid_all == u].mean() for u in uniq if (gid_all == u).any()]\n",
        "                    if len(g_mse) > 0:\n",
        "                        g_mse = np.asarray(g_mse, dtype=float)\n",
        "                        val_mse_group_mean = float(g_mse.mean())\n",
        "                        val_mse_group_std = float(g_mse.std(ddof=0))\n",
        "                        self.log(\"val_mse_group_mean\", val_mse_group_mean, prog_bar=False, on_step=False, on_epoch=True)\n",
        "                        self.log(\"val_mse_group_std\", val_mse_group_std, prog_bar=True, on_step=False, on_epoch=True)\n",
        "                    del g_mse, uniq\n",
        "\n",
        "                del gid_all, se_np\n",
        "\n",
        "        self._val_gids.clear()\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def log(self, name, value, *args, **kwargs):\n",
        "        if value is None:\n",
        "            return\n",
        "        super().log(name, value, *args, **kwargs)\n",
        "\n",
        "\n",
        "def _worker_init_fn(worker_id: int, seed: int):\n",
        "    \"\"\"\n",
        "    Глобальная функция для инициализации worker-а DataLoader-а.\n",
        "    pickle её «видит» и может передать в подпроцессы.\n",
        "    \"\"\"\n",
        "    set_seeds(seed + worker_id)\n",
        "\n",
        "\n",
        "def _extract_tensor(x, role=\"pred\"):\n",
        "    \"\"\"\n",
        "    Извлекает torch.Tensor из различных контейнеров/структур.\n",
        "    - dict: сперва пробуем ключи, характерные для предсказаний/таргета\n",
        "    - tuple/list: берём первый тензор или первый элемент, приводимый к тензору\n",
        "    - tensor: возвращаем как есть\n",
        "    \"\"\"\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "\n",
        "    if isinstance(x, dict):\n",
        "        # Наиболее типичные ключи в pytorch-forecasting / lightning шагах\n",
        "        preferred_keys = [\n",
        "            \"prediction\", \"pred\", \"output\", \"y_pred\", \"yhat\", \"y\", \"target\"\n",
        "        ]\n",
        "        for k in preferred_keys:\n",
        "            if k in x and isinstance(x[k], torch.Tensor):\n",
        "                return x[k]\n",
        "        # Если значения-словари/кортежи — попробуем рекурсивно\n",
        "        for v in x.values():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                return v\n",
        "            if isinstance(v, (list, tuple, dict)):\n",
        "                try:\n",
        "                    t = _extract_tensor(v, role=role)\n",
        "                    if isinstance(t, torch.Tensor):\n",
        "                        return t\n",
        "                except Exception:\n",
        "                    pass\n",
        "        raise TypeError(f\"Cannot extract tensor from dict for role={role}. Keys={list(x.keys())}\")\n",
        "\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        for item in x:\n",
        "            if isinstance(item, torch.Tensor):\n",
        "                return item\n",
        "        # если нет прямого тензора — попробуем рекурсивно\n",
        "        for item in x:\n",
        "            if isinstance(item, (list, tuple, dict)):\n",
        "                try:\n",
        "                    t = _extract_tensor(item, role=role)\n",
        "                    if isinstance(t, torch.Tensor):\n",
        "                        return t\n",
        "                except Exception:\n",
        "                    pass\n",
        "        raise TypeError(f\"Cannot extract tensor from {type(x)} for role={role}\")\n",
        "\n",
        "    # Последняя попытка — у объектов некоторых библиотек есть .values или .tensor\n",
        "    for attr in (\"values\", \"tensor\", \"data\"):\n",
        "        if hasattr(x, attr):\n",
        "            v = getattr(x, attr)\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                return v\n",
        "\n",
        "    raise TypeError(f\"Unsupported type for tensor extraction (role={role}): {type(x)}\")\n",
        "\n",
        "\n",
        "def _maybe_squeeze_last(x):\n",
        "    \"\"\"\n",
        "    Безопасно убираем последнюю размерность, если она равна 1.\n",
        "    Если x не тензор — возвращаем как есть.\n",
        "    \"\"\"\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return x\n",
        "    if x.dim() > 0 and x.size(-1) == 1:\n",
        "        return x.squeeze(-1)\n",
        "    return x\n",
        "\n",
        "\n",
        "def _align_pred_target(y_pred, y_actual):\n",
        "    \"\"\"\n",
        "    Приводим предсказания и таргет к совместимым формам для MSE:\n",
        "    - Извлекаем тензоры из возможных контейнеров.\n",
        "    - Сводим предсказание к (B, T_pred) с последней осью как временем.\n",
        "    - Таргет сводим к (B,) или (B, T_act).\n",
        "    - Если таргет (B,) — берём последний горизонт из предсказаний.\n",
        "    - Если таргет (B, T_act) — подгоняем по времени (обрезаем/проверяем равенство).\n",
        "    \"\"\"\n",
        "    # 1) Достаём тензоры\n",
        "    y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "    y_actual = _extract_tensor(y_actual, role=\"target\")\n",
        "\n",
        "    # 2) Сжимаем последнюю единичную ось\n",
        "    y_pred = _maybe_squeeze_last(y_pred)\n",
        "    y_actual = _maybe_squeeze_last(y_actual)\n",
        "\n",
        "    # Быстрый путь: формы совпали\n",
        "    if isinstance(y_pred, torch.Tensor) and isinstance(y_actual, torch.Tensor):\n",
        "        if y_pred.shape == y_actual.shape:\n",
        "            return y_pred, y_actual\n",
        "\n",
        "    # 3) Приводим предсказание к (B, T_pred)\n",
        "    y_pred_bt = _collapse_pred_to_bt(y_pred)  # (B, T_pred)\n",
        "\n",
        "    # 4) Приведём таргет к (B,) или (B, T_act)\n",
        "    if y_actual.dim() == 1:\n",
        "        # (B,) — ожидаем 1 шаг на таргет → берём последний горизонт из предсказаний\n",
        "        if y_pred_bt.dim() != 2 or y_pred_bt.size(0) != y_actual.size(0):\n",
        "            raise ValueError(f\"Batch mismatch: pred={tuple(y_pred_bt.shape)} vs target={tuple(y_actual.shape)}\")\n",
        "        y_pred_aligned = y_pred_bt[:, -1]  # последний шаг горизонта\n",
        "        return y_pred_aligned, y_actual\n",
        "\n",
        "    if y_actual.dim() == 2:\n",
        "        # (B, T_act)\n",
        "        if y_pred_bt.size(0) != y_actual.size(0):\n",
        "            raise ValueError(f\"Batch mismatch: pred={tuple(y_pred_bt.shape)} vs target={tuple(y_actual.shape)}\")\n",
        "        T_pred = y_pred_bt.size(1)\n",
        "        T_act = y_actual.size(1)\n",
        "        if T_pred == T_act:\n",
        "            return y_pred_bt, y_actual\n",
        "        if T_pred > T_act:\n",
        "            # Обрежем последние T_act шагов, чтобы соответствовать таргету\n",
        "            y_pred_bt = y_pred_bt[:, -T_act:]\n",
        "            return y_pred_bt, y_actual\n",
        "        # Если предсказаний по времени меньше, чем в таргете — это логическая ошибка настройки\n",
        "        raise ValueError(\n",
        "            f\"Prediction horizon shorter than target: pred T={T_pred}, target T={T_act} \"\n",
        "            f\"(pred shape={tuple(y_pred_bt.shape)}, target shape={tuple(y_actual.shape)})\"\n",
        "        )\n",
        "\n",
        "    # Случай редкий: если таргет внезапно >2D — пробуем схлопнуть по всем, кроме батча\n",
        "    if y_actual.dim() > 2:\n",
        "        # Схлопнём таргет к (B, T_act) по последней оси\n",
        "        reduce_dims = tuple(range(1, y_actual.dim() - 1))\n",
        "        if len(reduce_dims) > 0:\n",
        "            y_actual_bt = y_actual.mean(dim=reduce_dims)\n",
        "        else:\n",
        "            y_actual_bt = y_actual\n",
        "        # Рекурсивно выровняем теперь как (B, ?)\n",
        "        return _align_pred_target(y_pred_bt, y_actual_bt)\n",
        "\n",
        "    # Если таргет скалярный (редко, но вдруг), расширим до (B,) повтором\n",
        "    if y_actual.dim() == 0:\n",
        "        y_actual = y_actual.expand(y_pred_bt.size(0))\n",
        "        y_pred_aligned = y_pred_bt[:, -1]\n",
        "        return y_pred_aligned, y_actual\n",
        "\n",
        "    # Если сюда дошли — что-то совсем нетипичное\n",
        "    raise ValueError(\n",
        "        f\"Shapes still mismatch after alignment: pred={tuple(y_pred.shape)} vs target={tuple(y_actual.shape)}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Фиксируем seeds для воспроизводимости и стабильности\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    pl.seed_everything(seed, verbose=False)\n",
        "\n",
        "\n",
        "class PeakFriendlyHuber(Metric):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        delta: float = 0.5,\n",
        "        peak_thr: float = 0.85,\n",
        "        peak_weight: float = 1.6,  # Увеличено до 1.6 для stronger поощрения пиков\n",
        "        contrast_weight: float = 0.02,\n",
        "        center_band: float = 0.3,\n",
        "        clip_scale: float = 1.5,  # Новый: scale для soft-clip (tanh * scale, чтобы не сжимать середину сильно)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.delta = float(delta)\n",
        "        self.peak_thr = float(peak_thr)\n",
        "        self.peak_weight = float(peak_weight)\n",
        "        self.contrast_weight = float(contrast_weight)\n",
        "        self.center_band = float(center_band)\n",
        "        self.clip_scale = float(clip_scale)  # Новый параметр\n",
        "        self.mse = MeanSquaredError()\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth_l1(diff, delta):\n",
        "        absd = diff.abs()\n",
        "        return torch.where(absd < delta, 0.5 * (diff ** 2) / delta, absd - 0.5 * delta)\n",
        "\n",
        "    def to_prediction(self, y_pred):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        return super().to_prediction(y_pred)\n",
        "\n",
        "    def to_quantiles(self, y_pred, quantiles=None, **kwargs):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        return super().to_quantiles(y_pred, quantiles=quantiles, **kwargs)\n",
        "\n",
        "    def loss(self, y_pred, y_actual, **kwargs):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        y_actual = _extract_tensor(y_actual, role=\"target\")\n",
        "        y_pred, y_actual = _align_pred_target(y_pred, y_actual)\n",
        "\n",
        "        # Soft-clip (без изменений)\n",
        "        y_pred = torch.tanh(y_pred * self.clip_scale) / self.clip_scale\n",
        "\n",
        "        diff = y_pred - y_actual\n",
        "        base = self._smooth_l1(diff, self.delta)\n",
        "\n",
        "        # Адаптивный peak_weight: средний по батчу, scale от доли пиков\n",
        "        if self.peak_weight > 1.0:\n",
        "            with torch.no_grad():\n",
        "                peak_mag = torch.relu(y_actual.abs() - self.peak_thr)\n",
        "                peak_frac = (peak_mag > 0).float().mean()  # Доля пиков в батче\n",
        "                adaptive_weight = 1.0 + (self.peak_weight - 1.0) * peak_frac  # Больше веса, если много пиков\n",
        "                w = adaptive_weight * torch.clamp(peak_mag / (1.0 - self.peak_thr + 1e-8), 0.0, 1.0) + 1.0\n",
        "            huber_term = (base * w).mean()\n",
        "        else:\n",
        "            huber_term = base.mean()\n",
        "\n",
        "        # Лёгкий «anti-flatness» у центра: штрафим чрезмерно малую амплитуду,\n",
        "        # но только там, где таргет далеко от 0.\n",
        "        if self.contrast_weight > 0.0:\n",
        "            with torch.no_grad():\n",
        "                far_mask = (y_actual.abs() >= self.center_band).float()\n",
        "                near_mask = (y_actual.abs() < self.center_band).float()\n",
        "\n",
        "            # Прямая амплитуда предсказания\n",
        "            far_amp = (y_pred.abs() * far_mask).sum() / (far_mask.sum() + 1e-8)\n",
        "            near_amp = (y_pred.abs() * near_mask).sum() / (near_mask.sum() + 1e-8)\n",
        "\n",
        "            # Хотим far_amp >= near_amp + margin; введём небольшой margin\n",
        "            margin = 0.05\n",
        "            contrast = torch.relu((near_amp + margin) - far_amp)\n",
        "            loss = huber_term + self.contrast_weight * contrast\n",
        "        else:\n",
        "            loss = huber_term\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __call__(self, y_pred, y_actual, **kwargs):\n",
        "        return self.loss(y_pred, y_actual, **kwargs)\n",
        "\n",
        "    def update(self, y_pred, y_actual, **kwargs):\n",
        "        y_pred = _extract_tensor(y_pred, role=\"pred\")\n",
        "        y_actual = _extract_tensor(y_actual, role=\"target\")\n",
        "        y_pred, y_actual = _align_pred_target(y_pred, y_actual)\n",
        "        self.mse.update(y_pred, y_actual)\n",
        "\n",
        "    def compute(self):\n",
        "        return self.mse.compute()\n",
        "\n",
        "    def reset(self):\n",
        "        self.mse.reset()\n",
        "\n",
        "    def name(self):\n",
        "        return \"PeakFriendlyHuber\"\n",
        "\n",
        "class TFTAdapter(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Обёртка над TemporalFusionTransformer,\n",
        "    чтобы Trainer воспринимал модель нужного типа\n",
        "    и наш tft.training_step видел непустой self.trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tft: TemporalFusionTransformer):\n",
        "        super().__init__()\n",
        "        self.tft = tft\n",
        "\n",
        "    def on_fit_start(self) -> None:\n",
        "        # вызовется перед стартом Trainer.fit\n",
        "        # прикрепляем Trainer к внутреннему tft\n",
        "        self.tft.trainer = self.trainer\n",
        "        # и логгеры\n",
        "        self.tft.log = self.log\n",
        "        self.tft.log_dict = self.log_dict\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        return self.tft(*args, **kwargs)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        self.tft.trainer = self.trainer\n",
        "        result = self.tft.training_step(batch, batch_idx)\n",
        "        if isinstance(result, dict):\n",
        "            return result.get(\"loss\")\n",
        "        elif isinstance(result, tuple) and len(result) >= 2:\n",
        "            log_dict, out = result[:2]  # Берем первые два элемента\n",
        "            if isinstance(log_dict, dict):\n",
        "                return log_dict.get(\"loss\")\n",
        "        # Если ни один вариант не подошел\n",
        "        raise ValueError(f\"Unexpected return type from tft.training_step: {type(result)}\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # обеспечим корректную ссылку на тренер внутри tft (если нужно)\n",
        "        self.tft.trainer = self.trainer\n",
        "        self.tft.validation_step(batch, batch_idx)\n",
        "        return\n",
        "\n",
        "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
        "        # Просто передаём вызов внутреннему TFT, без dataloader_idx (не нужен для TFT)\n",
        "        return self.tft.predict_step(batch, batch_idx)\n",
        "\n",
        "    def predict(self, *args, **kwargs):\n",
        "        return self.tft.predict(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.tft.configure_optimizers()\n",
        "\n",
        "\n",
        "def to_dense(X):\n",
        "    \"\"\"Преобразование sparse matrix в dense numpy array\"\"\"\n",
        "    if hasattr(X, 'toarray'):\n",
        "        return X.toarray()\n",
        "    return np.asarray(X)\n",
        "\n",
        "def prepare_data_transformer(df, target_col):\n",
        "    df = df.dropna(subset=[target_col])\n",
        "    if 'time' in df.columns and 'time_idx' not in df.columns:\n",
        "        df = df.rename(columns={'time': 'time_idx'})\n",
        "    X = df.drop(columns=target_col)\n",
        "    y = df[target_col].copy()\n",
        "\n",
        "    exclude = ['time_idx', 'batch']\n",
        "    numeric_features = [\n",
        "        c for c in X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\n",
        "        if c not in exclude\n",
        "    ]\n",
        "    categorical_features = [\n",
        "        c for c in X.select_dtypes(include=['object', 'category']).columns\n",
        "        if c not in exclude\n",
        "    ]\n",
        "\n",
        "    preprocessing = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([\n",
        "                ('scaler', RobustScaler()),\n",
        "                ('yeo', PowerTransformer(method='yeo-johnson'))\n",
        "            ]), numeric_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "        ],\n",
        "        remainder='passthrough'  # passthrough → тут окажутся сначала все num→cat, а потом time_idx и batch\n",
        "    )\n",
        "    # Возвращаем дополнительные списки для передачи в модель\n",
        "    return X, y, preprocessing, numeric_features, categorical_features\n",
        "\n",
        "def split_features_batch_time(X_array, n_transformed):\n",
        "    \"\"\"\n",
        "    X_array: np.ndarray после преобразований shape=(N, n_transformed + 2)\n",
        "    n_transformed: сколько колонок ушло на num+cat\n",
        "    возвращает (features, time_raw, batch_raw)\n",
        "    \"\"\"\n",
        "    features = X_array[:, :n_transformed]\n",
        "    batch_raw = X_array[:, n_transformed].ravel()\n",
        "    time_raw = X_array[:, n_transformed + 1].ravel()\n",
        "    return features, batch_raw, time_raw\n",
        "\n",
        "\n",
        "def tft_output_transformer(x):\n",
        "    # Больше НЕ клипуем внутри графа. Пусть модель учится выходить за [-1,1],\n",
        "    # а мы ограничим при расчёте метрик и при возврате пользователю.\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_early_stopping_callback(patience=10, min_delta=0.001):\n",
        "    return EarlyStopping(\n",
        "        monitor=\"train_loss\",\n",
        "        patience=patience,\n",
        "        min_delta=min_delta,\n",
        "        mode=\"min\",\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "\n",
        "class SequenceTransformerRegressor(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    Temporal Fusion Transformer для последовательностей.\n",
        "    Интегрируется в Pipeline аналогично LSTM.\n",
        "    Адаптировано для стабильного обучения и алготрейдинга (реального времени).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 seq_len: int = 10,\n",
        "                 pred_len: int = 1,\n",
        "                 hidden_size: int = 64,  # Увеличено для лучшей емкости\n",
        "                 hidden_continuous_size: int = 24,\n",
        "                 epochs: int = 100,  # Увеличено для более долгого обучения\n",
        "                 batch_size: int = 128,  # Увеличено для стабильности\n",
        "                 learning_rate: float = 1e-3,  # Увеличено для более быстрого старта\n",
        "                 patience: int = 15,  # Увеличено для терпимости\n",
        "                 seed: int = 42,\n",
        "                 dropout: float = 0.25,  # Уменьшено для меньшей регуляризации\n",
        "                 weight_decay: float = 1e-4,  # Новый: для регуляризации\n",
        "                 verbose: int = 2,\n",
        "                 mask_prob: float = 0.1,  # Уменьшено, чтобы меньше шумить\n",
        "                 infer_stride: int = 2,\n",
        "                 ckpt_path=None,\n",
        "                 preprocessing=None,\n",
        "                 numeric_features=None,\n",
        "                 categorical_features=None,\n",
        "                 remainder_columns=None,\n",
        "                 min_encoder_length: int = 1):  # Больше логов\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_continuous_size = hidden_continuous_size\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.patience = patience\n",
        "        self.seed = seed\n",
        "        self.verbose = verbose\n",
        "        self._model = None\n",
        "        self._trainer = None\n",
        "        self._n_transformed = None\n",
        "        self._n_feat = None\n",
        "        self._train_dataset = None\n",
        "        self._feature_columns = None\n",
        "        self._dropout = dropout\n",
        "        self.norm_eps = 1e-6\n",
        "        self.norm_window = max(10, self.seq_len // 2)\n",
        "        self.ckpt_path = ckpt_path\n",
        "        self.weight_decay = weight_decay  # Новый\n",
        "        self.mask_prob = mask_prob\n",
        "        self.infer_stride = infer_stride\n",
        "        self.global_target_mean = 0.0  # Будем вычислять в fit\n",
        "        self.global_target_std = 1.0  # Fallback global scale\n",
        "        self.global_target_min = None\n",
        "        self.global_target_max = None\n",
        "        self.global_range = None\n",
        "        self.soft_clip_scale =  None\n",
        "        self.preprocessing = preprocessing\n",
        "        self.numeric_features = numeric_features or []\n",
        "        self.categorical_features = categorical_features or []\n",
        "        self.remainder_columns = remainder_columns or ['batch', 'time']\n",
        "        self.use_tanh_post = False\n",
        "        self.train_q_lo = None\n",
        "        self.train_q_hi = None\n",
        "        self.clip_scale = 1.5\n",
        "        self.future_fill_mode = \"repeat\"\n",
        "        self.smooth_window = max(5, self.seq_len // 5)  # For savgol\n",
        "        self.smooth_poly = 2\n",
        "        self.ema_alpha = 0.1\n",
        "        self.infer_batch_size = 512  # Новый: большой батч для inference\n",
        "        self.infer_stride = infer_stride if infer_stride is not None else 4\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'# Добавьте это, если нужно tanh в предикте\n",
        "        self.min_encoder_length = max(1, min_encoder_length)  # Не меньше 1\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        set_seeds(self.seed)\n",
        "\n",
        "        X = np.asarray(X)\n",
        "        if self._n_transformed is None:\n",
        "            self._n_transformed = X.shape[1] - 2\n",
        "            self._n_feat = X.shape[1]\n",
        "\n",
        "        feat, batch_raw, time_raw = split_features_batch_time(X, self._n_transformed)\n",
        "\n",
        "        df = pd.DataFrame(feat, columns=[f\"f{i}\" for i in range(feat.shape[1])])\n",
        "        self._feature_columns = df.columns.tolist()\n",
        "\n",
        "        df[\"batch\"] = pd.Series(batch_raw).astype(\"int64\")\n",
        "        df[\"time_raw\"] = pd.to_datetime(time_raw, utc=True, errors=\"coerce\")\n",
        "\n",
        "        if isinstance(y, (pd.Series, pd.DataFrame)):\n",
        "            y = y.reset_index(drop=True)\n",
        "        df[\"target\"] = pd.Series(y, index=df.index).astype(float)\n",
        "\n",
        "        before = len(df)\n",
        "        df = df.dropna(subset=[\"time_raw\", \"target\"]).reset_index(drop=True)\n",
        "        if self.verbose and len(df) < before:\n",
        "            print(f\"Dropped {before - len(df)} rows with invalid time/target\")\n",
        "\n",
        "        df = df.sort_values([\"batch\", \"time_raw\"]).reset_index(drop=True)\n",
        "        df[\"time_idx\"] = df.groupby(\"batch\").cumcount()\n",
        "\n",
        "        min_len = max(1, int(self.pred_len))  # Изменено: позволяем короткие батчи (encoder может быть < seq_len)\n",
        "        gsize = df.groupby(\"batch\").size()\n",
        "        valid_batches = gsize[gsize >= min_len].index\n",
        "        if len(valid_batches) == 0:\n",
        "            raise ValueError(f\"No batches with length >= {min_len}. Reduce pred_len.\")\n",
        "        if self.verbose and len(valid_batches) < gsize.index.nunique():\n",
        "            dropped = sorted(list(set(gsize.index) - set(valid_batches)))\n",
        "            print(f\"Warning: dropped {len(dropped)} short batches: {dropped[:8]}{' ...' if len(dropped) > 8 else ''}\")\n",
        "        df = df[df[\"batch\"].isin(valid_batches)].reset_index(drop=True)\n",
        "\n",
        "        batch_starts = df.groupby(\"batch\")[\"time_raw\"].min().sort_values()\n",
        "        uniq_batches = batch_starts.index.to_numpy()\n",
        "        n_total = len(uniq_batches)\n",
        "        val_frac = 0.3 if n_total >= 10 else 0.1  # Увеличено для лучшего обобщения\n",
        "        n_val = max(1, int(round(n_total * val_frac)))\n",
        "\n",
        "        embargo = 1 if n_total >= 8 else 0\n",
        "\n",
        "        train_end = max(0, n_total - n_val - embargo)\n",
        "        train_batches = uniq_batches[:train_end]\n",
        "        val_batches = uniq_batches[-n_val:]\n",
        "\n",
        "        if len(train_batches) == 0 and n_total > 1:\n",
        "            train_batches = uniq_batches[:-1]\n",
        "            val_batches = uniq_batches[-1:]\n",
        "\n",
        "        train_df = df[df[\"batch\"].isin(train_batches)].copy()\n",
        "        val_df = df[df[\"batch\"].isin(val_batches)].copy()\n",
        "\n",
        "        self.global_target_mean = train_df[\"target\"].mean()\n",
        "        self.global_target_std = train_df[\"target\"].std() + 1e-8\n",
        "        self.global_target_min = train_df[\"target\"].min()\n",
        "        self.global_target_max = train_df[\"target\"].max()\n",
        "        self.global_range = self.global_target_max - self.global_target_min + 1e-8\n",
        "        self.soft_clip_scale = max(1.0, self.global_target_std * 1.5)  # Adaptive soft clip for stable [-1,1] without compression\n",
        "\n",
        "        #self.clip_scale = max(1.0, self.global_target_std * 1.2)   # Adaptive to train variance\n",
        "\n",
        "        train_targets = train_df[\"target\"].values\n",
        "        #if len(train_targets) > 0:\n",
        "        #   self.train_q_lo, self.train_q_hi = np.quantile(train_targets, [0.01, 0.99])\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Train batches: {len(np.unique(train_batches))}\")\n",
        "            print(f\"Val batches: {len(np.unique(val_batches))}\")\n",
        "            print(f\"Train rows: {len(train_df)}, Val rows: {len(val_df)}\")\n",
        "\n",
        "        full_dataset = TimeSeriesDataSet(\n",
        "            df,\n",
        "            time_idx=\"time_idx\",\n",
        "            target=\"target\",\n",
        "            group_ids=[\"batch\"],\n",
        "            max_encoder_length=int(self.seq_len),\n",
        "            min_encoder_length=0,#int(self.min_encoder_length),  # Новый: позволяем короткие encoder\n",
        "            max_prediction_length=int(self.pred_len),\n",
        "            time_varying_unknown_reals=self._feature_columns,\n",
        "            target_normalizer=None,\n",
        "            allow_missing_timesteps=True,\n",
        "            add_relative_time_idx=True,\n",
        "            add_target_scales=False,\n",
        "            add_encoder_length=True,\n",
        "            min_prediction_length=1,\n",
        "        )\n",
        "\n",
        "        train_dataset = TimeSeriesDataSet.from_dataset(\n",
        "            full_dataset, train_df, predict=False, stop_randomization=True\n",
        "        )\n",
        "        val_dataset = TimeSeriesDataSet.from_dataset(\n",
        "            full_dataset, val_df, predict=False, stop_randomization=True\n",
        "        ) if len(val_df) > 0 else None\n",
        "\n",
        "        train_dl = train_dataset.to_dataloader(\n",
        "            train=True,\n",
        "            batch_size=int(self.batch_size),\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            worker_init_fn=None,\n",
        "            drop_last=False,\n",
        "            persistent_workers=False,\n",
        "        )\n",
        "        val_dl = None\n",
        "        if val_dataset is not None and len(val_dataset) > 0:\n",
        "            val_dl = val_dataset.to_dataloader(\n",
        "                train=False,\n",
        "                batch_size=int(self.batch_size),\n",
        "                shuffle=False,\n",
        "                num_workers=0,\n",
        "                worker_init_fn=None,\n",
        "                drop_last=False,\n",
        "                persistent_workers=False,\n",
        "            )\n",
        "\n",
        "        tft = CustomTFT.from_dataset(\n",
        "            train_dataset,\n",
        "            hidden_size=int(self.hidden_size),\n",
        "            output_size=1,\n",
        "            loss=PeakFriendlyHuber(\n",
        "                delta=0.5,\n",
        "                peak_thr=0.85,  # можно затем подвинуть 0.8..0.9\n",
        "                peak_weight=1.3,  # аккуратно: 1.3..1.6\n",
        "                contrast_weight=0.03,  # очень маленькая добавка\n",
        "                center_band=0.3,  # что считать «центром»\n",
        "                clip_scale=1.5\n",
        "            ),\n",
        "            optimizer=\"adam\",\n",
        "            learning_rate=float(self.learning_rate),  # оставьте тот, на котором MSE был лучше (у вас 1e-4 давал ~0.186)\n",
        "            lstm_layers=3,\n",
        "            hidden_continuous_size=self.hidden_continuous_size,\n",
        "            attention_head_size=4,\n",
        "            dropout=float(self._dropout),\n",
        "            reduce_on_plateau_patience=5,\n",
        "            reduce_on_plateau_min_lr=1e-6,\n",
        "            weight_decay=float(self.weight_decay),\n",
        "            mask_prob=float(self.mask_prob),\n",
        "            output_transformer=tft_output_transformer,\n",
        "        )\n",
        "\n",
        "        class GCCallback(pl.Callback):\n",
        "            def __init__(self):\n",
        "                super().__init__()\n",
        "                self.last_mem = psutil.Process().memory_info().rss / 1e6\n",
        "\n",
        "            def _check_gc(self):\n",
        "                current_mem = psutil.Process().memory_info().rss / 1e6\n",
        "                if current_mem - self.last_mem > 50:\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                self.last_mem = current_mem\n",
        "\n",
        "            def on_train_epoch_end(self, trainer, pl_module):\n",
        "                self._check_gc()\n",
        "\n",
        "            def on_validation_epoch_end(self, trainer, pl_module):\n",
        "                self._check_gc()\n",
        "\n",
        "            def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "                if batch_idx % 10 == 0:\n",
        "                    self._check_gc()\n",
        "\n",
        "            def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
        "                if batch_idx % 10 == 0:\n",
        "                    self._check_gc()\n",
        "\n",
        "        callbacks = []\n",
        "        if val_dl is not None:\n",
        "            callbacks.append(\n",
        "                get_early_stopping_callback(patience=self.patience, min_delta=1e-3))  # Новый: больше patience\n",
        "            checkpoint_callback = ModelCheckpoint(monitor=\"train_loss\", mode=\"min\", save_top_k=1, verbose=True)\n",
        "            callbacks.append(checkpoint_callback)\n",
        "        else:\n",
        "            checkpoint_callback = None\n",
        "\n",
        "        logger = TensorBoardLogger(save_dir=\"lightning_logs/\", name=\"my_model\") if self.verbose > 0 else False\n",
        "        if self.verbose > 0:\n",
        "            callbacks.append(LearningRateMonitor(logging_interval=\"step\"))\n",
        "            callbacks.append(NoValidationBar(refresh_rate=20))\n",
        "        callbacks.append(GCCallback())\n",
        "\n",
        "        self._model = TFTAdapter(tft)\n",
        "        self._trainer = pl.Trainer(\n",
        "            max_epochs=int(self.epochs),\n",
        "            enable_checkpointing=(checkpoint_callback is not None),\n",
        "            callbacks=callbacks,\n",
        "            logger=logger,\n",
        "            enable_model_summary=True,\n",
        "            gradient_clip_val=1.0,\n",
        "            gradient_clip_algorithm=\"norm\",\n",
        "            deterministic=True,\n",
        "            benchmark=False,\n",
        "            accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "            precision=32,\n",
        "            limit_val_batches=1.0 if val_dl is not None else 0.0,\n",
        "            enable_progress_bar=self.verbose > 0,\n",
        "            log_every_n_steps=50,\n",
        "            num_sanity_val_steps=0,\n",
        "        )\n",
        "\n",
        "        self._trainer.fit(\n",
        "            self._model,\n",
        "            train_dataloaders=train_dl,\n",
        "            val_dataloaders=val_dl if val_dl is not None else None,\n",
        "            ckpt_path=self.ckpt_path if self.ckpt_path and self.epochs > 0 else None,\n",
        "        )\n",
        "\n",
        "        if checkpoint_callback is not None and checkpoint_callback.best_model_path:\n",
        "            best_path = checkpoint_callback.best_model_path\n",
        "            if self.verbose:\n",
        "                print(f\"Loaded best model from {best_path} with val_loss={checkpoint_callback.best_model_score}\")\n",
        "            self._model = TFTAdapter.load_from_checkpoint(best_path, tft=tft)\n",
        "\n",
        "        self._train_dataset = full_dataset\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "        #return self\n",
        "        try:\n",
        "            y_train = train_df[\"target\"].to_numpy(dtype=float)\n",
        "            # robust percentiles — перестрахуемся от выносов: 1% и 99%\n",
        "            self._cal_p_low = float(np.nanpercentile(y_train, 1))\n",
        "            self._cal_p_high = float(np.nanpercentile(y_train, 99))\n",
        "            # амплитуды «типичных пиков»\n",
        "            top_mask = y_train >= self._cal_p_high\n",
        "            bot_mask = y_train <= self._cal_p_low\n",
        "            self._cal_mean_top = float(np.nanmean(y_train[top_mask])) if np.any(top_mask) else float(self.global_target_max)\n",
        "            self._cal_mean_bot = float(np.nanmean(y_train[bot_mask])) if np.any(bot_mask) else float(self.global_target_min)\n",
        "            # защита от вырождения\n",
        "            if not np.isfinite(self._cal_mean_top): self._cal_mean_top = float(self.global_target_max)\n",
        "            if not np.isfinite(self._cal_mean_bot): self._cal_mean_bot = float(self.global_target_min)\n",
        "        except Exception:\n",
        "            # безопасные фолбэки\n",
        "            self._cal_p_low, self._cal_p_high = self.global_target_min, self.global_target_max\n",
        "            self._cal_mean_top, self._cal_mean_bot = self.global_target_max, self.global_target_min\n",
        "        # ----------------------------------------------\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self._train_dataset is None or self._model is None:\n",
        "            raise RuntimeError(\"Model is not fitted yet\")\n",
        "\n",
        "        # Full suppress context (no logs/output, including PL/Torch/PF/Seed/GPU/TPU/HPU messages)\n",
        "        @contextlib.contextmanager\n",
        "        def suppress_all():\n",
        "            with open(os.devnull, \"w\") as devnull, contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n",
        "                # Подавление всех возможных логгеров\n",
        "                root_logger = logging.getLogger()\n",
        "                old_level = root_logger.level\n",
        "                root_logger.setLevel(logging.CRITICAL + 1)  # Выше CRITICAL, чтобы ничего не логировалось\n",
        "\n",
        "                # Специфические логгеры (расширенный список)\n",
        "                for logger_name in [\n",
        "                    \"pytorch_lightning\", \"lightning.pytorch\", \"lightning\",\n",
        "                    \"torch\", \"pytorch_forecasting\", \"optuna\",\n",
        "                    \"sklearn\", \"joblib\", \"numpy\", \"pandas\",\n",
        "                    \"scipy\", \"matplotlib\", \"seaborn\", \"plotly\",\n",
        "                    \"shap\", \"statsmodels\", \"torchmetrics\",\n",
        "                    \"lightning.pytorch.utilities.migration.utils\",  # Для Attribute 'loss' warnings\n",
        "                    \"lightning.pytorch.utilities.migration\",\n",
        "                    \"lightning.pytorch.utilities\",\n",
        "                    \"lightning.pytorch\"\n",
        "                ]:\n",
        "                    logging.getLogger(logger_name).setLevel(logging.CRITICAL + 1)\n",
        "\n",
        "                # Подавление всех предупреждений (расширенный список)\n",
        "                warnings.filterwarnings(\"ignore\")\n",
        "                warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "                warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "                warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*Attribute 'loss' is an instance of nn.Module.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*Attribute 'logging_metrics' is an instance of nn.Module.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*GPU available.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*TPU available.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*HPU available.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*This Pipeline instance is not fitted yet.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*Using an existing study with name.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*A value is trying to be set on a copy of a slice.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*The behavior of DataFrame concatenation.*\")\n",
        "                warnings.filterwarnings(\"ignore\", message=\".*torch.utils.checkpoint: the use_reentrant parameter.*\")\n",
        "\n",
        "                # Окружение (расширенное)\n",
        "                os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "                os.environ['LITMODELS_DISABLE_TIP'] = '1'\n",
        "                os.environ['HYDRA_FULL_ERROR'] = '1'\n",
        "                os.environ['TQDM_DISABLE'] = '0'  # Не подавлять tqdm (если вызван снаружи)\n",
        "                os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Для синхронизации CUDA, но без логов\n",
        "                os.environ['TORCH_USE_DETERMINISTIC_ALGORITHMS'] = '1'  # Без логов\n",
        "\n",
        "                try:\n",
        "                    yield\n",
        "                finally:\n",
        "                    root_logger.setLevel(old_level)\n",
        "                    warnings.resetwarnings()\n",
        "\n",
        "        with suppress_all():\n",
        "            set_seeds(self.seed)\n",
        "            pl.seed_everything(self.seed, verbose=False, workers=True)\n",
        "\n",
        "            if self.preprocessing is not None and not isinstance(X, np.ndarray):\n",
        "                X = self.preprocessing.transform(X)\n",
        "\n",
        "            X = np.asarray(X)\n",
        "            N = X.shape[0]\n",
        "            if N < self.min_encoder_length:\n",
        "                return [float(np.nan)] * N\n",
        "\n",
        "            feat, _, time_raw = split_features_batch_time(X, self._n_transformed)\n",
        "            df = pd.DataFrame(feat, columns=self._feature_columns)\n",
        "            df[\"__row_id\"] = np.arange(N)\n",
        "            df[\"time_raw\"] = pd.to_datetime(time_raw, utc=True, errors=\"coerce\")\n",
        "            df[\"batch\"] = np.int64(0)\n",
        "            df[\"target\"] = self.global_target_mean\n",
        "\n",
        "            df = df.dropna(subset=[\"time_raw\"]).reset_index(drop=True)\n",
        "            df_sorted = df.sort_values(\"time_raw\").reset_index(drop=True)\n",
        "            df_sorted[\"time_idx\"] = np.arange(len(df_sorted), dtype=np.int64)\n",
        "\n",
        "            eff_N = len(df_sorted)\n",
        "            if eff_N < self.min_encoder_length:\n",
        "                out = np.full(N, np.nan, dtype=float)\n",
        "                return out.tolist()\n",
        "\n",
        "            step_ns = int(60 * 1e9)\n",
        "            if eff_N >= 2:\n",
        "                diffs = df_sorted[\"time_raw\"].view(\"int64\").astype(\"int64\").to_numpy()\n",
        "                diffs = np.diff(diffs)\n",
        "                step_ns = int(np.nan_to_num(np.median(diffs), nan=step_ns))\n",
        "                if step_ns <= 0:\n",
        "                    step_ns = int(60 * 1e9)\n",
        "\n",
        "            out_raw = np.full(eff_N, np.nan, dtype=float)\n",
        "\n",
        "            tft = self._model.tft\n",
        "            orig_log = tft.log\n",
        "            tft.log = lambda *args, **kwargs: None\n",
        "\n",
        "            try:\n",
        "                eff_encoder_len = min(self.seq_len, eff_N)\n",
        "                M = max(0, eff_N - eff_encoder_len + 1)\n",
        "                stride = self.pred_len  # Keep original stride to preserve logic\n",
        "                window_starts = np.arange(0, M, stride)\n",
        "                K = len(window_starts)\n",
        "\n",
        "                if K == 0:\n",
        "                    K = 1\n",
        "                    window_starts = np.array([0])\n",
        "                    eff_encoder_len = eff_N\n",
        "\n",
        "                num_feat_cols = len(self._feature_columns)\n",
        "\n",
        "                # Vectorized construction of all_feats\n",
        "                enc_indices = window_starts[:, np.newaxis] + np.arange(eff_encoder_len)\n",
        "                all_enc_feats = feat[enc_indices]  # (K, eff_encoder_len, num_feat_cols)\n",
        "                last_enc_feats = all_enc_feats[:, -1, :]\n",
        "                all_fut_feats = np.repeat(last_enc_feats[:, np.newaxis, :], self.pred_len, axis=1)  # (K, pred_len, num_feat_cols)\n",
        "                all_feats = np.concatenate([all_enc_feats, all_fut_feats], axis=1).reshape(-1, num_feat_cols)\n",
        "\n",
        "                # Vectorized all_time_idx\n",
        "                orig_time_idx = df_sorted[\"time_idx\"].values\n",
        "                all_enc_time_idx = orig_time_idx[enc_indices]  # (K, eff_encoder_len)\n",
        "                last_time_idx = all_enc_time_idx[:, -1]\n",
        "                fut_offsets = np.arange(1, self.pred_len + 1)\n",
        "                all_fut_time_idx = last_time_idx[:, np.newaxis] + fut_offsets  # (K, pred_len)\n",
        "                all_time_idx = np.concatenate([all_enc_time_idx, all_fut_time_idx], axis=1).reshape(-1)\n",
        "\n",
        "                # Vectorized all_batch\n",
        "                batch_per_window = np.arange(K)[:, np.newaxis]\n",
        "                all_enc_batch = np.repeat(batch_per_window, eff_encoder_len, axis=1)  # (K, eff_encoder_len)\n",
        "                all_fut_batch = np.repeat(batch_per_window, self.pred_len, axis=1)  # (K, pred_len)\n",
        "                all_batch = np.concatenate([all_enc_batch, all_fut_batch], axis=1).reshape(-1)\n",
        "\n",
        "                # all_target (constant)\n",
        "                all_target = np.full(K * (eff_encoder_len + self.pred_len), self.global_target_mean, dtype=np.float32)\n",
        "\n",
        "                # Vectorized all_time_raw using int64 ns\n",
        "                orig_time_raw_int = df_sorted[\"time_raw\"].view(\"int64\").values\n",
        "                all_enc_time_int = orig_time_raw_int[enc_indices]  # (K, eff_encoder_len)\n",
        "                last_time_int = all_enc_time_int[:, -1]\n",
        "                fut_offsets_ns = fut_offsets * step_ns\n",
        "                all_fut_time_int = last_time_int[:, np.newaxis] + fut_offsets_ns  # (K, pred_len)\n",
        "                all_time_int_flat = np.concatenate([all_enc_time_int.reshape(-1), all_fut_time_int.reshape(-1)])\n",
        "                all_time_raw = pd.to_datetime(all_time_int_flat, unit='ns', utc=True).values  # object array of Timestamp\n",
        "\n",
        "                pred_df = pd.DataFrame(all_feats, columns=self._feature_columns)\n",
        "                pred_df[\"time_idx\"] = all_time_idx\n",
        "                pred_df[\"batch\"] = all_batch\n",
        "                pred_df[\"target\"] = all_target\n",
        "                pred_df[\"time_raw\"] = all_time_raw\n",
        "\n",
        "                pred_df[\"batch\"] = pred_df[\"batch\"].astype(\"int64\")\n",
        "                pred_df[\"time_idx\"] = pred_df[\"time_idx\"].astype(\"int64\")\n",
        "                pred_df[\"target\"] = pred_df[\"target\"].astype(\"float32\")\n",
        "\n",
        "                sliding_dataset = TimeSeriesDataSet.from_dataset(\n",
        "                    self._train_dataset,\n",
        "                    pred_df,\n",
        "                    predict=True,\n",
        "                    stop_randomization=True,\n",
        "                    min_prediction_length=self.pred_len,\n",
        "                    max_prediction_length=self.pred_len,\n",
        "                )\n",
        "\n",
        "                test_dl = sliding_dataset.to_dataloader(\n",
        "                    train=False,\n",
        "                    batch_size=K,  # Full batch for max speed (K small due to stride=pred_len)\n",
        "                    num_workers=0,\n",
        "                    persistent_workers=False,\n",
        "                    pin_memory=False,\n",
        "                )\n",
        "\n",
        "                preds = tft.predict(\n",
        "                    test_dl,\n",
        "                    mode=\"prediction\",\n",
        "                    return_x=False,\n",
        "                    trainer_kwargs={\n",
        "                        \"logger\": False,\n",
        "                        \"enable_progress_bar\": False,\n",
        "                        \"enable_model_summary\": False,\n",
        "                        \"enable_checkpointing\": False,\n",
        "                        \"accelerator\": \"gpu\" if self.device == \"cuda\" else \"cpu\"\n",
        "                    },\n",
        "                )\n",
        "\n",
        "                if isinstance(preds, torch.Tensor):\n",
        "                    preds = preds.detach().cpu()\n",
        "                    if preds.dim() == 3 and preds.size(-1) == 1:\n",
        "                        preds = preds.squeeze(-1)\n",
        "                    if preds.dim() == 2:\n",
        "                        yhat_stride = preds[:, 0].numpy()  # First step per window\n",
        "                    elif preds.dim() == 1:\n",
        "                        yhat_stride = preds.numpy()\n",
        "                    elif preds.dim() == 0:\n",
        "                        yhat_stride = np.array([preds.item()])\n",
        "                    else:\n",
        "                        yhat_stride = np.full(K, self.global_target_mean)\n",
        "                else:\n",
        "                    yhat_stride = np.full(K, self.global_target_mean)\n",
        "\n",
        "                # Fill out_raw at window ends\n",
        "                for j, i in enumerate(window_starts):\n",
        "                    out_raw[i + eff_encoder_len - 1] = yhat_stride[j]\n",
        "\n",
        "                # Causal interpolation for missed points (linear from past, no future look)\n",
        "                s = pd.Series(out_raw)\n",
        "                s = s.interpolate(method='linear', limit_direction='forward')  # Only forward for causality\n",
        "                s = s.ffill()  # Fill initial with first pred\n",
        "                out_raw = s.values\n",
        "\n",
        "                # Global map to [-1,1] (consistent scale, preserves relative peaks)\n",
        "                out = (out_raw - self.global_target_min) / self.global_range * 2 - 1\n",
        "                out = np.tanh(out * self.soft_clip_scale) / np.tanh(self.soft_clip_scale)\n",
        "                out = np.clip(out, -1.0, 1.0)\n",
        "\n",
        "                # Fill any remaining nans (unlikely) with mapped mean\n",
        "                nan_mask = np.isnan(out)\n",
        "                mapped_mean = (self.global_target_mean - self.global_target_min) / self.global_range * 2 - 1\n",
        "                out[nan_mask] = mapped_mean\n",
        "\n",
        "            finally:\n",
        "                tft.log = orig_log\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            original_pos = df_sorted[\"__row_id\"].to_numpy()\n",
        "            out_final = np.full(N, np.nan, dtype=float)\n",
        "            out_final[original_pos] = out\n",
        "\n",
        "            return out_final.tolist()\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # Чтобы sklearn корректно передавал параметры в Pipeline\n",
        "        return {\n",
        "            \"seq_len\": self.seq_len,\n",
        "            \"pred_len\": self.pred_len,\n",
        "            \"hidden_size\": self.hidden_size,\n",
        "            \"epochs\": self.epochs,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"patience\": self.patience,\n",
        "            \"seed\": self.seed,\n",
        "            \"dropout\": self._dropout,\n",
        "            \"weight_decay\": self.weight_decay,\n",
        "            \"verbose\": self.verbose,\n",
        "            \"ckpt_path\": self.ckpt_path\n",
        "        }\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "def _tensor_state_dict_to(dtype: torch.dtype, state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    out = {}\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            # важно: сохраняем в CPU и ровно в dtype (float32 для идентичности)\n",
        "            out[k] = v.detach().to('cpu', dtype=dtype)\n",
        "        else:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "\n",
        "def _save_bytes_compressed_zip(file_path: str, bytes_map: Dict[str, bytes], compression=zipfile.ZIP_DEFLATED, compresslevel=9):\n",
        "    with zipfile.ZipFile(file_path, mode='w', compression=compression, compresslevel=compresslevel) as zf:\n",
        "        for name, b in bytes_map.items():\n",
        "            zf.writestr(name, b)\n",
        "\n",
        "\n",
        "def _load_bytes_from_zip(file_path: str) -> Dict[str, bytes]:\n",
        "    out = {}\n",
        "    with zipfile.ZipFile(file_path, mode='r') as zf:\n",
        "        for name in zf.namelist():\n",
        "            out[name] = zf.read(name)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _safe_json_dump(obj: Dict[str, Any]) -> bytes:\n",
        "    def to_builtin(x):\n",
        "        import numpy as _np\n",
        "        import pandas as _pd\n",
        "        if isinstance(x, (_np.integer,)):\n",
        "            return int(x)\n",
        "        if isinstance(x, (_np.floating,)):\n",
        "            return float(x)\n",
        "        if isinstance(x, (_np.ndarray,)):\n",
        "            return x.tolist()\n",
        "        if isinstance(x, (_pd.Timestamp,)):\n",
        "            return x.isoformat()\n",
        "        return x\n",
        "\n",
        "    def convert(v):\n",
        "        if isinstance(v, dict):\n",
        "            return {k: convert(val) for k, val in v.items()}\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            return [convert(i) for i in v]\n",
        "        return to_builtin(v)\n",
        "\n",
        "    return json.dumps(convert(obj), ensure_ascii=False, separators=(\",\", \":\")).encode(\"utf-8\")\n",
        "\n",
        "\n",
        "def _safe_json_load(b: bytes) -> Dict[str, Any]:\n",
        "    return json.loads(b.decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "def _extract_tsd_feature_lists(tds) -> Dict[str, Any]:\n",
        "    def g(name, default=None):\n",
        "        return getattr(tds, name, default)\n",
        "\n",
        "    lists = {}\n",
        "    for name in [\n",
        "        \"time_varying_known_reals\",\n",
        "        \"time_varying_unknown_reals\",\n",
        "        \"static_reals\",\n",
        "        \"time_varying_known_categoricals\",\n",
        "        \"time_varying_unknown_categoricals\",\n",
        "        \"static_categoricals\",\n",
        "        \"target_categoricals\",\n",
        "        \"known_reals\",\n",
        "        \"unknown_reals\",\n",
        "        \"known_categoricals\",\n",
        "        \"unknown_categoricals\",\n",
        "        \"reals\",\n",
        "        \"categoricals\",\n",
        "    ]:\n",
        "        val = g(name, None)\n",
        "        if val is not None:\n",
        "            try:\n",
        "                lists[name] = list(val)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return lists\n",
        "\n",
        "\n",
        "def save_transformer(\n",
        "    pipeline: Pipeline,\n",
        "    path: str,\n",
        "    *,\n",
        "    # ЖЁСТКО: float32 по умолчанию для идентичности. Не выставляйте True, пока не сравните предикты.\n",
        "    float16_weights: bool = False,\n",
        "    preprocessing_filename: str = \"preprocessing.joblib.lzma\",\n",
        "    model_zip_filename: str = \"model_weights.zip\",\n",
        "    meta_json_filename: str = \"meta.json\"\n",
        "):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    if not isinstance(pipeline, Pipeline):\n",
        "        raise TypeError(\"Expected sklearn Pipeline\")\n",
        "\n",
        "    steps_dict = dict(pipeline.named_steps)\n",
        "    if \"preprocessing\" not in steps_dict or \"model\" not in steps_dict:\n",
        "        raise ValueError(\"Pipeline must have 'preprocessing' and 'model' steps\")\n",
        "\n",
        "    preprocessing = steps_dict[\"preprocessing\"]\n",
        "    model: SequenceTransformerRegressor = steps_dict[\"model\"]\n",
        "\n",
        "    if model._model is None or model._train_dataset is None:\n",
        "        raise RuntimeError(\"Model must be fitted before saving\")\n",
        "\n",
        "    # 1) preprocessing\n",
        "    prep_path = os.path.join(path, preprocessing_filename)\n",
        "    with lzma.open(prep_path, \"wb\", preset=9) as f:\n",
        "        joblib.dump(preprocessing, f)\n",
        "\n",
        "    # 2) параметры TDS\n",
        "    tds = model._train_dataset\n",
        "    def g(name, default=None):\n",
        "        return getattr(tds, name, default)\n",
        "\n",
        "    train_dataset_params = {\n",
        "        \"time_idx\": g(\"time_idx\", \"time_idx\"),\n",
        "        \"target\": g(\"target\", \"target\"),\n",
        "        \"group_ids\": list(g(\"group_ids\", [\"batch\"])),\n",
        "        \"max_encoder_length\": int(g(\"max_encoder_length\", model.seq_len)),\n",
        "        \"max_prediction_length\": int(g(\"max_prediction_length\", model.pred_len)),\n",
        "        \"target_normalizer\": None,\n",
        "        \"allow_missing_timesteps\": bool(g(\"allow_missing_timesteps\", True)),\n",
        "        \"add_relative_time_idx\": bool(g(\"add_relative_time_idx\", True)),\n",
        "        \"add_target_scales\": bool(g(\"add_target_scales\", False)),\n",
        "        \"add_encoder_length\": bool(g(\"add_encoder_length\", True)),\n",
        "        \"min_prediction_length\": int(g(\"min_prediction_length\", 1)),\n",
        "        \"min_encoder_length\": int(g(\"min_encoder_length\", 0)),\n",
        "    }\n",
        "    feature_lists = _extract_tsd_feature_lists(tds)\n",
        "\n",
        "    # 3) веса TFT — строго в float32 (если float16_weights=False)\n",
        "    tft = model._model.tft\n",
        "    state = tft.state_dict()\n",
        "    dtype = torch.float16 if float16_weights else torch.float32\n",
        "    state = _tensor_state_dict_to(dtype, state)\n",
        "    buf = io.BytesIO()\n",
        "    torch.save(state, buf, _use_new_zipfile_serialization=True)\n",
        "    buf.seek(0)\n",
        "    weights_zip_path = os.path.join(path, model_zip_filename)\n",
        "    _save_bytes_compressed_zip(weights_zip_path, {\"tft_state_dict.pt\": buf.getvalue()})\n",
        "\n",
        "    # 4) метаданные\n",
        "    meta = {\n",
        "        \"class\": \"SequenceTransformerRegressor\",\n",
        "        \"params\": {\n",
        "            \"seq_len\": model.seq_len,\n",
        "            \"pred_len\": model.pred_len,\n",
        "            \"hidden_size\": model.hidden_size,\n",
        "            \"hidden_continuous_size\": model.hidden_continuous_size,\n",
        "            \"epochs\": model.epochs,\n",
        "            \"batch_size\": model.batch_size,\n",
        "            \"learning_rate\": model.learning_rate,\n",
        "            \"patience\": model.patience,\n",
        "            \"seed\": model.seed,\n",
        "            \"dropout\": model._dropout,\n",
        "            \"weight_decay\": model.weight_decay,\n",
        "            \"verbose\": model.verbose,\n",
        "            \"mask_prob\": model.mask_prob,\n",
        "            \"infer_stride\": model.infer_stride,\n",
        "            \"ckpt_path\": None,\n",
        "            \"min_encoder_length\": model.min_encoder_length,\n",
        "            \"lstm_layers\": 3,\n",
        "            \"attention_head_size\": 4,\n",
        "            \"output_size\": 1,\n",
        "        },\n",
        "        \"feature_columns\": model._feature_columns,\n",
        "        \"n_transformed\": model._n_transformed,\n",
        "        \"n_feat\": model._n_feat,\n",
        "        \"global_stats\": {\n",
        "            \"global_target_mean\": model.global_target_mean,\n",
        "            \"global_target_std\": model.global_target_std,\n",
        "            \"global_target_min\": model.global_target_min,\n",
        "            \"global_target_max\": model.global_target_max,\n",
        "            \"global_range\": model.global_range,\n",
        "            \"soft_clip_scale\": model.soft_clip_scale,\n",
        "            \"clip_scale\": model.clip_scale,\n",
        "        },\n",
        "        \"train_dataset_params\": train_dataset_params,\n",
        "        \"feature_lists\": feature_lists,\n",
        "        \"torch_dtype\": \"float16\" if float16_weights else \"float32\",\n",
        "        \"preproc_feature_names_out\": list(getattr(preprocessing, \"get_feature_names_out\", lambda: [])()),\n",
        "    }\n",
        "\n",
        "    meta_path = os.path.join(path, meta_json_filename)\n",
        "    with open(meta_path, \"wb\") as f:\n",
        "        f.write(_safe_json_dump(meta))\n",
        "\n",
        "    weights_size_mb = os.path.getsize(weights_zip_path) / (1024 * 1024)\n",
        "    prep_size_mb = os.path.getsize(prep_path) / (1024 * 1024)\n",
        "    meta_size_kb = os.path.getsize(meta_path) / 1024.0\n",
        "    print(f\"Saved: weights={weights_size_mb:.2f} MB, preprocessing={prep_size_mb:.2f} MB, meta={meta_size_kb:.1f} KB → dir={path}\")\n",
        "\n",
        "# Требуется наличие в окружении:\n",
        "# - SequenceTransformerRegressor\n",
        "# - TimeSeriesDataSet\n",
        "# - CustomTFT, TFTAdapter\n",
        "# - tft_output_transformer\n",
        "# - PeakFriendlyHuber\n",
        "# - to_dense\n",
        "# - вспомогательные функции _load_bytes_from_zip, _safe_json_load из предыдущей версии\n",
        "\n",
        "\n",
        "def load_transformer_exact(\n",
        "    path: str,\n",
        "    *,\n",
        "    preprocessing_filename: str = \"preprocessing.joblib.lzma\",\n",
        "    model_zip_filename: str = \"model_weights.zip\",\n",
        "    meta_json_filename: str = \"meta.json\",\n",
        "    device: Optional[str] = None,\n",
        ") -> Pipeline:\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", message=\".*Attribute 'loss' is an instance of `nn.Module`.*\")\n",
        "    warnings.filterwarnings(\"ignore\", message=\".*Attribute 'logging_metrics' is an instance of `nn.Module`.*\")\n",
        "\n",
        "    # 1) preprocessing\n",
        "    prep_path = os.path.join(path, preprocessing_filename)\n",
        "    with lzma.open(prep_path, \"rb\") as f:\n",
        "        preprocessing = joblib.load(f)\n",
        "\n",
        "    # 2) meta\n",
        "    meta_path = os.path.join(path, meta_json_filename)\n",
        "    if not os.path.isfile(meta_path):\n",
        "        raise FileNotFoundError(meta_path)\n",
        "    with open(meta_path, \"rb\") as f:\n",
        "        meta = json.loads(f.read().decode(\"utf-8\"))\n",
        "\n",
        "    params = meta[\"params\"]\n",
        "    feature_columns = meta[\"feature_columns\"]\n",
        "    n_transformed = meta[\"n_transformed\"]\n",
        "    n_feat = meta[\"n_feat\"]\n",
        "    global_stats = meta[\"global_stats\"]\n",
        "    tds_params = meta[\"train_dataset_params\"]\n",
        "    feature_lists = meta.get(\"feature_lists\", {})\n",
        "    saved_torch_dtype = meta.get(\"torch_dtype\", \"float32\")\n",
        "\n",
        "    # 3) регрессор\n",
        "    model = SequenceTransformerRegressor(\n",
        "        seq_len=params[\"seq_len\"],\n",
        "        pred_len=params[\"pred_len\"],\n",
        "        hidden_size=params[\"hidden_size\"],\n",
        "        hidden_continuous_size=params[\"hidden_continuous_size\"],\n",
        "        epochs=params[\"epochs\"],\n",
        "        batch_size=params[\"batch_size\"],\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        patience=params[\"patience\"],\n",
        "        seed=params[\"seed\"],\n",
        "        dropout=params[\"dropout\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        verbose=params[\"verbose\"],\n",
        "        mask_prob=params[\"mask_prob\"],\n",
        "        infer_stride=params[\"infer_stride\"],\n",
        "        ckpt_path=None,\n",
        "        preprocessing=preprocessing,\n",
        "        min_encoder_length=params.get(\"min_encoder_length\", 1),\n",
        "    )\n",
        "\n",
        "    model._feature_columns = feature_columns\n",
        "    model._n_transformed = n_transformed\n",
        "    model._n_feat = n_feat\n",
        "    model.global_target_mean = float(global_stats[\"global_target_mean\"])\n",
        "    model.global_target_std = float(global_stats[\"global_target_std\"])\n",
        "    model.global_target_min = float(global_stats[\"global_target_min\"])\n",
        "    model.global_target_max = float(global_stats[\"global_target_max\"])\n",
        "    model.global_range = float(global_stats[\"global_range\"])\n",
        "    model.soft_clip_scale = float(global_stats[\"soft_clip_scale\"])\n",
        "    model.clip_scale = float(global_stats.get(\"clip_scale\", 1.5))\n",
        "    model.device = device\n",
        "\n",
        "    # 4) синтетический df\n",
        "    max_enc = int(tds_params[\"max_encoder_length\"])\n",
        "    max_pred = int(tds_params[\"max_prediction_length\"])\n",
        "    num_feat_cols = len(feature_columns)\n",
        "    rows = max_enc + max_pred\n",
        "\n",
        "    synth_df_base = pd.DataFrame(\n",
        "        np.zeros((rows, num_feat_cols), dtype=np.float32),\n",
        "        columns=feature_columns,\n",
        "    )\n",
        "    synth_df_base[\"time_idx\"] = np.arange(rows, dtype=np.int64)\n",
        "    synth_df_base[\"target\"] = np.zeros(rows, dtype=np.float32)\n",
        "    synth_df_base[\"batch\"] = 0\n",
        "\n",
        "    # Попытка №1: «обычная» конструкция TDS как при обучении\n",
        "    tds_kwargs = dict(\n",
        "        time_idx=tds_params[\"time_idx\"],\n",
        "        target=tds_params[\"target\"],\n",
        "        group_ids=tds_params[\"group_ids\"],\n",
        "        max_encoder_length=tds_params[\"max_encoder_length\"],\n",
        "        max_prediction_length=tds_params[\"max_prediction_length\"],\n",
        "        target_normalizer=None,\n",
        "        allow_missing_timesteps=tds_params[\"allow_missing_timesteps\"],\n",
        "        add_relative_time_idx=tds_params[\"add_relative_time_idx\"],\n",
        "        add_target_scales=tds_params[\"add_target_scales\"],\n",
        "        add_encoder_length=tds_params[\"add_encoder_length\"],\n",
        "        min_prediction_length=tds_params[\"min_prediction_length\"],\n",
        "        min_encoder_length=tds_params.get(\"min_encoder_length\", 0),\n",
        "    )\n",
        "\n",
        "    if \"time_varying_unknown_reals\" in feature_lists:\n",
        "        tds_kwargs[\"time_varying_unknown_reals\"] = list(feature_lists[\"time_varying_unknown_reals\"])\n",
        "    else:\n",
        "        tds_kwargs[\"time_varying_unknown_reals\"] = list(feature_columns)\n",
        "\n",
        "    for key in [\n",
        "        \"time_varying_known_reals\",\n",
        "        \"static_reals\",\n",
        "        \"time_varying_known_categoricals\",\n",
        "        \"time_varying_unknown_categoricals\",\n",
        "        \"static_categoricals\",\n",
        "        \"known_reals\",\n",
        "        \"unknown_reals\",\n",
        "        \"known_categoricals\",\n",
        "        \"unknown_categoricals\",\n",
        "    ]:\n",
        "        if key in feature_lists:\n",
        "            tds_kwargs[key] = list(feature_lists[key])\n",
        "\n",
        "    dataset = TimeSeriesDataSet(synth_df_base.copy(), **tds_kwargs)\n",
        "\n",
        "    # Проверяем порядок reals\n",
        "    saved_reals = feature_lists.get(\"reals\")\n",
        "    rebuild_forced = False\n",
        "    if saved_reals is not None:\n",
        "        # В PF список dataset.reals может быть кортежами/объектами -- приводим к строкам\n",
        "        ds_reals = list(getattr(dataset, \"reals\", []))\n",
        "        ds_reals = [str(x) for x in ds_reals]\n",
        "        saved_reals_str = [str(x) for x in saved_reals]\n",
        "        if ds_reals != saved_reals_str:\n",
        "            rebuild_forced = True\n",
        "\n",
        "    if rebuild_forced:\n",
        "        # Попытка №2: Форсируем порядок каналов reals.\n",
        "        # Для этого отключим автоматические добавления и сами сгенерируем служебные колонки\n",
        "        # encoder_length и relative_time_idx в synth_df.\n",
        "        synth_df = synth_df_base.copy()\n",
        "        # relative_time_idx: от 0 до rows-1\n",
        "        synth_df[\"relative_time_idx\"] = np.arange(rows, dtype=np.int64)\n",
        "        # encoder_length: длина encoder для каждой позиции; для синтетики можно установить константу max_enc\n",
        "        # PF ожидает целочисленную encoder_length, соответствующую длине энкодера на каждом шаге.\n",
        "        # Для инициализации архитектуры достаточно положить валидные числа.\n",
        "        enc_len = np.zeros(rows, dtype=np.int64)\n",
        "        enc_len[:max_enc] = np.arange(1, max_enc + 1, dtype=np.int64)\n",
        "        enc_len[max_enc:] = max_enc\n",
        "        synth_df[\"encoder_length\"] = enc_len\n",
        "\n",
        "        # Теперь задаём TDS без add_* фичей, и передаём time_varying_unknown_reals в порядке saved_reals.\n",
        "        # saved_reals начинается с [\"encoder_length\",\"relative_time_idx\", ... f0..fN]\n",
        "        tds_kwargs_forced = dict(\n",
        "            time_idx=tds_params[\"time_idx\"],\n",
        "            target=tds_params[\"target\"],\n",
        "            group_ids=tds_params[\"group_ids\"],\n",
        "            max_encoder_length=tds_params[\"max_encoder_length\"],\n",
        "            max_prediction_length=tds_params[\"max_prediction_length\"],\n",
        "            target_normalizer=None,\n",
        "            allow_missing_timesteps=tds_params[\"allow_missing_timesteps\"],\n",
        "            add_relative_time_idx=False,\n",
        "            add_target_scales=tds_params[\"add_target_scales\"],\n",
        "            add_encoder_length=False,\n",
        "            min_prediction_length=tds_params[\"min_prediction_length\"],\n",
        "            min_encoder_length=tds_params.get(\"min_encoder_length\", 0),\n",
        "            time_varying_unknown_reals=list(saved_reals),  # порядок каналов фиксируем здесь\n",
        "            # Не задаём known/unknown cats/reals дополнительно, чтобы не нарушить порядок\n",
        "        )\n",
        "        dataset = TimeSeriesDataSet(synth_df, **tds_kwargs_forced)\n",
        "\n",
        "        # Контроль: проверим снова порядок\n",
        "        ds_reals2 = [str(x) for x in list(getattr(dataset, \"reals\", []))]\n",
        "        if ds_reals2 != [str(x) for x in saved_reals]:\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to force reals order. Got {ds_reals2[:8]}..., expected {list(saved_reals)[:8]}...\"\n",
        "            )\n",
        "\n",
        "    # 5) TFT 1-в-1\n",
        "    tft = CustomTFT.from_dataset(\n",
        "        dataset,\n",
        "        hidden_size=int(model.hidden_size),\n",
        "        output_size=int(params.get(\"output_size\", 1)),\n",
        "        loss=PeakFriendlyHuber(\n",
        "            delta=0.5,\n",
        "            peak_thr=0.85,\n",
        "            peak_weight=1.3,\n",
        "            contrast_weight=0.03,\n",
        "            center_band=0.3,\n",
        "            clip_scale=1.5,\n",
        "        ),\n",
        "        optimizer=\"adam\",\n",
        "        learning_rate=float(model.learning_rate),\n",
        "        lstm_layers=int(params.get(\"lstm_layers\", 3)),\n",
        "        hidden_continuous_size=int(model.hidden_continuous_size),\n",
        "        attention_head_size=int(params.get(\"attention_head_size\", 4)),\n",
        "        dropout=float(model._dropout),\n",
        "        reduce_on_plateau_patience=5,\n",
        "        reduce_on_plateau_min_lr=1e-6,\n",
        "        weight_decay=float(model.weight_decay),\n",
        "        mask_prob=float(model.mask_prob),\n",
        "        output_transformer=tft_output_transformer,\n",
        "    )\n",
        "\n",
        "    model._model = TFTAdapter(tft)\n",
        "    model._train_dataset = dataset\n",
        "\n",
        "    # 6) загрузка весов\n",
        "    weights_zip_path = os.path.join(path, model_zip_filename)\n",
        "    with zipfile.ZipFile(weights_zip_path, mode='r') as zf:\n",
        "        if \"tft_state_dict.pt\" not in zf.namelist():\n",
        "            raise RuntimeError(\"tft_state_dict.pt not found in weights zip\")\n",
        "        state = torch.load(io.BytesIO(zf.read(\"tft_state_dict.pt\")), map_location=\"cpu\")\n",
        "\n",
        "    missing, unexpected = tft.load_state_dict(state, strict=False)\n",
        "    if missing or unexpected:\n",
        "        warnings.warn(f\"load_state_dict: missing={missing}, unexpected={unexpected}\")\n",
        "\n",
        "    # 7) eval + dropout off\n",
        "    tft.eval()\n",
        "    for m in tft.modules():\n",
        "        if isinstance(m, torch.nn.Dropout):\n",
        "            m.p = 0.0\n",
        "\n",
        "    model._model.to(device)\n",
        "\n",
        "    restored = Pipeline([\n",
        "        (\"preprocessing\", preprocessing),\n",
        "        (\"to_dense\", FunctionTransformer(to_dense)),\n",
        "        (\"model\", model),\n",
        "    ])\n",
        "    return restored\n",
        "\n",
        "def apply_linear_calibration(y_pred: np.ndarray, calib: dict) -> np.ndarray:\n",
        "    a, b = calib.get('a', 1.0), calib.get('b', 0.0)\n",
        "    return a * np.ravel(y_pred) + b\n",
        "\n",
        "def ranker_postprocess_minus1_1(y_pred_ranker: np.ndarray) -> np.ndarray:\n",
        "    yp = np.ravel(y_pred_ranker)\n",
        "    return 2.0 * ((yp + 15.0) / (5.0 + 15.0)) - 1.0\n",
        "\n",
        "def load_optimized_pipeline(out_dir, model_type='regressor'):\n",
        "    \"\"\"\n",
        "    Загружает оптимизированный pipeline.\n",
        "    Возвращает полный pipe.\n",
        "    \"\"\"\n",
        "    # Загружаем preproc\n",
        "    preproc_file = f'preproc_{model_type}.pkl' if model_type == 'ranker' else 'preproc.pkl'\n",
        "    preproc = joblib.load(os.path.join(out_dir, preproc_file))\n",
        "\n",
        "    # Booster из gz\n",
        "    gz_file = os.path.join(out_dir, f'{model_type}.txt.gz')\n",
        "    temp_file = os.path.join(out_dir, f'temp_{model_type}.txt')\n",
        "    with gzip.open(gz_file, 'rb') as f_in:\n",
        "        with open(temp_file, 'wb') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    booster = lgb.Booster(model_file=temp_file)\n",
        "    os.remove(temp_file)\n",
        "\n",
        "    # Реконструируем модель (LGBM wrapper)\n",
        "    step_name = 'model' if model_type == 'regressor' else 'ranker'\n",
        "    if model_type == 'regressor':\n",
        "        model = lgb.LGBMRegressor()\n",
        "    elif model_type == 'ranker':\n",
        "        model = lgb.LGBMRanker()\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model_type\")\n",
        "\n",
        "    model._Booster = booster\n",
        "    model.fitted_ = True  # Отметить как обученную\n",
        "    # Устанавливаем n_features_in_ из booster\n",
        "    model.n_features_in_ = booster.num_feature()\n",
        "\n",
        "    # Полный pipe\n",
        "    pipe = Pipeline([\n",
        "        ('preprocessing', preproc),\n",
        "        ('to_dense', FunctionTransformer(to_dense, feature_names_out=\"one-to-one\")),\n",
        "        (step_name, model),\n",
        "    ])\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def load_model_for_ticker(ticker, models_dir=r'C:\\Users\\aleksandrovva1\\Desktop\\data science\\0-trade\\t\\models'):\n",
        "    \"\"\"\n",
        "    Загружает модель(и) для тикера из директории models//.\n",
        "    Возвращает dict с: pipe_reg, pipe_rank (если combined), meta, threshold.\n",
        "    \"\"\"\n",
        "    out_dir = os.path.join(models_dir, ticker)\n",
        "    if not os.path.exists(out_dir):\n",
        "        raise FileNotFoundError(f\"Directory for {ticker} not found: {out_dir}\")\n",
        "\n",
        "    # Загружаем meta\n",
        "    with open(os.path.join(out_dir, 'meta.json'), 'r') as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    best_method = meta['best_method']\n",
        "    thresh = meta.get('sell_threshold', 0.0)  # Fallback\n",
        "\n",
        "    pipe_reg = None\n",
        "    pipe_rank = None\n",
        "\n",
        "    if best_method == 'regressor':\n",
        "        pipe_reg = load_optimized_pipeline(out_dir, 'regressor')\n",
        "\n",
        "    elif best_method == 'ranker':\n",
        "        pipe_rank = load_optimized_pipeline(out_dir, 'ranker')\n",
        "\n",
        "    elif best_method == 'combined':\n",
        "        pipe_reg = load_optimized_pipeline(out_dir, 'regressor')\n",
        "        pipe_rank = load_optimized_pipeline(out_dir, 'ranker')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown best_method: {best_method}\")\n",
        "\n",
        "    return {\n",
        "        'best_method': best_method,\n",
        "        'pipe_reg': pipe_reg,\n",
        "        'pipe_rank': pipe_rank,\n",
        "        'meta': meta,\n",
        "        'threshold': thresh,\n",
        "        'calib_reg': meta.get('reg_calibration', {'a': 1.0, 'b': 0.0}),\n",
        "        'w_reg': meta.get('best_w_reg', 0.5) if best_method == 'combined' else None\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_mif(ticker_symbol, benchmark_symbol,\n",
        "                  window=20, volatility_ratio_weight=0.6):\n",
        "    \"\"\"\n",
        "    MIF для российских акций с учетом особенностей рынка.\n",
        "\n",
        "    :param ticker_symbol: Тикер акции (например: 'SBER.ME', 'GAZP.ME')\n",
        "    :param benchmark_symbol: Эталонный индекс (по умолчанию IMOEX.ME)\n",
        "    \"\"\"\n",
        "    # Загрузка данных с учетом постсуффиксов Московской биржи\n",
        "    data = pd.DataFrame(\n",
        "        {'ticker_symbol': close_price[ticker_symbol], 'benchmark_symbol': close_price[benchmark_symbol]})\n",
        "\n",
        "    # Фильтрация некорректных данных\n",
        "    data = data.dropna(how='all').ffill().bfill()\n",
        "\n",
        "    # Рассчет доходностей\n",
        "    returns = data[ticker_symbol].pct_change().dropna()\n",
        "    bench_returns = data[benchmark_symbol].pct_change().dropna()\n",
        "\n",
        "    # Выравнивание индексов\n",
        "    aligned_returns, aligned_bench = returns.align(bench_returns, join='inner')\n",
        "\n",
        "    # Расчет компонентов\n",
        "    rolling_corr = aligned_returns.rolling(window).corr(aligned_bench)\n",
        "    volatility_ratio = aligned_bench.rolling(window).std() / aligned_returns.rolling(window).std()\n",
        "\n",
        "    # Нормализация\n",
        "    norm_corr = (rolling_corr + 1) / 2\n",
        "    norm_vol_ratio = np.clip(volatility_ratio, 0, 2) / 2\n",
        "\n",
        "    # Итоговый индекс\n",
        "    mif = (volatility_ratio_weight * norm_vol_ratio +\n",
        "           (1 - volatility_ratio_weight) * norm_corr)\n",
        "\n",
        "    return mif.dropna()\n",
        "\n",
        "\n",
        "def send_buy_signal_to_telegram(ticker, price):\n",
        "    \"\"\"Отправка сигнала покупки в Telegram.\"\"\"\n",
        "    if not tg_channels or 'a1_screen' not in tg_channels:\n",
        "        return\n",
        "\n",
        "    message = f\"Сигнал покупки: {ticker} на уровне {price}\"\n",
        "    bot.send_message(tg_channels['a1_screen'], message)\n",
        "\n",
        "\n",
        "def send_sell_signal_to_telegram(ticker, price):\n",
        "    \"\"\"Отправка сигнала продажи в Telegram.\"\"\"\n",
        "    if not tg_channels or 'a1_screen' not in tg_channels:\n",
        "        return\n",
        "\n",
        "    message = f\"Сигнал продажи: {ticker} на уровне {price}\"\n",
        "    bot.send_message(tg_channels['a1_screen'], message)\n",
        "\n",
        "\n",
        "def send_error_to_telegram(error_message):\n",
        "    \"\"\"Отправка сообщения об ошибке в Telegram.\"\"\"\n",
        "    if not tg_channels or 'error' not in tg_channels:\n",
        "        return\n",
        "\n",
        "    bot.send_message(tg_channels['error'], f\"Ошибка: {error_message}\")\n",
        "\n",
        "\n",
        "def calculate_window_sums(arr, window_size=9):\n",
        "    \"\"\"Вычисление сумм в окне.\"\"\"\n",
        "    if not arr:\n",
        "        return []\n",
        "    return [sum(arr[max(0, i - window_size + 1):i + 1]) for i in range(len(arr))]\n",
        "\n",
        "\n",
        "def get_formatted_time(timestamp):\n",
        "    dt_object = datetime.datetime.fromtimestamp(timestamp / 1000.0)\n",
        "    time_str = dt_object.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    return time_str\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "\n",
        "\n",
        "def prepare_regime_params(optuna_params):\n",
        "    \"\"\"\n",
        "    Преобразует параметры из формата Optuna в два словаря: базовые параметры режимов и параметры расчета.\n",
        "\n",
        "    Args:\n",
        "        optuna_params (dict): Словарь с параметрами из Optuna\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь с двумя ключами: 'base_params' (параметры режимов) и 'calc_params' (остальные параметры)\n",
        "    \"\"\"\n",
        "    # Инициализируем словари для базовых параметров и параметров расчета\n",
        "    start_params = {}\n",
        "    base_params = {}\n",
        "    calc_params = {}\n",
        "\n",
        "    # Сначала обрабатываем параметры режимов (0-4)\n",
        "\n",
        "    start_params['moving_average_length'] = optuna_params.get('moving_average_length', 14)\n",
        "    start_params['atr_period'] = optuna_params.get('atr_period', 10)\n",
        "    for regime in range(5):\n",
        "        regime_key = f'regime_{regime}_'\n",
        "        regime_params = {}\n",
        "\n",
        "        # Основные параметры режима\n",
        "        regime_params['average_type'] = optuna_params.get(f'{regime_key}average_type', 'SMA')\n",
        "        regime_params['moving_average_length'] = optuna_params.get(f'{regime_key}ma_length', 50)\n",
        "        regime_params['atr_period'] = optuna_params.get(f'{regime_key}atr_period', 14)\n",
        "        regime_params['atr_multiplier'] = optuna_params.get(f'{regime_key}atr_multiplier', 3.0)\n",
        "\n",
        "        # Параметры AMA, если они есть\n",
        "        ama_atr_period = optuna_params.get(f'{regime_key}ama_atr_period')\n",
        "        ama_min_period = optuna_params.get(f'{regime_key}ama_min_period')\n",
        "        ama_max_period = optuna_params.get(f'{regime_key}ama_max_period')\n",
        "\n",
        "        if regime_params['average_type'] == 'AMA' and all(p is not None for p in [ama_atr_period, ama_min_period, ama_max_period]):\n",
        "            regime_params['ama_params'] = {\n",
        "                'atr_period': int(ama_atr_period),\n",
        "                'min_period': int(ama_min_period),\n",
        "                'max_period': int(ama_max_period)\n",
        "            }\n",
        "\n",
        "        base_params[regime] = regime_params\n",
        "\n",
        "    # Теперь собираем все остальные параметры в calc_params\n",
        "    other_params = [\n",
        "        'rsi_length', 'use_smoothing', 'smoothing_length', 'smoothing_type',\n",
        "        'alma_sigma', 'rsi_overbought', 'rsi_oversold', 'use_knn',\n",
        "        'knn_neighbors', 'knn_lookback', 'knn_weight', 'feature_count',\n",
        "        'use_filter', 'filter_method', 'filter_strength', 'sma_length',\n",
        "        'ema_length', 'rsi_helbuth'\n",
        "    ]\n",
        "\n",
        "    for param in other_params:\n",
        "        if param in optuna_params:\n",
        "            calc_params[param] = optuna_params[param]\n",
        "\n",
        "    return {\n",
        "        'start_params': start_params,\n",
        "        'base_params': base_params,\n",
        "        'calc_params': calc_params\n",
        "    }\n",
        "\n",
        "params_for_classifier = {\n",
        "    'ppo_short_window': 27,\n",
        "    'cmo_window': 40,\n",
        "    'ppo_long_window': 127,\n",
        "    'macd_fast_period': [91],\n",
        "    'macd_slow_period': [76],\n",
        "    'macd_signal_period': [39],\n",
        "    'rsi_period': 197,\n",
        "    'williams_r_window': 33,\n",
        "    'overbought_rsi_period': 124,\n",
        "    'overbought_stochastic_period': 200,\n",
        "    'fear_greed_window': 121,\n",
        "    'bollinger_window': 155,\n",
        "    'atr_window': 159,\n",
        "    'asset_growth_window': 5,\n",
        "    'volume_window': 149,\n",
        "    'volume_quantile': 0.8591071502449321\n",
        "}\n",
        "\n",
        "class ClassifierFeatureCalculator:\n",
        "    def __init__(self, df, window=14):\n",
        "        \"\"\"\n",
        "        Инициализация класса для расчета технических индикаторов.\n",
        "        :param df: DataFrame с исходными данными\n",
        "        :param window: окно для скользящих расчетов\n",
        "        :param ticker: тикер актива\n",
        "        \"\"\"\n",
        "        self.df = df.copy()\n",
        "        self.WINDOW = window\n",
        "        # self.ticker = ticker\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "        \"\"\"\n",
        "        Подготовка данных: проверка индекса, сортировка и базовые преобразования.\n",
        "        \"\"\"\n",
        "        self.df = self.df.reset_index(drop=True)\n",
        "        # Убедимся, что данные имеют правильный формат\n",
        "        self.df = self.df[['time', 'open', 'close', 'high', 'low', 'volume', 'buy_signal', 'target']].apply(\n",
        "            pd.to_numeric, errors='coerce'\n",
        "        )\n",
        "        # self.df['ticker'] = self.ticker\n",
        "\n",
        "    def calculate_cmo(self, cmo_window=14):\n",
        "        \"\"\"\n",
        "        Расчет Chande Momentum Oscillator (CMO) для всего DataFrame.\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "\n",
        "        # Расчет суммы высоких и низких цен за период\n",
        "        # Используем векторизованные операции вместо apply\n",
        "        max_window = close.rolling(cmo_window).max()\n",
        "        min_window = close.rolling(cmo_window).min()\n",
        "\n",
        "        # Расчет количества максимумов и минимумов в окне\n",
        "        # Используем broadcast для сравнения\n",
        "        is_max = close.rolling(cmo_window).apply(lambda x: (x == x.max()).sum(), raw=True)\n",
        "        is_min = close.rolling(cmo_window).apply(lambda x: (x == x.min()).sum(), raw=True)\n",
        "\n",
        "        sum_high = is_max * max_window\n",
        "        sum_low = is_min * min_window\n",
        "\n",
        "        # Расчет CMO\n",
        "        cmo = 100 * (sum_high - sum_low) / (sum_high + sum_low + 1e-10)\n",
        "        self.df['CMO'] = cmo\n",
        "        self.df['ago20_CMO'] = self.df['CMO'].shift(20)\n",
        "        self.df['ago50_CMO'] = self.df['CMO'].shift(50)\n",
        "\n",
        "        return self.df\n",
        "\n",
        "    def calculate_williams_r(self, williams_r_window=14):\n",
        "        \"\"\"\n",
        "        Расчет Williams %R для всего DataFrame.\n",
        "        \"\"\"\n",
        "        high = self.df['high']\n",
        "        low = self.df['low']\n",
        "        close = self.df['close']\n",
        "\n",
        "        highest_high = high.rolling(williams_r_window).max()\n",
        "        lowest_low = low.rolling(williams_r_window).min()\n",
        "\n",
        "        williams_r = ((highest_high - close) / (highest_high - lowest_low)) * (-100)\n",
        "        self.df['WILLR'] = williams_r\n",
        "        self.df['ago50_WILLR'] = self.df['WILLR'].shift(50)\n",
        "        self.df['ago20_WILLR'] = self.df['WILLR'].shift(20)\n",
        "        self.df['ago3_WILLR'] = self.df['WILLR'].shift(3)\n",
        "\n",
        "    def calculate_macd(self, macd_fast_periods=[12], macd_slow_periods=[26], macd_signal_periods=[9]):\n",
        "        \"\"\"\n",
        "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "\n",
        "        # Создаем множества для уникальных периодов\n",
        "        unique_fast = set(macd_fast_periods)\n",
        "        unique_slow = set(macd_slow_periods)\n",
        "\n",
        "        # Предварительно вычисляем все необходимые EMA и скользящие средние\n",
        "        ema_cache = {}\n",
        "        rolling_cache = {}\n",
        "\n",
        "        # Кешируем быстрые EMA\n",
        "        for fp in unique_fast:\n",
        "            ema_cache[f'ema_{fp}'] = close.ewm(span=fp, adjust=False).mean()\n",
        "\n",
        "        # Кешируем медленные EMA и скользящие средние\n",
        "        for sp in unique_slow:\n",
        "            ema_cache[f'ema_{sp}'] = close.ewm(span=sp, adjust=False).mean()\n",
        "            rolling_cache[f'rolling_{sp}'] = close.rolling(window=sp).mean()\n",
        "\n",
        "        # Основной цикл вычислений\n",
        "        for fp in macd_fast_periods:\n",
        "            ema_fast = ema_cache[f'ema_{fp}']\n",
        "            for sp in macd_slow_periods:\n",
        "                ema_slow = ema_cache[f'ema_{sp}']\n",
        "                rolling_mean = rolling_cache[f'rolling_{sp}']\n",
        "\n",
        "                # Вычисляем MACD и нормализацию\n",
        "                macd = ema_fast - ema_slow\n",
        "                macd_norm = macd / rolling_mean\n",
        "\n",
        "                # Сохраняем MACD только один раз для комбинации fp/sp\n",
        "                self.df[f'MACD'] = macd_norm\n",
        "\n",
        "                # Обрабатываем сигнальные периоды\n",
        "                for sig in macd_signal_periods:\n",
        "                    # Вычисляем сигнальную линию\n",
        "                    signal = macd.ewm(span=sig, adjust=False).mean()\n",
        "                    signal_norm = signal / rolling_mean\n",
        "\n",
        "                    # Сохраняем результаты\n",
        "                    self.df[f'MACD_Signal'] = signal_norm\n",
        "                    self.df[f'MACD_Hist'] = macd_norm - signal_norm\n",
        "        return self.df\n",
        "\n",
        "    def calculate_rsi(self, rsi_period=14):\n",
        "        \"\"\"\n",
        "        Расчет RSI и его сдвиги.\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "        delta = close.diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(rsi_period).mean()\n",
        "        avg_loss = loss.rolling(rsi_period).mean()\n",
        "        rs = avg_gain / (avg_loss + 1e-10)\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        self.df['RSI'] = rsi\n",
        "        self.df['ago10_RSI'] = self.df['RSI'].shift(10)\n",
        "        self.df['ago3_RSI'] = self.df['RSI'].shift(3)\n",
        "        self.df['ago50_RSI'] = self.df['RSI'].shift(50)\n",
        "        self.df['ago1_RSI'] = self.df['RSI'].shift(1)\n",
        "        self.df['ago5_RSI'] = self.df['RSI'].shift(5)\n",
        "\n",
        "    def calculate_overbought_oversold_index(self, rsi_window=14, stoch_window=14):\n",
        "        \"\"\"\n",
        "        Расчет индекса перекупленности/перепроданности.\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "        low = self.df['low']\n",
        "        high = self.df['high']\n",
        "\n",
        "        # RSI\n",
        "        delta = close.diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(rsi_window).mean()\n",
        "        avg_loss = loss.rolling(rsi_window).mean()\n",
        "        rs = avg_gain / (avg_loss + 1e-10)\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # Stochastic\n",
        "        lowest_low = low.rolling(stoch_window).min()\n",
        "        highest_high = high.rolling(stoch_window).max()\n",
        "        stochastic = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-10)\n",
        "\n",
        "        # Индекс перекупленности/перепроданности\n",
        "        overbought_oversold = (rsi + stochastic) / 2\n",
        "        self.df['Overbought_Oversold_Index'] = overbought_oversold\n",
        "        self.df['ago50_Overbought_Oversold_Index'] = self.df['Overbought_Oversold_Index'].shift(50)\n",
        "        self.df['ago10_Overbought_Oversold_Index'] = self.df['Overbought_Oversold_Index'].shift(10)\n",
        "\n",
        "    def calculate_fear_greed_index(self, fear_greed_window=25):\n",
        "        \"\"\"\n",
        "        Расчет индекса страха и жадности и сдвигов.\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "        volume = self.df['volume']\n",
        "\n",
        "        # Пример расчета страха/жадности\n",
        "        volatility = close.pct_change().rolling(window=fear_greed_window).std()\n",
        "        volume_change = volume.pct_change().rolling(window=fear_greed_window).mean()\n",
        "        price_trend = close / close.rolling(window=fear_greed_window).mean()\n",
        "\n",
        "        fear_greed = (volatility + volume_change + price_trend) / 3 * 100\n",
        "        self.df['Fear_Greed_Index'] = fear_greed\n",
        "        self.df['ago50_Fear_Greed_Index'] = self.df['Fear_Greed_Index'].shift(50)\n",
        "        self.df['ago30_Fear_Greed_Index'] = self.df['Fear_Greed_Index'].shift(30)\n",
        "\n",
        "    def calculate_bollinger_bands(self, bollinger_window=20):\n",
        "        \"\"\"\n",
        "        Расчет Bollinger Bands (ширины полос) и сдвигов.\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "        ma = close.rolling(bollinger_window).mean()\n",
        "        std = close.rolling(bollinger_window).std()\n",
        "        bb_width = (2 * std) / ma\n",
        "\n",
        "        self.df['BB_Width'] = bb_width\n",
        "        self.df['ago30_BB_Width'] = self.df['BB_Width'].shift(30)\n",
        "\n",
        "    def calc_price_features(self):\n",
        "        # Добавляем защиту от деления на ноль\n",
        "        epsilon = 1e-10\n",
        "\n",
        "        self.df['perc_var_open_close'] = ((self.df['close'] - self.df['open']) / (self.df['open'] + epsilon)) * 100\n",
        "        self.df['candle_range_perc'] = ((self.df['high'] - self.df['low']) / (self.df['open'] + epsilon)) * 100\n",
        "\n",
        "        self.df['ago3_perc_var_open_close'] = self.df['perc_var_open_close'].shift(3)\n",
        "        self.df['ago5_perc_var_open_close'] = self.df['perc_var_open_close'].shift(5)\n",
        "        self.df['ago30_perc_var_open_close'] = self.df['perc_var_open_close'].shift(30)\n",
        "\n",
        "        self.df['ago30_candle_range_perc'] = self.df['candle_range_perc'].shift(30)\n",
        "        self.df['ago5_candle_range_perc'] = self.df['candle_range_perc'].shift(5)\n",
        "        self.df['ago3_candle_range_perc'] = self.df['candle_range_perc'].shift(3)\n",
        "\n",
        "    def calculate_overbought_oversold_index(self, overbought_rsi_period=25, overbought_stochastic_period=50):\n",
        "        \"\"\"\n",
        "        Расчет индекса перекупленности/перепроданности для нескольких окон с оптимизацией.\n",
        "        :param rsi_periods: Список периодов для RSI.\n",
        "        :param stochastic_periods: Список периодов для стохастического осциллятора.\n",
        "        \"\"\"\n",
        "        df = self.df\n",
        "\n",
        "        # Предварительные вычисления для RSI\n",
        "        delta = df['close'].diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = (-delta).where(delta < 0, 0)\n",
        "\n",
        "        avg_gain = gain.rolling(window=overbought_rsi_period).mean()\n",
        "        avg_loss = loss.rolling(window=overbought_rsi_period).mean()\n",
        "        rs = avg_gain / (avg_loss + 1e-10)\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # Предварительные вычисления для Stochastic\n",
        "        stochastic_dict = {}\n",
        "        low_min = df['low'].rolling(window=overbought_stochastic_period).min()\n",
        "        high_max = df['high'].rolling(window=overbought_stochastic_period).max()\n",
        "        stochastic = 100 * (df['close'] - low_min) / (high_max - low_min + 1e-10)\n",
        "\n",
        "        # Генерация всех комбинаций и расчет индекса\n",
        "        combined_index = (rsi + stochastic) / 2\n",
        "        self.df[f'Overbought_Oversold_Index'] = combined_index\n",
        "        self.df[f'ago10_Overbought_Oversold_Index'] = self.df[f'Overbought_Oversold_Index'].shift(10)\n",
        "        self.df[f'ago50_Overbought_Oversold_Index'] = self.df[f'Overbought_Oversold_Index'].shift(50)\n",
        "\n",
        "    def calculate_atr(self, atr_window=14):\n",
        "        \"\"\"\n",
        "        Расчет ATR и его сдвигов.\n",
        "        \"\"\"\n",
        "        high = self.df['high']\n",
        "        low = self.df['low']\n",
        "        close = self.df['close']\n",
        "\n",
        "        tr1 = high - low\n",
        "        tr2 = np.abs(high - close.shift(1))\n",
        "        tr3 = np.abs(low - close.shift(1))\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_window).mean()\n",
        "\n",
        "        self.df['ATR'] = atr\n",
        "        self.df['ago10_ATR'] = self.df['ATR'].shift(10)\n",
        "\n",
        "    def calculate_ppo(self, short_window=12, long_window=26):\n",
        "        \"\"\"\n",
        "        Расчет Percentage Price Oscillator (PPO) для всего DataFrame.\n",
        "        \"\"\"\n",
        "        close = self.df['close']\n",
        "\n",
        "        # Расчет скользящих средних\n",
        "        short_ma = close.rolling(short_window).mean()\n",
        "        long_ma = close.rolling(long_window).mean()\n",
        "\n",
        "        # Расчет PPO\n",
        "        ppo = ((short_ma - long_ma) / long_ma) * 100\n",
        "        self.df['PPO'] = ppo\n",
        "        self.df['ago50_PPO'] = self.df['PPO'].shift(50)\n",
        "\n",
        "    def calculate_asset_growth(self, asset_growth_window=3):\n",
        "        \"\"\"\n",
        "        Расчет Asset Growth и его сдвигов.\n",
        "        \"\"\"\n",
        "        asset = self.df['close']\n",
        "        asset_growth = asset.pct_change(asset_growth_window).fillna(0) * 100\n",
        "        self.df['Asset_Growth'] = asset_growth\n",
        "        self.df['ago50_Asset_Growth'] = self.df['Asset_Growth'].shift(50)\n",
        "        self.df['ago5_Asset_Growth'] = self.df['Asset_Growth'].shift(5)\n",
        "        self.df['ago30_Asset_Growth'] = self.df['Asset_Growth'].shift(30)\n",
        "\n",
        "    def calculate_asset_to_equity_ratio(self):\n",
        "        \"\"\"\n",
        "        Расчет Asset to Equity Ratio и его сдвигов.\n",
        "        \"\"\"\n",
        "        asset = self.df['close']\n",
        "        equity = self.df['low']\n",
        "        ratio = asset / (equity + 1e-10)\n",
        "\n",
        "        self.df['Asset_To_Equity_Ratio'] = ratio\n",
        "        self.df['ago50_Asset_To_Equity_Ratio'] = self.df['Asset_To_Equity_Ratio'].shift(50)\n",
        "        self.df['ago5_Asset_To_Equity_Ratio'] = self.df['Asset_To_Equity_Ratio'].shift(5)\n",
        "        self.df['ago30_Asset_To_Equity_Ratio'] = self.df['Asset_To_Equity_Ratio'].shift(30)\n",
        "        self.df['ago3_Asset_To_Equity_Ratio'] = self.df['Asset_To_Equity_Ratio'].shift(3)\n",
        "\n",
        "    def calculate_volume_quantile_deviation(self, volume_window=100, volume_quantile=0.65):\n",
        "        volume = self.df['volume']\n",
        "        quantile_value = volume.rolling(window=volume_window).quantile(volume_quantile)\n",
        "        quantile_str = f\"{volume_quantile:.2f}\".replace('.', '_')  # Преобразование в строку\n",
        "        self.df[f\"Volume_Quantile_Deviation_%\"] = ((volume - quantile_value) / quantile_value) * 100\n",
        "        self.df[f\"ago3_Volume_Quantile_Deviation_%\"] = self.df[f\"Volume_Quantile_Deviation_%\"].shift(3)\n",
        "\n",
        "    def calculate_anomaly_score(self):\n",
        "        \"\"\"\n",
        "        Расчет аномалий через изолированный лес.\n",
        "        \"\"\"\n",
        "        self.df['price_change'] = self.df['close'].pct_change().fillna(0)\n",
        "        self.df['anomaly_score'] = IsolationForest(contamination=0.05).fit_predict(\n",
        "            self.df[['price_change', 'volume']]\n",
        "        )\n",
        "        self.df['ago1_anomaly_score'] = self.df['anomaly_score'].shift(1)\n",
        "        self.df['ago20_anomaly_score'] = self.df['anomaly_score'].shift(20)\n",
        "        self.df['ago30_anomaly_score'] = self.df['anomaly_score'].shift(30)\n",
        "\n",
        "        self.df['ago50_price_change30'] = self.df['close'].pct_change(30).fillna(0).shift(50)\n",
        "        self.df['ago30_price_change50'] = self.df['close'].pct_change(50).fillna(0).shift(30)\n",
        "        self.df['ago50_price_change'] = self.df['price_change'].shift(50)\n",
        "        self.df['ago10_price_change50'] = self.df['close'].pct_change(50).fillna(0).shift(10)\n",
        "        self.df['ago30_price_change60'] = self.df['close'].pct_change(60).fillna(0).shift(30)\n",
        "        self.df['ago30_price_change'] = self.df['price_change'].shift(30)\n",
        "        self.df['ago10_price_change70'] = self.df['close'].pct_change(70).fillna(0).shift(10)\n",
        "\n",
        "    def calculate_perc_var_open_close(self):\n",
        "        \"\"\"\n",
        "        Расчет процентного изменения открытия и закрытия.\n",
        "        \"\"\"\n",
        "        self.df['perc_var_open_close'] = (\n",
        "                                                 (self.df['close'] - self.df['open']) / (self.df['open'] + 1e-10)\n",
        "                                         ) * 100\n",
        "        self.df['ago50_perc_var_open_close'] = self.df['perc_var_open_close'].shift(50)\n",
        "\n",
        "    def calculate_trend(self, pct_window=6, window=464):\n",
        "        # Рассчитываем returns с заполнением вперед\n",
        "        returns = self.df['close'].pct_change(pct_window).fillna(method='ffill').fillna(0)\n",
        "        # Убедимся, что данных достаточно для STL\n",
        "        if len(returns) < 2 * window:\n",
        "            raise ValueError(f\"Need at least {2 * window} data points for window={window}\")\n",
        "\n",
        "        res = STL(returns, period=window).fit()\n",
        "\n",
        "        # Сбрасываем индекс для корректного shift()\n",
        "        # df = self.df.reset_index(drop=True)\n",
        "\n",
        "        self.df['trend'] = res.trend\n",
        "        self.df['ago50_trend'] = self.df['trend'].shift(50)\n",
        "\n",
        "        self.df['seasonal'] = res.seasonal\n",
        "        self.df['ago1_seasonal'] = self.df['seasonal'].shift(1)\n",
        "\n",
        "    def calculate_features(self, **params):\n",
        "        self.calculate_cmo(params.get('cmo_window', 14))\n",
        "        self.calculate_ppo(params.get('ppo_short_window', 12), params.get('ppo_long_window', 26))\n",
        "        self.calculate_macd(params.get('macd_fast_period', [12]),\n",
        "                            params.get('macd_slow_period', [26]),\n",
        "                            params.get('macd_signal_period', [9]))\n",
        "        self.calculate_rsi(params.get('rsi_period', 14))\n",
        "        self.calculate_williams_r(params.get('williams_r_window', 14))\n",
        "        self.calculate_overbought_oversold_index(params.get('overbought_rsi_period', 14),\n",
        "                                                 params.get('overbought_stochastic_period', 14))\n",
        "        self.calculate_fear_greed_index(params.get('fear_greed_window', 25))\n",
        "        self.calculate_bollinger_bands(params.get('bollinger_window', 20))\n",
        "        self.calculate_atr(params.get('atr_window', 14))\n",
        "        self.calculate_asset_growth(params.get('asset_growth_window', 3))\n",
        "        self.calculate_anomaly_score()\n",
        "        self.calculate_volume_quantile_deviation(params.get('volume_window', 100),\n",
        "                                                 params.get('volume_quantile', 0.65))\n",
        "        self.calculate_asset_to_equity_ratio()\n",
        "        self.calculate_trend()\n",
        "        self.calculate_perc_var_open_close()\n",
        "        self.calc_price_features()\n",
        "\n",
        "        numeric_cols = self.df.select_dtypes(include=[np.float64]).columns\n",
        "        self.df[numeric_cols] = self.df[numeric_cols].astype(np.float32)\n",
        "\n",
        "        return self.df.iloc[[-1]].copy()\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "def find_klines(ticker, lookback=40):\n",
        "    candles = []\n",
        "    try:\n",
        "        # Получаем временные метки свечей\n",
        "        time_list = time_last_kline_end[ticker]  # Формат: ['2025-06-03T14:30:00+00:00', ...]\n",
        "\n",
        "        # Получаем время покупки\n",
        "        buy_time_str = open_trades[ticker]['buy_time']  # Может быть в любом формате\n",
        "\n",
        "        # 1. Универсальное преобразование времени покупки в naive datetime\n",
        "        if isinstance(buy_time_str, str):\n",
        "            # Удаляем временную зону если она есть\n",
        "            if '+' in buy_time_str:\n",
        "                buy_time_str = buy_time_str.split('+')[0]\n",
        "            # Удаляем микросекунды если они есть\n",
        "            if '.' in buy_time_str:\n",
        "                buy_time_str = buy_time_str.split('.')[0]\n",
        "\n",
        "            # Парсим как naive datetime\n",
        "            buy_time = dt.strptime(buy_time_str, \"%Y-%m-%dT%H:%M:%S\")\n",
        "        else:\n",
        "            buy_time = buy_time_str\n",
        "            if buy_time.tzinfo is not None:  # Если есть временная зона\n",
        "                buy_time = buy_time.replace(tzinfo=None)  # Удаляем временную зону\n",
        "\n",
        "        # Добавляем 3 часа если нужно\n",
        "        buy_time += timedelta(hours=3)\n",
        "        buy_time = buy_time.replace(second=0, microsecond=0)\n",
        "\n",
        "        # 2. Преобразуем список временных меток в datetime\n",
        "        time_series = []\n",
        "        for t in time_list:\n",
        "            # Обрабатываем каждую временную метку аналогично buy_time\n",
        "            if isinstance(t, str):\n",
        "                if '+' in t:\n",
        "                    t = t.split('+')[0]\n",
        "                if '.' in t:\n",
        "                    t = t.split('.')[0]\n",
        "                t_dt = dt.strptime(t, \"%Y-%m-%dT%H:%M:%S\")\n",
        "            else:\n",
        "                t_dt = t\n",
        "                if t_dt.tzinfo is not None:\n",
        "                    t_dt = t_dt.replace(tzinfo=None)\n",
        "            time_series.append(t_dt)\n",
        "\n",
        "        # 3. Находим ближайшую временную метку\n",
        "        closest_idx = 0\n",
        "        min_diff = float('inf')\n",
        "        for i, t in enumerate(time_series):\n",
        "            diff = abs((t - buy_time).total_seconds())\n",
        "            if diff < min_diff:\n",
        "                min_diff = diff\n",
        "                closest_idx = i\n",
        "\n",
        "        # 4. Проверяем что разница не слишком большая (не более 30 минут)\n",
        "        if min_diff > 1800:  # 30 минут в секундах\n",
        "            print(f\"Предупреждение: для {ticker} большая разница во времени: {min_diff/60:.1f} минут\")\n",
        "\n",
        "        # 5. Определяем диапазон свечей\n",
        "        start_idx = max(0, closest_idx - lookback)\n",
        "        end_idx = len(time_series)  # Берем все свечи до конца\n",
        "\n",
        "        # 6. Формируем данные свечей\n",
        "        candles.append({\n",
        "            \"time\": time_last_kline_end[ticker][start_idx:end_idx],\n",
        "            \"open\": open_price[ticker][start_idx:end_idx],\n",
        "            \"close\": close_price[ticker][start_idx:end_idx],\n",
        "            \"high\": high_price[ticker][start_idx:end_idx],\n",
        "            \"low\": low_price[ticker][start_idx:end_idx],\n",
        "            \"volume\": volume[ticker][start_idx:end_idx],\n",
        "            \"pmax\": pmax[ticker][start_idx:end_idx],\n",
        "            \"ma\": ma[ticker][start_idx:end_idx],\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.send_message(\n",
        "                        self.error_tg,\n",
        "                        f\"{ticker} ошибка в find_klines\"\n",
        "                    )\n",
        "        raise  # Пробрасываем исключение дальше\n",
        "\n",
        "    return candles\n",
        "\n",
        "\n",
        "async def fetch_tinkoff_candles(client, figi, start_time, end_time, interval):\n",
        "    \"\"\"Получение исторических данных из Tinkoff API (асинхронная версия)\"\"\"\n",
        "    candles = []\n",
        "    current_time = start_time\n",
        "\n",
        "    while current_time < end_time:\n",
        "        try:\n",
        "            # Используем асинхронный генератор\n",
        "            async for candle in client.get_all_candles(\n",
        "                    figi=figi,\n",
        "                    from_=current_time,\n",
        "                    to=end_time,\n",
        "                    interval=interval):\n",
        "                candles.append({\n",
        "                    \"time\": candle.time + timedelta(hours=3),\n",
        "                    \"open\": float(candle.open.units + candle.open.nano * 1e-9),\n",
        "                    \"close\": float(candle.close.units + candle.close.nano * 1e-9),\n",
        "                    \"high\": float(candle.high.units + candle.high.nano * 1e-9),\n",
        "                    \"low\": float(candle.low.units + candle.low.nano * 1e-9),\n",
        "                    \"volume\": candle.volume,\n",
        "                })\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка получения данных: {e}\")\n",
        "            break\n",
        "\n",
        "    return candles\n",
        "\n",
        "\n",
        "def prepare_tinkoff_data(candles):\n",
        "    \"\"\"Подготовка данных из Tinkoff в DataFrame\"\"\"\n",
        "    if not candles:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(candles[0])\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    df = df.sort_values('time').drop_duplicates('time').reset_index(drop=True)\n",
        "    df.set_index('time', inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def calculate_indicators(df):\n",
        "    \"\"\"Расчет индикаторов для отчета\"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # Вычисляем VAR (Volatility Adjusted Ratio)\n",
        "    high_array = df['high'].values\n",
        "    low_array = df['low'].values\n",
        "    close_array = df['close'].values\n",
        "\n",
        "    # Функции для расчета индикаторов из TinkoffInvestDataCollector\n",
        "    def generate_var(high, low, period=10):\n",
        "        hl2 = (high + low) / 2\n",
        "        diff = np.diff(hl2, prepend=hl2[0])\n",
        "        vud1 = np.where(diff > 0, diff, 0)\n",
        "        vdd1 = np.where(diff < 0, -diff, 0)\n",
        "\n",
        "        vUD = np.convolve(vud1, np.ones(9) / 9, mode='valid')\n",
        "        vDD = np.convolve(vdd1, np.ones(9) / 9, mode='valid')\n",
        "\n",
        "        vCMO = (vUD - vDD) / (vUD + vDD + 1e-10)\n",
        "        vCMO = np.nan_to_num(vCMO, nan=0.0)\n",
        "\n",
        "        alpha = 2 / (period + 1)\n",
        "        var = np.zeros_like(hl2)\n",
        "        var_before = 0.0\n",
        "\n",
        "        for i in range(len(hl2)):\n",
        "            if i < len(vCMO):\n",
        "                var[i] = (alpha * abs(vCMO[i]) * hl2[i]) + (1 - alpha * abs(vCMO[i])) * var_before\n",
        "            else:\n",
        "                var[i] = var_before\n",
        "            var_before = var[i]\n",
        "\n",
        "        return var\n",
        "\n",
        "    def generate_pmax(var, close, high, low, atr_period=14, atr_multiplier=3):\n",
        "        \"\"\"Расчет PMAX индикатора\"\"\"\n",
        "        tr = np.maximum(high - low,\n",
        "                        np.maximum(abs(high - np.roll(close, 1)),\n",
        "                                   abs(low - np.roll(close, 1))))\n",
        "        atr = pd.Series(tr).rolling(atr_period).mean().values\n",
        "\n",
        "        pmax = []\n",
        "        previous_final_upper = 0\n",
        "        previous_final_lower = 0\n",
        "        previous_var = 0\n",
        "        previous_pmax = 0\n",
        "\n",
        "        for i in range(len(close)):\n",
        "            atrc = atr[i] if i < len(atr) and not np.isnan(atr[i]) else 0\n",
        "            varc = var[i] if i < len(var) else 0\n",
        "\n",
        "            upper = varc + atr_multiplier * atrc\n",
        "            lower = varc - atr_multiplier * atrc\n",
        "\n",
        "            final_upper = upper if (\n",
        "                    upper < previous_final_upper or previous_var > previous_final_upper) else previous_final_upper\n",
        "            final_lower = lower if (\n",
        "                    lower > previous_final_lower or previous_var < previous_final_lower) else previous_final_lower\n",
        "\n",
        "            if previous_pmax == previous_final_upper:\n",
        "                pmaxc = final_upper if varc <= final_upper else final_lower\n",
        "            else:\n",
        "                pmaxc = final_lower if varc >= final_lower else final_upper\n",
        "\n",
        "            pmax.append(pmaxc)\n",
        "            previous_var = varc\n",
        "            previous_final_upper = final_upper\n",
        "            previous_final_lower = final_lower\n",
        "            previous_pmax = pmaxc\n",
        "\n",
        "        return pmax\n",
        "\n",
        "    # Вычисляем индикаторы\n",
        "    var = generate_var(high_array, low_array)\n",
        "    pmax = generate_pmax(var, close_array, high_array, low_array, ATR_PERIOD, ATR_MULTIPLIER)\n",
        "\n",
        "    # Добавляем в DataFrame\n",
        "    df['VAR'] = var\n",
        "    df['PMAX'] = pmax\n",
        "    df['WMA'] = df['close'].rolling(MOVING_AVERAGE_LENGHT).mean()\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_tinkoff_trade_chart(df, file_path, ticker, alert_price, alert_time,\n",
        "                             exit_time, exit_price, pnl, reason):\n",
        "    \"\"\"Сохранение графика для сделки в Tinkoff\"\"\"\n",
        "    if df.empty:\n",
        "        return\n",
        "\n",
        "    # Преобразуем цены в проценты относительно alert_price\n",
        "    price_cols = ['open', 'high', 'low', 'close', 'pmax', 'ma']\n",
        "    for col in price_cols:\n",
        "        df[col] = ((df[col] - alert_price) / alert_price) * 100\n",
        "\n",
        "    alert_price_percent = 0\n",
        "    exit_price_percent = ((exit_price - alert_price) / alert_price) * 100\n",
        "\n",
        "    trade_start_index = df.index.get_loc(pd.to_datetime(open_trades[ticker]['buy_time']).replace(second=0, microsecond=0).isoformat())\n",
        "\n",
        "    # Преобразуем временные метки\n",
        "    alert_time = pd.to_datetime(alert_time.replace(second=0, microsecond=0))\n",
        "    exit_time = pd.to_datetime(exit_time.replace(second=0, microsecond=0))\n",
        "\n",
        "    # Линии и точки на графике\n",
        "    alert_line = [alert_price_percent] * len(df)\n",
        "\n",
        "    # Вертикальные линии для времени входа и выхода\n",
        "    trade_start_marker = np.full(len(df), np.nan)\n",
        "    trade_start_marker[trade_start_index] = 0\n",
        "\n",
        "    # Подготовка дополнительных графиков\n",
        "    add_plot = [\n",
        "        mpf.make_addplot(alert_line, color='gray', linestyle='--', label='Alert price', secondary_y=False),\n",
        "        mpf.make_addplot(trade_start_marker, type='scatter', marker='v', markersize=100,\n",
        "        color='orange', label='Trade Start'),\n",
        "        mpf.make_addplot(df['pmax'], color='blue', label='PMAX', secondary_y=False),\n",
        "        mpf.make_addplot(df['ma'], color='green', label=f'MA', secondary_y=False)\n",
        "    ]\n",
        "\n",
        "    '''if not reason:\n",
        "         exit_scatter = np.full(len(df), np.nan)\n",
        "         exit_idx = df.index.get_loc(exit_time)\n",
        "         exit_scatter[exit_idx] = exit_price_percent\n",
        "         add_plot.append(\n",
        "            mpf.make_addplot(exit_scatter, type='scatter', marker='^', markersize=50, color='red', label='Exit', alpha=0.7))'''\n",
        "\n",
        "    title = (f'\\n\\n\\nTinkoff Trade Report; {ticker}; Alert time: {alert_time}; Alert price: {alert_price:.2f};\\n'\n",
        "             f'Exit time: {exit_time}; Exit price: {exit_price:.2f};\\n'\n",
        "             f'PNL: {pnl:.2f}%')\n",
        "\n",
        "    # Сохранение графика\n",
        "    mpf.plot(df, type='candle', style='charles', addplot=add_plot, title=title,\n",
        "            ylabel='Price Change (%)', savefig=file_path, figsize=(15, 10))\n",
        "\n",
        "def save_and_send_tinkoff_report(ticker, alert_price, trade_start_time, exit_time,\n",
        "                                 exit_price, pnl, reason, link, tg_channels):\n",
        "    \"\"\"Сохранение и отправка отчета по сделке в Tinkoff\"\"\"\n",
        "    try:\n",
        "        # Конвертация времени\n",
        "        #trade_start = dt.utcfromtimestamp(trade_start_time / 1000)\n",
        "        #exit_dt = dt.utcfromtimestamp(exit_time / 1000)\n",
        "\n",
        "        # Формирование данных результата\n",
        "        results = {\n",
        "            'symbol': ticker,\n",
        "            'alert_time': trade_start_time,\n",
        "            'alert_price': alert_price,\n",
        "            'exit_time': exit_time,\n",
        "            'exit_price': exit_price,\n",
        "            'pnl': pnl,\n",
        "            'reason': reason,\n",
        "            'link': link\n",
        "        }\n",
        "\n",
        "        # Сохранение в индивидуальный файл\n",
        "        df = pd.DataFrame([results])\n",
        "        filename = f\"tinkoff_trade_{ticker}_{trade_start_time.strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
        "        file_path = f'{path_to_save}reports/{filename}'\n",
        "        os.makedirs(f'{path_to_save}reports', exist_ok=True)\n",
        "        df.to_excel(file_path, index=False)\n",
        "\n",
        "        # Работа с итоговым файлом за неделю\n",
        "        week_number = trade_start_time.isocalendar()[1]\n",
        "        year = trade_start_time.year\n",
        "        summary_filename = f\"tinkoff_trades_week_{week_number}_{year}.xlsx\"\n",
        "        summary_path = f'{path_to_save}reports/weekly/{summary_filename}'\n",
        "        os.makedirs(f'{path_to_save}reports/weekly', exist_ok=True)\n",
        "\n",
        "        if os.path.exists(summary_path):\n",
        "            existing_df = pd.read_excel(summary_path)\n",
        "            updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
        "        else:\n",
        "            updated_df = df\n",
        "\n",
        "        updated_df.to_excel(summary_path, index=False)\n",
        "\n",
        "        # Отправка в Telegram (замените на вашу реализацию)\n",
        "        if 'bot' in globals():\n",
        "            with open(file_path, 'rb') as file:\n",
        "                bot.send_document(tg_channels['sells'], file)\n",
        "\n",
        "    except Exception as e:\n",
        "        if 'bot' in globals():\n",
        "            bot.send_message(tg_channels['error'], f\"Ошибка в отчете Tinkoff: {e}\")\n",
        "\n",
        "class FastRollingMode:\n",
        "    def __init__(self, window_size):\n",
        "        self.window = deque(maxlen=window_size)\n",
        "        self.counts = {}\n",
        "\n",
        "    def update(self, new_val):\n",
        "        if len(self.window) == self.window.maxlen:\n",
        "            old_val = self.window.popleft()\n",
        "            self.counts[old_val] -= 1\n",
        "            if self.counts[old_val] == 0:\n",
        "                del self.counts[old_val]\n",
        "\n",
        "        self.window.append(new_val)\n",
        "        self.counts[new_val] = self.counts.get(new_val, 0) + 1\n",
        "        return max(self.counts.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "\n",
        "def extract_features(df: pd.DataFrame, window: int = 126):\n",
        "    \"\"\"\n",
        "    Вычисляет устойчивые признаки для кластеризации рыночных режимов.\n",
        "    \"\"\"\n",
        "\n",
        "    def calculate_macd(df, macd_fast_periods=[12], macd_slow_periods=[26], macd_signal_periods=[9]):\n",
        "        \"\"\"\n",
        "        Быстрый расчет нормализованного MACD с использованием векторизованных операций\n",
        "        \"\"\"\n",
        "        close = df['close']\n",
        "\n",
        "        # Создаем множества для уникальных периодов\n",
        "        unique_fast = set(macd_fast_periods)\n",
        "        unique_slow = set(macd_slow_periods)\n",
        "\n",
        "\n",
        "        # Предварительно вычисляем все необходимые EMA и скользящие средние\n",
        "        ema_cache = {}\n",
        "        rolling_cache = {}\n",
        "\n",
        "        # Кешируем быстрые EMA\n",
        "        for fp in unique_fast:\n",
        "            ema_cache[f'ema_{fp}'] = close.ewm(span=fp, adjust=False).mean()\n",
        "\n",
        "        # Кешируем медленные EMA и скользящие средние\n",
        "        for sp in unique_slow:\n",
        "            ema_cache[f'ema_{sp}'] = close.ewm(span=sp, adjust=False).mean()\n",
        "            rolling_cache[f'rolling_{sp}'] = close.rolling(window=sp).mean()\n",
        "\n",
        "        # Основной цикл вычислений\n",
        "        for fp in macd_fast_periods:\n",
        "            ema_fast = ema_cache[f'ema_{fp}']\n",
        "            for sp in macd_slow_periods:\n",
        "                ema_slow = ema_cache[f'ema_{sp}']\n",
        "                rolling_mean = rolling_cache[f'rolling_{sp}']\n",
        "\n",
        "                # Вычисляем MACD и нормализацию\n",
        "                macd = ema_fast - ema_slow\n",
        "                macd_norm = macd / rolling_mean\n",
        "\n",
        "                # Сохраняем MACD только один раз для комбинации fp/sp\n",
        "\n",
        "                # Обрабатываем сигнальные периоды\n",
        "                for sig in macd_signal_periods:\n",
        "                    # Вычисляем сигнальную линию\n",
        "                    signal = macd.ewm(span=sig, adjust=False).mean()\n",
        "                    signal_norm = signal / rolling_mean\n",
        "\n",
        "        return pd.DataFrame([macd_norm, signal_norm, macd_norm - signal_norm]).T.fillna(0)\n",
        "\n",
        "    def calculate_atr(df, atr_window=14):\n",
        "        \"\"\"\n",
        "        Расчет ATR и его сдвигов.\n",
        "        \"\"\"\n",
        "        high = df['high']\n",
        "        low = df['low']\n",
        "        close = df['close']\n",
        "\n",
        "        tr1 = high - low\n",
        "        tr2 = np.abs(high - close.shift(1))\n",
        "        tr3 = np.abs(low - close.shift(1))\n",
        "        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "        atr = tr.rolling(atr_window).mean()\n",
        "\n",
        "        return pd.Series(atr).fillna(0)\n",
        "\n",
        "    def calculate_rsi(df, rsi_period=14):\n",
        "        \"\"\"\n",
        "        Расчет RSI и его сдвиги.\n",
        "        \"\"\"\n",
        "        close = df['close']\n",
        "        delta = close.diff()\n",
        "        gain = delta.where(delta > 0, 0)\n",
        "        loss = -delta.where(delta < 0, 0)\n",
        "        avg_gain = gain.rolling(rsi_period).mean()\n",
        "        avg_loss = loss.rolling(rsi_period).mean()\n",
        "        rs = avg_gain / (avg_loss + 1e-10)\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        return pd.Series(rsi).fillna(0)\n",
        "\n",
        "    def calculate_bollinger_bands(df, bollinger_window=20):\n",
        "        \"\"\"\n",
        "        Расчет Bollinger Bands (ширины полос) и сдвигов.\n",
        "        \"\"\"\n",
        "        close = df['close']\n",
        "        ma = close.rolling(bollinger_window).mean()\n",
        "        std = close.rolling(bollinger_window).std()\n",
        "        bb_width = (2 * std) / ma\n",
        "\n",
        "        return pd.Series(bb_width).fillna(0)\n",
        "\n",
        "    macd_trend = calculate_macd(df, macd_slow_periods=[window], macd_fast_periods=[window//3],\n",
        "                                 macd_signal_periods=[window//6])\n",
        "    atr = calculate_atr(df, atr_window=window)\n",
        "    rel_volatility = atr / df[\"close\"]\n",
        "    rsi_ind = calculate_rsi(df, rsi_period=window//2)\n",
        "    volume_ratio = df['volume'].rolling(window).apply(\n",
        "        lambda x: x[-1]/x.mean(), raw=True\n",
        "    ).fillna(1).values\n",
        "\n",
        "    features = np.column_stack([\n",
        "        macd_trend,\n",
        "        rel_volatility,\n",
        "        rsi_ind,\n",
        "        volume_ratio\n",
        "    ])\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "class MachineLearningRSI:\n",
        "    def __init__(self,\n",
        "                 rsi_length=300,\n",
        "                 use_smoothing=True,\n",
        "                 smoothing_length=268,\n",
        "                 smoothing_type='ALMA',\n",
        "                 alma_sigma=6,\n",
        "                 rsi_overbought=70,\n",
        "                 rsi_oversold=30,\n",
        "                 use_knn=True,\n",
        "                 knn_neighbors=7,\n",
        "                 knn_lookback=500,\n",
        "                 knn_weight=0.6,\n",
        "                 feature_count=5,\n",
        "                 use_filter=True,\n",
        "                 filter_method='Kalman',\n",
        "                 filter_strength=0.7,\n",
        "                 sma_length=20 + 7*24*4*3,\n",
        "                 ema_length=21 + 7*24*4*3\n",
        "                 ):\n",
        "\n",
        "        # Базовые параметры\n",
        "        self.rsi_length = rsi_length\n",
        "        self.use_smoothing = use_smoothing\n",
        "        self.smoothing_length = smoothing_length\n",
        "        self.smoothing_type = smoothing_type\n",
        "        self.alma_sigma = alma_sigma\n",
        "\n",
        "        # Пороговые уровни\n",
        "        self.rsi_overbought = rsi_overbought\n",
        "        self.rsi_oversold = rsi_oversold\n",
        "\n",
        "        # Параметры KNN\n",
        "        self.use_knn = use_knn\n",
        "        self.knn_neighbors = knn_neighbors\n",
        "        self.knn_lookback = knn_lookback\n",
        "        self.knn_weight = knn_weight\n",
        "        self.feature_count = feature_count\n",
        "\n",
        "        # Фильтрация\n",
        "        self.use_filter = use_filter\n",
        "        self.filter_method = filter_method\n",
        "        self.filter_strength = filter_strength\n",
        "\n",
        "        self.sma_length = sma_length\n",
        "        self.ema_length = ema_length\n",
        "\n",
        "    def calculate_rsi(self, close: pd.Series, length: int) -> pd.Series:\n",
        "        \"\"\"Расчет RSI через RMA аналогично PineScript ta.rsi\"\"\"\n",
        "        delta = close.diff()\n",
        "        gain = delta.clip(lower=0)\n",
        "        loss = -delta.clip(upper=0)\n",
        "        avg_gain = gain.ewm(alpha=1/length, min_periods=length, adjust=False).mean()\n",
        "        avg_loss = loss.ewm(alpha=1/length, min_periods=length, adjust=False).mean()\n",
        "        rs = avg_gain / avg_loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi\n",
        "\n",
        "    def smooth(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Корректный ALMA\"\"\"\n",
        "        if self.smoothing_type == 'SMA':\n",
        "            return series.rolling(self.smoothing_length).mean()\n",
        "        elif self.smoothing_type == 'EMA':\n",
        "            return series.ewm(span=self.smoothing_length, adjust=False).mean()\n",
        "        elif self.smoothing_type == 'ALMA':\n",
        "            m = self.smoothing_length\n",
        "            offset = 0.85\n",
        "            sigma = self.alma_sigma\n",
        "\n",
        "            def alma(series):\n",
        "                window = np.arange(m)\n",
        "                weights = np.exp(-((window - offset * (m-1))**2) / (2*(sigma**2)))\n",
        "                weights /= weights.sum()\n",
        "                return np.convolve(series, weights, mode='valid')\n",
        "\n",
        "            def alma_causal(series: pd.Series, length: int = 9, offset: float = 0.85, sigma: float = 6) -> pd.Series:\n",
        "                \"\"\"\n",
        "                Казуальная реализация ALMA (Arnaud Legoux Moving Average)\n",
        "                Использует только прошлые и текущие значения, без lookahead bias.\n",
        "                \"\"\"\n",
        "                if length > len(series):\n",
        "                    return pd.Series(np.nan, index=series.index)\n",
        "\n",
        "                # Предвычисление весов ALMA\n",
        "                window = np.arange(length)\n",
        "                m = offset * (length - 1)\n",
        "                s = length / sigma\n",
        "                weights = np.exp(-((window - m) ** 2) / (2 * s ** 2))\n",
        "                weights /= weights.sum()\n",
        "\n",
        "                # Применяем ALMA казуально (rolling + dot product)\n",
        "                alma_vals = []\n",
        "                for i in range(length - 1, len(series)):\n",
        "                    window_data = series.iloc[i - length + 1:i + 1]\n",
        "                    if window_data.isnull().any():\n",
        "                        alma_vals.append(np.nan)\n",
        "                    else:\n",
        "                        alma_vals.append(np.dot(weights, window_data.values))\n",
        "\n",
        "                # Паддинг NaN в начало, чтобы сохранить индекс\n",
        "                alma_series = pd.Series([np.nan] * (length - 1) + alma_vals, index=series.index)\n",
        "\n",
        "                return alma_series\n",
        "\n",
        "            alma_series = alma_causal(series.fillna(method='ffill'), m, offset, sigma)#, index=series.index[pad:-pad])\n",
        "            alma_series = alma_series.reindex(series.index, method='nearest')\n",
        "            return alma_series\n",
        "        else:\n",
        "            return series\n",
        "\n",
        "    def feature_extraction(self, close: pd.Series, rsi: pd.Series) -> pd.DataFrame:\n",
        "        \"\"\"Извлечение признаков для KNN\"\"\"\n",
        "        features = pd.DataFrame(index=close.index)\n",
        "        features['rsi'] = self.normalize(rsi, self.knn_lookback)\n",
        "\n",
        "        if self.feature_count >= 2:\n",
        "            features['momentum_rsi'] = self.normalize(rsi.diff(3), self.knn_lookback)\n",
        "        if self.feature_count >= 3:\n",
        "            features['volatility_rsi'] = self.normalize(rsi.rolling(10).std(), self.knn_lookback)\n",
        "        if self.feature_count >= 4:\n",
        "            features['slope_rsi'] = self.normalize(self.get_slope(rsi, 5), self.knn_lookback)\n",
        "        if self.feature_count >= 5:\n",
        "            features['momentum_price'] = self.normalize(close.diff(5), self.knn_lookback)\n",
        "\n",
        "        return features.dropna()\n",
        "\n",
        "    def normalize(self, series: pd.Series, period: int) -> pd.Series:\n",
        "        \"\"\"Мин-макс нормализация\"\"\"\n",
        "        min_val = series.rolling(period).min()\n",
        "        max_val = series.rolling(period).max()\n",
        "        norm = (series - min_val) / (max_val - min_val)\n",
        "        return norm.clip(0, 1)\n",
        "\n",
        "    def get_slope(self, series: pd.Series, window: int) -> pd.Series:\n",
        "        \"\"\"Расчет наклона линейной регрессии\"\"\"\n",
        "        idx = np.arange(window)\n",
        "        def linreg(x):\n",
        "            A = np.vstack([idx, np.ones(len(idx))]).T\n",
        "            m, c = np.linalg.lstsq(A, x, rcond=None)[0]\n",
        "            return m\n",
        "        return series.rolling(window).apply(linreg, raw=True)\n",
        "\n",
        "    def apply_knn(self, features: pd.DataFrame, rsi: pd.Series) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Оптимизированная, но идентичная оригиналу версия KNN.\n",
        "        Сохраняет точную математику оригинального apply_knn_b с ускорением через BallTree.\n",
        "        \"\"\"\n",
        "        # Сохраняем структуру как в оригинале\n",
        "        full_index = rsi.index\n",
        "        common_index = features.index.intersection(rsi.index)\n",
        "        features = features.loc[common_index]\n",
        "        rsi = rsi.loc[common_index]\n",
        "\n",
        "        enhanced_rsi = pd.Series(index=full_index, data=np.nan)\n",
        "        enhanced_rsi.loc[rsi.index] = rsi\n",
        "\n",
        "        if len(features) < self.knn_lookback:\n",
        "            return enhanced_rsi\n",
        "\n",
        "        feature_array = features.values\n",
        "        rsi_array = rsi.values\n",
        "\n",
        "        # Основное изменение: BallTree строится на скользящем окне\n",
        "        for t in range(self.knn_lookback, len(feature_array)):\n",
        "            window_start = t - self.knn_lookback\n",
        "            window_end = t\n",
        "            X_window = feature_array[window_start:window_end]\n",
        "            y_window = rsi_array[window_start:window_end]\n",
        "\n",
        "            # Строим дерево только на текущем окне\n",
        "            tree = BallTree(X_window, metric='euclidean')\n",
        "            distances, indices = tree.query(feature_array[t].reshape(1, -1),\n",
        "                                          k=self.knn_neighbors)\n",
        "\n",
        "            # Точное воспроизведение оригинальной логики взвешивания\n",
        "            weights = np.where(distances[0] < 1e-6, 1.0, 1.0 / distances[0])\n",
        "            prediction = np.average(y_window[indices[0]], weights=weights)\n",
        "\n",
        "            idx = common_index[t]\n",
        "            enhanced_rsi.loc[idx] = (1 - self.knn_weight) * rsi.loc[idx] + self.knn_weight * prediction\n",
        "\n",
        "        return enhanced_rsi\n",
        "\n",
        "    def kalman_filter(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Калман-фильтр с параметрами ближе к PineScript\"\"\"\n",
        "        n = len(series)\n",
        "        xhat = np.full(n, np.nan)\n",
        "        P = np.zeros(n)\n",
        "        R = self.filter_strength * 0.1  # Очень маленький measurement noise\n",
        "        Q = self.filter_strength * 0.01  # Очень маленький process noise\n",
        "\n",
        "        first_valid_idx = series.first_valid_index()\n",
        "        if first_valid_idx is None:\n",
        "            return pd.Series(xhat, index=series.index)\n",
        "\n",
        "        first_idx = series.index.get_loc(first_valid_idx)\n",
        "        xhat[first_idx] = series.iloc[first_idx]\n",
        "        P[first_idx] = 1.0\n",
        "\n",
        "        for k in range(first_idx + 1, n):\n",
        "            if np.isnan(series.iloc[k]):\n",
        "                xhat[k] = xhat[k - 1]\n",
        "                P[k] = P[k - 1] + Q\n",
        "            else:\n",
        "                xhatminus = xhat[k-1]\n",
        "                Pminus = P[k-1] + Q\n",
        "                K = Pminus / (Pminus + R)\n",
        "                xhat[k] = xhatminus + K * (series.iloc[k] - xhatminus)\n",
        "                P[k] = (1 - K) * Pminus\n",
        "\n",
        "        return pd.Series(xhat, index=series.index)\n",
        "\n",
        "    def filter_series(self, series: pd.Series) -> pd.Series:\n",
        "        \"\"\"Применение фильтрации к финальному RSI\"\"\"\n",
        "        if self.filter_method == 'None':\n",
        "            return series\n",
        "        elif self.filter_method == 'Kalman':\n",
        "            return self.kalman_filter(series)\n",
        "        elif self.filter_method == 'DoubleEMA':\n",
        "            ema1 = series.ewm(span=int(self.filter_strength * 10)).mean()\n",
        "            ema2 = ema1.ewm(span=int(self.filter_strength * 5)).mean()\n",
        "            return ema2\n",
        "        elif self.filter_method == 'ALMA':\n",
        "            return self.smooth(series)\n",
        "        else:\n",
        "            return series\n",
        "\n",
        "    def week_level(self, close):\n",
        "        sma_length = self.sma_length\n",
        "        ema_length = self.ema_length\n",
        "\n",
        "        # Вычисление 20-недельной SMA\n",
        "        SMA_20w = close.rolling(window=sma_length, min_periods=1).mean()\n",
        "\n",
        "        # Вычисление 21-недельной EMA\n",
        "        MA_21w = close.ewm(span=ema_length, adjust=False).mean()\n",
        "\n",
        "        return SMA_20w, MA_21w\n",
        "\n",
        "    def fit(self, close: pd.Series) -> pd.Series:\n",
        "        \"\"\"Основная функция расчёта\"\"\"\n",
        "        rsi = self.calculate_rsi(close, self.rsi_length)\n",
        "        if self.use_smoothing:\n",
        "            rsi = self.smooth(rsi)\n",
        "        if self.use_knn:\n",
        "            features = self.feature_extraction(close, rsi)\n",
        "\n",
        "            rsi = self.apply_knn(features, rsi)\n",
        "\n",
        "        if self.use_filter:\n",
        "            rsi = self.filter_series(rsi)\n",
        "\n",
        "        sma, ma = self.week_level(close)\n",
        "\n",
        "        return rsi.clip(0, 100).iloc[-1], sma.iloc[-1], ma.iloc[-1]\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "class TinkoffInvestDataCollector:\n",
        "    def __init__(self, token, path_to_save, sector, country, tg_channels, period, index):\n",
        "        self.token = token\n",
        "        self.path_to_save = path_to_save\n",
        "        self.sector = sector\n",
        "        self.country = country\n",
        "\n",
        "        #self.adaptive_system = AdaptiveTradingSystem(regime_params)\n",
        "\n",
        "        # Telegram channels\n",
        "        self.error_tg = tg_channels['error']\n",
        "        self.a1_screen_tg = tg_channels['a1_screen']\n",
        "        self.memmory_id = tg_channels['memory']\n",
        "        self.stat_tg = tg_channels['stat']\n",
        "        self.index = index\n",
        "        self.indexes = {}\n",
        "\n",
        "        # Данные\n",
        "        self.tickers = {}\n",
        "        self.ticker_to_figi = {}\n",
        "        self.figi_to_ticker = {}\n",
        "        self.active_tickers = {}\n",
        "\n",
        "        #Метаданные\n",
        "        self.stream_last_activity = {}  # ключ: str(figi_chunk_hash), значение: datetime\n",
        "        self.stream_tasks = {}\n",
        "\n",
        "        # Списки данных по тикерам\n",
        "        self.running = True\n",
        "\n",
        "        if period == 5:\n",
        "            self.candleintervall = SubscriptionInterval.SUBSCRIPTION_INTERVAL_FIVE_MINUTES\n",
        "            self.timestop = 5\n",
        "            self.update_interval = timedelta(minutes=5)\n",
        "        if period == 1:\n",
        "            self.candleintervall = SubscriptionInterval.SUBSCRIPTION_INTERVAL_ONE_MINUTE\n",
        "            self.timestop = 1\n",
        "            self.update_interval = timedelta(minutes=1)\n",
        "        if period == 15:\n",
        "            self.candleintervall = SubscriptionInterval.SUBSCRIPTION_INTERVAL_FIFTEEN_MINUTES\n",
        "            self.timestop = 15\n",
        "            self.update_interval = timedelta(minutes=15)\n",
        "        if period == 60:\n",
        "            self.candleintervall = SubscriptionInterval.SUBSCRIPTION_INTERVAL_ONE_HOUR\n",
        "            self.timestop = 60\n",
        "            self.update_interval = timedelta(minutes=60)\n",
        "\n",
        "        self._smoothers       = {}    # ticker → FastRollingMode\n",
        "        self._regime_history  = {}    # ticker → deque[int]\n",
        "        self._adaptive_params = {}\n",
        "        self._regime_hist    = {}\n",
        "\n",
        "    def _get_candle_interval_enum(self):\n",
        "        \"\"\"Определяет enum-интервал для вызова REST API Tinkoff\"\"\"\n",
        "        if self.timestop == 1:\n",
        "            return CandleInterval.CANDLE_INTERVAL_1_MIN\n",
        "        elif self.timestop == 5:\n",
        "            return CandleInterval.CANDLE_INTERVAL_5_MIN\n",
        "        elif self.timestop == 15:\n",
        "            return CandleInterval.CANDLE_INTERVAL_15_MIN\n",
        "        elif self.timestop == 60:\n",
        "            return CandleInterval.CANDLE_INTERVAL_HOUR\n",
        "        else:\n",
        "            raise ValueError(\"Неподдерживаемый интервал\")\n",
        "\n",
        "    def _clean_data(self, candles):\n",
        "        \"\"\"Очистка и преобразование данных\"\"\"\n",
        "        if not candles:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = pd.DataFrame(candles)\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        return df.sort_values(\"time\").drop_duplicates(\"time\").reset_index(drop=True)\n",
        "\n",
        "    async def retrospective_signal_handler(self, ticker: str, periods_passed: int):\n",
        "        \"\"\"\n",
        "        Обработка сигналов на основании пропущенных свечей.\n",
        "        :param ticker: тикер\n",
        "        :param periods_passed: количество новых свечей, загруженных из backfill\n",
        "        \"\"\"\n",
        "        # Получаем последние N свечей (включая новые)\n",
        "        closes = close_price[ticker][-periods_passed:]\n",
        "        end_times = time_last_kline_end[ticker][-periods_passed:]\n",
        "\n",
        "\n",
        "        for i in range(periods_passed,0,-1):\n",
        "            signal = self.generate_signal(ma[ticker][-i-1:][:2] , pmax[ticker][-i-1:][:2])\n",
        "            if signal == 'buy':\n",
        "                await self._handle_buy_signal(ticker, closes[-i], end_times[-i])\n",
        "                print(f'[BUY] {ticker}, за время пропуска был сигнал на покупку в {end_times[-i]} по цене {closes[-i]}')\n",
        "            elif signal == 'sell':\n",
        "                await self._handle_sell_signal(ticker, closes[-i], end_times[-i])\n",
        "                print(f'[SELL] {ticker}, за время пропуска был сигнал на продажу в {end_times[-i]} по цене {closes[-i]}')\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    async def _handle_buy_signal(self, ticker, price, time_str):\n",
        "        \"\"\"\n",
        "        Обработка сигнала на покупку для тикера.\n",
        "        \"\"\"\n",
        "        if ticker in open_trades and open_trades[ticker].get('status') == 'open':\n",
        "            return  # Уже открыта сделка — пропускаем\n",
        "\n",
        "\n",
        "        way = phase_ticker_params[ticker]['way']\n",
        "        ticker_parametrs = prepare_regime_params(phase_ticker_params[ticker]['params'])['calc_params']\n",
        "        stop_loss_y_n = phase_ticker_params[ticker]['change_tp']\n",
        "        stop_loss = phase_ticker_params[ticker]['stop_loss_pst_tp']\n",
        "\n",
        "        #ticker_parametrs = ticker_params[ticker]['params']\n",
        "        #way = ticker_params[ticker]['way']\n",
        "        #stop_loss_y_n = ticker_params[ticker]['change']\n",
        "        #stop_loss = ticker_params[ticker]['stop_loss_pst']\n",
        "\n",
        "        should_open = False\n",
        "\n",
        "        if way == 'Базовый':\n",
        "            should_open = True\n",
        "\n",
        "        elif way == 'RSI':\n",
        "            rsi_length = ticker_parametrs['rsi_length']\n",
        "            use_smoothing = ticker_parametrs['use_smoothing']\n",
        "            smoothing_length = ticker_parametrs['smoothing_length']\n",
        "            smoothing_type = ticker_parametrs['smoothing_type']\n",
        "            alma_sigma = ticker_parametrs['alma_sigma']\n",
        "            rsi_overbought = ticker_parametrs['rsi_overbought']\n",
        "            rsi_oversold = ticker_parametrs['rsi_oversold']\n",
        "            use_knn = ticker_parametrs['use_knn']\n",
        "            knn_neighbors = ticker_parametrs['knn_neighbors']\n",
        "            knn_lookback = ticker_parametrs['knn_lookback']\n",
        "            knn_weight = ticker_parametrs['knn_weight']\n",
        "            feature_count = ticker_parametrs['feature_count']\n",
        "            use_filter = ticker_parametrs['use_filter']\n",
        "            filter_method = ticker_parametrs['filter_method']\n",
        "            filter_strength = ticker_parametrs['filter_strength']\n",
        "            sma_length = ticker_parametrs['sma_length']\n",
        "            ema_length = ticker_parametrs['ema_length']\n",
        "            rsi_theasold = ticker_parametrs['rsi_helbuth']\n",
        "\n",
        "            ml_rsi = MachineLearningRSI(\n",
        "                rsi_length=int(rsi_length),\n",
        "                use_smoothing=use_smoothing,\n",
        "                smoothing_length=int(smoothing_length),\n",
        "                smoothing_type=smoothing_type,\n",
        "                alma_sigma=int(alma_sigma),\n",
        "                rsi_overbought=int(rsi_overbought),\n",
        "                rsi_oversold=int(rsi_oversold),\n",
        "                use_knn=use_knn,\n",
        "                knn_neighbors=int(knn_neighbors),\n",
        "                knn_lookback=int(knn_lookback),\n",
        "                knn_weight=knn_weight,\n",
        "                feature_count=int(feature_count),\n",
        "                use_filter=use_filter,\n",
        "                filter_method=filter_method,\n",
        "                filter_strength=filter_strength,\n",
        "                sma_length=int(sma_length),\n",
        "                ema_length=int(ema_length)\n",
        "            )\n",
        "\n",
        "            rsi, _, _ = ml_rsi.fit(pd.Series(close_price[ticker]))\n",
        "            if rsi >= rsi_theasold:\n",
        "                should_open = True\n",
        "\n",
        "        elif way == 'RSI_SMA_EMA':\n",
        "            rsi_length = ticker_parametrs['rsi_length']\n",
        "            use_smoothing = ticker_parametrs['use_smoothing']\n",
        "            smoothing_length = ticker_parametrs['smoothing_length']\n",
        "            smoothing_type = ticker_parametrs['smoothing_type']\n",
        "            alma_sigma = ticker_parametrs['alma_sigma']\n",
        "            rsi_overbought = ticker_parametrs['rsi_overbought']\n",
        "            rsi_oversold = ticker_parametrs['rsi_oversold']\n",
        "            use_knn = ticker_parametrs['use_knn']\n",
        "            knn_neighbors = ticker_parametrs['knn_neighbors']\n",
        "            knn_lookback = ticker_parametrs['knn_lookback']\n",
        "            knn_weight = ticker_parametrs['knn_weight']\n",
        "            feature_count = ticker_parametrs['feature_count']\n",
        "            use_filter = ticker_parametrs['use_filter']\n",
        "            filter_method = ticker_parametrs['filter_method']\n",
        "            filter_strength = ticker_parametrs['filter_strength']\n",
        "            sma_length = ticker_parametrs['sma_length']\n",
        "            ema_length = ticker_parametrs['ema_length']\n",
        "            rsi_theasold = ticker_parametrs['rsi_helbuth']\n",
        "\n",
        "            ml_rsi = MachineLearningRSI(\n",
        "                rsi_length=int(rsi_length),\n",
        "                use_smoothing=use_smoothing,\n",
        "                smoothing_length=int(smoothing_length),\n",
        "                smoothing_type=smoothing_type,\n",
        "                alma_sigma=int(alma_sigma),\n",
        "                rsi_overbought=int(rsi_overbought),\n",
        "                rsi_oversold=int(rsi_oversold),\n",
        "                use_knn=use_knn,\n",
        "                knn_neighbors=int(knn_neighbors),\n",
        "                knn_lookback=int(knn_lookback),\n",
        "                knn_weight=knn_weight,\n",
        "                feature_count=int(feature_count),\n",
        "                use_filter=use_filter,\n",
        "                filter_method=filter_method,\n",
        "                filter_strength=filter_strength,\n",
        "                sma_length=int(sma_length),\n",
        "                ema_length=int(ema_length)\n",
        "            )\n",
        "            rsi, sma, ma_sma = ml_rsi.fit(pd.Series(close_price[ticker]))\n",
        "            if rsi > rsi_theasold and sma > price and ma_sma > price:\n",
        "                should_open = True\n",
        "\n",
        "        if should_open:\n",
        "            trade_obj = {\n",
        "                'buy_price': price,\n",
        "                'buy_time': pd.to_datetime(time_str).isoformat(),\n",
        "                'max_price': price,\n",
        "                'status': 'open',\n",
        "                'reason': 'BASE'\n",
        "            }\n",
        "            if stop_loss_y_n != 'No':\n",
        "                trade_obj['stop_loss'] = stop_loss\n",
        "            open_trades[ticker] = trade_obj\n",
        "\n",
        "            bot.send_message(self.stat_tg, f\"[TRADE OPENED] {ticker} по {price} в {time_str}\")\n",
        "        else:\n",
        "            bot.send_message(self.stat_tg, f\"[TRADE NOT OPENED] {ticker} по {price} в {time_str} — не прошёл фильтр RSI\")\n",
        "\n",
        "    async def _handle_sell_signal(self, ticker, price, time_str):\n",
        "        \"\"\"\n",
        "        Обработка сигнала на продажу.\n",
        "        \"\"\"\n",
        "        if (ticker in open_trades\n",
        "            and open_trades[ticker].get('status') == 'open'):\n",
        "            reason = open_trades[ticker].get('reason', 'UNKNOWN')\n",
        "            await self.close_trade(ticker, price, open_trades[ticker]['max_price'], reason)\n",
        "            bot.send_message(self.stat_tg, f\"[TRADE CLOSED] {ticker} по {price} в {time_str}\")\n",
        "\n",
        "    async def backfill_missing_candles(self):\n",
        "        \"\"\"\n",
        "        Ускоренный метод дозагрузки пропущенных свечей с учетом временных зон.\n",
        "        \"\"\"\n",
        "        #print(\"[BACKFILL] Запуск дозагрузки пропущенных свечей...\")\n",
        "\n",
        "        # Создаем пустые структуры для тикеров без открытых сделок\n",
        "        for ticker in self.active_tickers:\n",
        "            if ticker not in open_trades or type(open_trades[ticker]) is list:\n",
        "                open_trades[ticker] = {}\n",
        "\n",
        "        now_utc = dt.utcnow().replace(second=0, microsecond=0, tzinfo=pytz.UTC)\n",
        "        #print(f\"[TIME SYNC] Текущее UTC время сервера: {now_utc}\")\n",
        "\n",
        "        async def process_ticker(ticker):\n",
        "            try:\n",
        "                figi = self.ticker_to_figi[ticker]\n",
        "                now_utc = dt.utcnow().replace(second=0, microsecond=0, tzinfo=pytz.UTC)\n",
        "\n",
        "                # Проверяем, есть ли данные о последней свече\n",
        "                if not time_last_kline_end.get(ticker):\n",
        "                    #print(f\"[SKIP] {ticker}: нет исторических данных\")\n",
        "                    return\n",
        "\n",
        "                # Последнее сохранённое время\n",
        "                last_time_local = pd.to_datetime(time_last_kline_end[ticker][-1])\n",
        "                last_time_utc = last_time_local - timedelta(hours=3)\n",
        "\n",
        "                #print(f\"[SYNC] {ticker}: последняя свеча в UTC: {last_time_utc}\")\n",
        "\n",
        "                # Список сохранённых временных меток\n",
        "                saved_candle_times_set = set(\n",
        "                    (pd.to_datetime(t) - timedelta(hours=3)).strftime('%Y-%m-%dT%H:%M:%S')\n",
        "                    for t in time_last_kline_start[ticker]\n",
        "                )\n",
        "\n",
        "                # Начинаем с последней известной свечи\n",
        "                from_time = last_time_utc\n",
        "                max_to_time = now_utc - self.update_interval  # Только закрытые свечи\n",
        "\n",
        "                while from_time < max_to_time:\n",
        "                    to_time = min(max_to_time, from_time + timedelta(minutes=1000))\n",
        "                    #print(f\"[API REQ] {ticker}: from {from_time} to {to_time}\")\n",
        "\n",
        "                    try:\n",
        "                        response = await client.market_data.get_candles(\n",
        "                            figi=figi,\n",
        "                            from_=from_time,\n",
        "                            to=to_time,\n",
        "                            interval=self._get_candle_interval_enum()\n",
        "                        )\n",
        "                    except Exception as api_err:\n",
        "                        #print(f\"[API ERROR] {ticker}: ошибка API: {api_err}\")\n",
        "                        return\n",
        "\n",
        "                    if not response.candles:\n",
        "                        #print(f\"[WARN] {ticker}: API не вернул свечи\")\n",
        "                        break\n",
        "\n",
        "                    new_candles = [\n",
        "                        candle for candle in response.candles\n",
        "                        if candle.time.strftime('%Y-%m-%dT%H:%M:%S') not in saved_candle_times_set\n",
        "                        and candle.time.replace(tzinfo=pytz.UTC) < max_to_time  # фильтрация незакрытых\n",
        "                    ]\n",
        "\n",
        "                    if not new_candles:\n",
        "                        #print(f\"[SKIP] {ticker}: все свечи уже сохранены или незакрыты\")\n",
        "                        break\n",
        "\n",
        "                    #print(f\"[SAVE] {ticker}: сохраняем {len(new_candles)} новых свечей\")\n",
        "\n",
        "                    # Обновляем данные\n",
        "                    for candle in new_candles:\n",
        "                        ts_utc = candle.time.replace(tzinfo=pytz.UTC)\n",
        "                        ts_local = ts_utc + timedelta(hours=3)\n",
        "                        tc_local = ts_local + self.update_interval\n",
        "\n",
        "                        open_price[ticker].append(float(candle.open.units) + candle.open.nano * 1e-9)\n",
        "                        close_price[ticker].append(float(candle.close.units) + candle.close.nano * 1e-9)\n",
        "                        high_price[ticker].append(float(candle.high.units) + candle.high.nano * 1e-9)\n",
        "                        low_price[ticker].append(float(candle.low.units) + candle.low.nano * 1e-9)\n",
        "                        volume[ticker].append(candle.volume)\n",
        "\n",
        "                        time_last_kline_start[ticker].append(ts_local.isoformat())\n",
        "                        time_last_kline_end[ticker].append(tc_local.isoformat())\n",
        "\n",
        "                    # FIFO — обрезаем до MAX_CANDLES\n",
        "                    for arr in [\n",
        "                        open_price, close_price, high_price, low_price,\n",
        "                        volume, time_last_kline_start, time_last_kline_end\n",
        "                    ]:\n",
        "                        arr[ticker] = arr[ticker][-MAX_CANDLES:]\n",
        "\n",
        "                    await self.calculate_indicators_and_signals(ticker, len(new_candles))\n",
        "\n",
        "                    await self.retrospective_signal_handler(ticker, len(new_candles))\n",
        "\n",
        "                    # Продвигаем окно\n",
        "                    from_time = to_time\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] {ticker}: ошибка обработки: {e}\")\n",
        "\n",
        "        # Параллелизуем обработку тикеров\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            await asyncio.gather(*(process_ticker(ticker) for ticker in self.active_tickers))\n",
        "\n",
        "    async def calculate_indicators_and_signals(self, ticker, periods_passed=None):\n",
        "\n",
        "        try:\n",
        "            # 1. Сырые данные\n",
        "            high_arr = np.array(high_price.get(ticker))\n",
        "            low_arr = np.array(low_price.get(ticker))\n",
        "            close_arr = np.array(close_price.get(ticker))\n",
        "            vol_arr = np.array(volume.get(ticker))\n",
        "            n = len(close_arr)\n",
        "            if n < 2:\n",
        "                return\n",
        "\n",
        "            # 2. Параметры и инициализация\n",
        "            params       = phase_ticker_params[ticker]['params']\n",
        "            # размер окна для признаков\n",
        "            window_feat  = int(params['moving_average_length'] * 9.5)\n",
        "            # формируем DataFrame-мини-окно для extract_features\n",
        "            import pandas as pd\n",
        "            hist_df = pd.DataFrame({\n",
        "                'high':   high_arr,\n",
        "                'low':    low_arr,\n",
        "                'close':  close_arr,\n",
        "                'volume': vol_arr\n",
        "            })\n",
        "            features = extract_features(hist_df, window=window_feat)  # ваша функция\n",
        "            # scale + predict\n",
        "            scaled = scaler_global.transform(features)\n",
        "            labels = kmeans_global.predict(scaled)\n",
        "\n",
        "            # сглаживаем\n",
        "            window_smooth = int(params['atr_period'] * 5.5)\n",
        "            smoother = FastRollingMode(window_size=window_smooth)\n",
        "            regimes = np.array([smoother.update(l) for l in labels], dtype=int)\n",
        "\n",
        "            # 2) Готовим базовые параметры по режиму\n",
        "            regime_params = prepare_regime_params(params)['base_params']\n",
        "\n",
        "            # 3) PRE-COMPUTE MA & ATR для каждого режима\n",
        "            ma_cache  = {}\n",
        "            atr_cache = {}\n",
        "            for regime, p in regime_params.items():\n",
        "                atype = p['average_type']\n",
        "                L     = p['moving_average_length']\n",
        "                P     = p['atr_period']\n",
        "\n",
        "                if atype == 'SMA':\n",
        "                    ma_data = self.generateSma(high_arr, low_arr, window=L)\n",
        "                elif atype == 'VAR':\n",
        "                    ma_data = self.generateVar(high_arr, low_arr, moving_average_length=L)\n",
        "                elif atype == 'EMA':\n",
        "                    ma_data = self.generateEma(high_arr, low_arr, moving_average_length=L)\n",
        "                elif atype == 'AMA':\n",
        "                    ama_p = p.get('ama_params', {'atr_period':14,'min_period':5,'max_period':50})\n",
        "                    ma_data = self.generateAma(\n",
        "                        high_arr, low_arr, close_arr,\n",
        "                        atr_period=ama_p['atr_period'],\n",
        "                        min_period=ama_p['min_period'],\n",
        "                        max_period=ama_p['max_period']\n",
        "                    )\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown MA type {atype!r}\")\n",
        "\n",
        "                ma_cache[regime]  = ma_data\n",
        "                atr_cache[regime] = self.generateAtr(high_arr, low_arr, close_arr, length=P)\n",
        "\n",
        "            # 4) MERGE по маске режимов\n",
        "            var_all = np.empty(n, dtype=float)\n",
        "            atr_all = np.empty(n, dtype=float)\n",
        "            mul_all = np.empty(n, dtype=float)\n",
        "\n",
        "            for regime, p in regime_params.items():\n",
        "                mask = (regimes == regime)\n",
        "                var_all[mask] = ma_cache[regime][mask]\n",
        "                atr_all[mask] = atr_cache[regime][mask]\n",
        "                mul_all[mask] = p['atr_multiplier']\n",
        "\n",
        "            # Заполняем NAN в начале var_all\n",
        "            if np.isnan(var_all[0]):\n",
        "                first = var_all[~np.isnan(var_all)][0]\n",
        "                var_all[np.isnan(var_all)] = first\n",
        "\n",
        "            # 5) PMax state machine\n",
        "            atr_all = np.nan_to_num(atr_all, nan=0.0)\n",
        "            first_var = var_all[~np.isnan(var_all)][0]\n",
        "            var_all = np.nan_to_num(var_all, nan=first_var)\n",
        "            pmax_all = np.empty(n, dtype=float)\n",
        "            prev_v   = var_all[0]\n",
        "            prev_a   = atr_all[0]\n",
        "            prev_m   = mul_all[0]\n",
        "            prev_fu  = prev_v + prev_m * prev_a\n",
        "            prev_fl  = prev_v - prev_m * prev_a\n",
        "            prev_p   = prev_fl\n",
        "            pmax_all[0] = prev_p\n",
        "\n",
        "            for i in range(1, n):\n",
        "                v = var_all[i]; a = atr_all[i]; m = mul_all[i]\n",
        "                bu = v + m * a\n",
        "                bl = v - m * a\n",
        "\n",
        "                fu = bu if (bu < prev_fu or prev_v > prev_fu) else prev_fu\n",
        "                fl = bl if (bl > prev_fl or prev_v < prev_fl) else prev_fl\n",
        "\n",
        "                if prev_p == prev_fu:\n",
        "                    p = fu if v <= fu else fl\n",
        "                else:\n",
        "                    p = fl if v >= fl else fu\n",
        "\n",
        "                pmax_all[i] = p\n",
        "                prev_v, prev_fu, prev_fl, prev_p = v, fu, fl, p\n",
        "\n",
        "            # 6) Обновляем ваши словари ma[] и pmax[]\n",
        "            if periods_passed is None:\n",
        "                ma[ticker]   = ma[ticker][1:] + [var_all[-1]]\n",
        "                pmax[ticker] = pmax[ticker][1:] + [pmax_all[-1]]\n",
        "            else:\n",
        "                ma[ticker]   = ma[ticker][periods_passed:] + list(var_all[-periods_passed:])\n",
        "                pmax[ticker] = pmax[ticker][periods_passed:] + list(pmax_all[-periods_passed:])\n",
        "\n",
        "            signal = self.generate_signal(ma[ticker][-2:], pmax[ticker][-2:])\n",
        "\n",
        "            # Сохраняем сигнал\n",
        "            signals[ticker] = signal\n",
        "\n",
        "            # 9. Финальный сигнал\n",
        "            if signal == 'buy':\n",
        "                send_buy_signal_to_telegram(ticker, close_arr[-1])\n",
        "            elif signal == 'sell':\n",
        "                send_sell_signal_to_telegram(ticker, close_arr[-1])\n",
        "        except Exception as e:\n",
        "            bot.send_message(\n",
        "                      self.error_tg,\n",
        "                      f\"Ошибка в calculate_indicators_and_signals {ticker} с ошибкой {e}\"\n",
        "                  )\n",
        "\n",
        "\n",
        "    async def calculate_indicators_and_signals1(self, ticker, periods_passed = None):\n",
        "        \"\"\"\n",
        "        Вычисление индикаторов и сигналов для каждого тикера.\n",
        "\n",
        "        \"\"\"\n",
        "        # Получаем последние данные для тикера\n",
        "\n",
        "        #Paramsticker_params\n",
        "        params = ticker_params[ticker]['params']\n",
        "        MOVING_AVERAGE_LENGHT = params['moving_average_length']\n",
        "        ATR_PERIOD = params['atr_period']\n",
        "        ATR_MULTIPLIER = params['atr_multiplier']\n",
        "        average_type = params['average_type']\n",
        "        if average_type == 'AMA':\n",
        "            ama_params = {\n",
        "              'atr_period': int(params['ama_atr_period']),\n",
        "              'min_period': int(params['ama_min_period']),\n",
        "              'max_period': int(params['ama_max_period'])\n",
        "            }\n",
        "\n",
        "\n",
        "        high_data = np.array(high_price.get(ticker)[-3000:])\n",
        "        low_data = np.array(low_price.get(ticker)[-3000:])\n",
        "        close_data = np.array(close_price.get(ticker)[-3000:])\n",
        "        # volume_data = volume.get(ticker)[-1000:]\n",
        "\n",
        "        #if high_data.size==0 or low_data.size==0 or close_data.size==0:\n",
        "        #    pass\n",
        "\n",
        "        if len(high_data) == 0 or len(low_data) == 0 or len(close_data) == 0:\n",
        "            return\n",
        "\n",
        "        if average_type == 'SMA':\n",
        "            ma_data = self.generateSma(high_data, low_data, window=MOVING_AVERAGE_LENGHT)\n",
        "        elif average_type == 'VAR':\n",
        "            ma_data = self.generateVar(high_data, low_data, moving_average_length=MOVING_AVERAGE_LENGHT)\n",
        "        elif average_type == 'AMA':\n",
        "            if ama_params is None:\n",
        "                raise ValueError(\"Для AMA необходимо указать параметры ama_params.\")\n",
        "            ma_data = self.generateAma(high_data, low_data, close_data, **ama_params)\n",
        "        else:\n",
        "            raise ValueError(\"Неподдерживаемый тип скользящего среднего.\")\n",
        "\n",
        "        # ema_data = self.generateEma(high_data, low_data)\n",
        "        # atr_data = self.generateAtr(high_data, low_data, close_data)\n",
        "        pmax_data = self.generatePMax(ma_data, close_data, high_data, low_data, atr_period=ATR_PERIOD,\n",
        "                                      atr_multiplier=ATR_MULTIPLIER)\n",
        "\n",
        "        # Обновляем словари с индикаторами\n",
        "\n",
        "        if periods_passed == None:\n",
        "            ma[ticker] = ma[ticker][1:] + [ma_data[-1]]\n",
        "            # ema[ticker] = ema[ticker][1:] + [ema_data[-1]]\n",
        "            # atr[ticker] = atr[ticker][1:] + [atr_data[-1]]\n",
        "            pmax[ticker] = pmax[ticker][1:] + [pmax_data[-1]]\n",
        "        else:\n",
        "            #print(len(ma[ticker][periods_passed:]), len(list(ma_data)[-periods_passed:]))\n",
        "            ma[ticker] = ma[ticker][periods_passed:] + list(ma_data)[-periods_passed:]\n",
        "            pmax[ticker] = pmax[ticker][periods_passed:] + list(pmax_data)[-periods_passed:]\n",
        "\n",
        "        # Вычисляем сигналы\n",
        "        signal = self.generate_signal(ma[ticker][-2:], pmax[ticker][-2:])\n",
        "\n",
        "        # Сохраняем сигнал\n",
        "        signals[ticker] = signal\n",
        "\n",
        "\n",
        "        # Отправляем сигнал в Telegram если необходимо\n",
        "        if signal == 'buy':\n",
        "            send_buy_signal_to_telegram(ticker, close_data[-1])\n",
        "        elif signal == 'sell':\n",
        "            send_sell_signal_to_telegram(ticker, close_data[-1])\n",
        "\n",
        "    def generateAma(self, high_array, low_array, close_array, atr_period=14, min_period=5, max_period=50):\n",
        "        \"\"\"\n",
        "        Генерация адаптивного скользящего среднего на основе волатильности.\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param close_array: Массив значений close.\n",
        "        :param atr_period: Период для расчета ATR.\n",
        "        :param min_period: Минимальный период скользящего среднего.\n",
        "        :param max_period: Максимальный период скользящего среднего.\n",
        "        :return: Массив значений адаптивного скользящего среднего.\n",
        "        \"\"\"\n",
        "        # Рассчитываем ATR\n",
        "        atr = self._calculate_atr(high_array, low_array, close_array, atr_period)\n",
        "\n",
        "        # Нормализуем ATR для использования в качестве коэффициента\n",
        "        normalized_atr = (atr - np.min(atr)) / (np.max(atr) - np.min(atr) + 1e-10)\n",
        "\n",
        "        # Рассчитываем динамический период\n",
        "        dynamic_period = min_period + (max_period - min_period) * normalized_atr\n",
        "\n",
        "        # Рассчитываем адаптивное скользящее среднее (гибрид SMA и EMA)\n",
        "        adaptive_ma = np.zeros_like(close_array)\n",
        "        for i in range(len(close_array)):\n",
        "            if i < int(dynamic_period[i]):\n",
        "                adaptive_ma[i] = np.mean(close_array[:i+1])  # SMA для начальных значений\n",
        "            else:\n",
        "                period = int(dynamic_period[i])\n",
        "                alpha = 2 / (period + 1)\n",
        "                adaptive_ma[i] = alpha * close_array[i] + (1 - alpha) * adaptive_ma[i-1]  # EMA\n",
        "\n",
        "        return adaptive_ma\n",
        "\n",
        "    def _calculate_atr(self, high_array, low_array, close_array, period=14):\n",
        "        \"\"\"\n",
        "        Рассчитывает Average True Range (ATR).\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param close_array: Массив значений close.\n",
        "        :param period: Период для расчета ATR.\n",
        "        :return: Массив значений ATR.\n",
        "        \"\"\"\n",
        "        tr = np.zeros_like(high_array)\n",
        "        tr[0] = high_array[0] - low_array[0]\n",
        "\n",
        "        for i in range(1, len(high_array)):\n",
        "            hl = high_array[i] - low_array[i]\n",
        "            hc = abs(high_array[i] - close_array[i-1])\n",
        "            lc = abs(low_array[i] - close_array[i-1])\n",
        "            tr[i] = max(hl, hc, lc)\n",
        "\n",
        "        atr = np.zeros_like(tr)\n",
        "        atr[period-1] = np.mean(tr[:period])\n",
        "\n",
        "        for i in range(period, len(tr)):\n",
        "            atr[i] = (atr[i-1] * (period-1) + tr[i]) / period\n",
        "\n",
        "        return atr\n",
        "\n",
        "    def generateVar(self, high_array, low_array, moving_average_length=14):\n",
        "        \"\"\"\n",
        "        Генерация VAR (Volatility Adjusted Ratio).\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param moving_average_length: Период для расчета VAR.\n",
        "        :return: Массив значений VAR.\n",
        "        \"\"\"\n",
        "        # Константа alpha\n",
        "        valpha = 2 / (moving_average_length + 1)\n",
        "\n",
        "        # Вычисляем среднее значение между high и low\n",
        "        hl2 = (high_array + low_array) / 2\n",
        "\n",
        "        # Вычисляем разницы для vud1 и vdd1\n",
        "        diff = np.diff(hl2, prepend=hl2[0])\n",
        "        vud1 = np.where(diff > 0, diff, 0)\n",
        "        vdd1 = np.where(diff < 0, -diff, 0)\n",
        "\n",
        "        # Функция для расчета скользящих сумм\n",
        "        def calculate_window_sums(arr, window_size=9):\n",
        "            cumsum = np.cumsum(arr)\n",
        "            return cumsum - np.concatenate((np.zeros(window_size), cumsum[:-window_size]))\n",
        "\n",
        "        # Вычисляем vUD и vDD\n",
        "        vUD = calculate_window_sums(vud1, 9)\n",
        "        vDD = calculate_window_sums(vdd1, 9)\n",
        "\n",
        "        # Вычисляем vCMO\n",
        "        epsilon = 1e-10\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            vCMO = np.divide(vUD - vDD, vUD + vDD + epsilon)\n",
        "        vCMO = np.nan_to_num(vCMO, nan=0.0)\n",
        "\n",
        "        # Вычисляем VAR\n",
        "        cmo_abs = np.abs(vCMO)\n",
        "        var = np.zeros_like(hl2)\n",
        "        var_before = 0.0\n",
        "        for i in range(len(hl2)):\n",
        "            if i < len(cmo_abs):\n",
        "                var[i] = (valpha * cmo_abs[i] * hl2[i]) + (1 - valpha * cmo_abs[i]) * var_before\n",
        "            else:\n",
        "                var[i] = var_before\n",
        "            var_before = var[i]\n",
        "        del valpha, hl2, vud1, vdd1, var_before, vUD, vDD, vCMO\n",
        "        return var\n",
        "\n",
        "    def generateEma(self, high_array, low_array, moving_average_length=14):\n",
        "        \"\"\"Вычисление EMA.\"\"\"\n",
        "        if high_array.size==0 or low_array.size==0:\n",
        "            return []\n",
        "\n",
        "        hl2 = [(high + low) / 2 for high, low in zip(high_array, low_array)]\n",
        "        ema = np.full_like(hl2, np.nan)\n",
        "        alpha = 2 / (moving_average_length + 1)\n",
        "\n",
        "        if moving_average_length <= 1:\n",
        "            return hl2\n",
        "\n",
        "        start_idx = moving_average_length - 1\n",
        "        sma = np.mean(hl2[:moving_average_length])\n",
        "        ema[start_idx] = sma\n",
        "\n",
        "        for i in range(start_idx + 1, len(hl2)):\n",
        "            ema[i] = alpha * hl2[i] + (1 - alpha) * ema[i - 1]\n",
        "\n",
        "        del hl2, alpha, start_idx, sma\n",
        "        return ema\n",
        "\n",
        "    def generateAtr(self, high_array, low_array, close_array, length=14):\n",
        "        \"\"\"Вычисление ATR.\"\"\"\n",
        "        if high_array.size==0 or low_array.size==0 or close_array.size==0:\n",
        "            return []\n",
        "\n",
        "        n = len(high_array)\n",
        "        if n == 0 or length > n:\n",
        "            return []\n",
        "\n",
        "        tr = np.zeros(n)\n",
        "        atr = np.full(n, np.nan)\n",
        "\n",
        "        prev_close = np.roll(close_array, 1)\n",
        "        prev_close[0] = np.nan\n",
        "\n",
        "        tr[0] = high_array[0] - low_array[0]\n",
        "\n",
        "        for i in range(1, n):\n",
        "            hl = high_array[i] - low_array[i]\n",
        "            hc = abs(high_array[i] - prev_close[i])\n",
        "            lc = abs(low_array[i] - prev_close[i])\n",
        "            tr[i] = max(hl, hc, lc)\n",
        "\n",
        "        if n >= length:\n",
        "            atr[length - 1] = np.mean(tr[:length])\n",
        "            alpha = 1.0 / length\n",
        "            for i in range(length, n):\n",
        "                atr[i] = alpha * tr[i] + (1 - alpha) * atr[i - 1]\n",
        "\n",
        "        del n, tr, prev_close, hl, hc, lc, alpha\n",
        "        return atr\n",
        "\n",
        "    def generateSma(self, high_array, low_array, window=14):\n",
        "        \"\"\"\n",
        "        Генерация Simple Moving Average (SMA).\n",
        "\n",
        "        :param high_array: Массив значений high.\n",
        "        :param low_array: Массив значений low.\n",
        "        :param window: Период SMA.\n",
        "        :return: Массив значений SMA.\n",
        "        \"\"\"\n",
        "        hl2 = (high_array + low_array) * 0.5\n",
        "\n",
        "        if window <= 1:\n",
        "            return hl2\n",
        "\n",
        "        # Создаем массив для результатов с NaN\n",
        "        sma = np.full_like(hl2, np.nan)\n",
        "\n",
        "        # Рассчитываем кумулятивную сумму\n",
        "        cumsum = np.cumsum(hl2)\n",
        "\n",
        "        # Создаем сдвинутый кумулятивный массив\n",
        "        shifted_cumsum = np.zeros_like(cumsum)\n",
        "        shifted_cumsum[window:] = cumsum[:-window]\n",
        "\n",
        "        # Вычисляем SMA для валидных периодов\n",
        "        valid = slice(window - 1, None)\n",
        "        sma[valid] = (cumsum[valid] - shifted_cumsum[valid]) / window\n",
        "\n",
        "        del valid, shifted_cumsum, cumsum, hl2\n",
        "        return sma\n",
        "\n",
        "    def generatePMax(self, var_array, close_array, high_array, low_array, atr_period=14, atr_multiplier=3):\n",
        "        \"\"\"Вычисление PMAX.\"\"\"\n",
        "        if high_array.size==0 or low_array.size==0 or close_array.size==0 or len(var_array) == 0:\n",
        "            return []\n",
        "\n",
        "        atr = self.generateAtr(high_array, low_array, close_array, length=atr_period)\n",
        "        pmax = []\n",
        "        previous_final_upperband = 0\n",
        "        previous_final_lowerband = 0\n",
        "        previous_var = 0\n",
        "        previous_pmax = 0\n",
        "\n",
        "        for i in range(len(close_array)):\n",
        "            atrc = atr[i] if i < len(atr) and not np.isnan(atr[i]) else 0\n",
        "            varc = var_array[i] if i < len(var_array) else 0\n",
        "\n",
        "            basic_upperband = varc + atr_multiplier * atrc\n",
        "            basic_lowerband = varc - atr_multiplier * atrc\n",
        "\n",
        "            final_upperband = basic_upperband if (\n",
        "                    basic_upperband < previous_final_upperband or previous_var > previous_final_upperband) else previous_final_upperband\n",
        "            final_lowerband = basic_lowerband if (\n",
        "                    basic_lowerband > previous_final_lowerband or previous_var < previous_final_lowerband) else previous_final_lowerband\n",
        "\n",
        "            if previous_pmax == previous_final_upperband:\n",
        "                pmaxc = final_upperband if varc <= final_upperband else final_lowerband\n",
        "            else:\n",
        "                pmaxc = final_lowerband if varc >= final_lowerband else final_upperband\n",
        "\n",
        "            pmax.append(pmaxc)\n",
        "            previous_var = varc\n",
        "            previous_final_upperband = final_upperband\n",
        "            previous_final_lowerband = final_lowerband\n",
        "            previous_pmax = pmaxc\n",
        "\n",
        "        del atr, previous_final_upperband, previous_final_lowerband, previous_var, previous_pmax, pmaxc\n",
        "        return pmax\n",
        "\n",
        "    def generate_signal(self, var, pmax):\n",
        "        \"\"\"Генерация сигнала на основе VAR и PMAX.\"\"\"\n",
        "\n",
        "        if len(var)== 0 or not pmax:\n",
        "            return None\n",
        "\n",
        "        last_var = var[-1]\n",
        "\n",
        "        previous_var = var[-2] if len(var) >= 2 else last_var\n",
        "        last_pmax = pmax[-1]\n",
        "        previous_pmax = pmax[-2] if len(pmax) >= 2 else last_pmax\n",
        "\n",
        "        if last_var > last_pmax and previous_var < previous_pmax:\n",
        "            del last_var, previous_var, last_pmax, previous_pmax\n",
        "            return 'buy'\n",
        "        elif last_var < last_pmax and previous_var > previous_pmax:\n",
        "            del last_var, previous_var, last_pmax, previous_pmax\n",
        "            return 'sell'\n",
        "        else:\n",
        "            del last_var, previous_var, last_pmax, previous_pmax\n",
        "            return 'hold'\n",
        "\n",
        "    async def process_buy_signal(self, ticker):\n",
        "        \"\"\"Обрабатывает BUY сигнал.\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Проверяем, если тикер уже в активных сделках\n",
        "            if ticker in open_trades:\n",
        "                return\n",
        "\n",
        "            # Формируем данные для классификатора\n",
        "            ticker_data = {\n",
        "                'open': open_price[ticker],\n",
        "                'close': close_price[ticker],\n",
        "                'high': high_price[ticker],\n",
        "                'low': low_price[ticker],\n",
        "                'volume': volume[ticker],\n",
        "            }\n",
        "            df = pd.DataFrame(ticker_data)\n",
        "            feature_calculator = ClassifierFeatureCalculator(df)\n",
        "\n",
        "            # Вычисляем фичи и предиктим через классификатор\n",
        "            features = feature_calculator.calculate_features(**params_for_classifier)\n",
        "            prediction = classifier_model['pipeline'].predict(features)\n",
        "\n",
        "            # Если модель классификации возвращает 1, открываем сделку\n",
        "            if prediction[0] == 1:\n",
        "                print(f'Проверка тикета {ticker} на предмет прохода/непрохода, результат 1 с оценкой {classifier_model[\"pipeline\"].predict_proba(features)}')\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "                print(f'Проверка тикета {ticker} на предмет прохода/непрохода, результат 0 с оценкой {classifier_model[\"pipeline\"].predict_proba(features)}')\n",
        "        except Exception as e:\n",
        "            bot.send_message(\n",
        "                      self.error_tg,\n",
        "                      f\"Ошибка в process_buy_signal {ticker} с ошибкой {e}\"\n",
        "                  )\n",
        "\n",
        "    '''async def close_trade(self, ticker, current_price, max_price, reason):\n",
        "        \"\"\"\n",
        "        Закрытие сделки.\n",
        "        \"\"\"\n",
        "        trade_data = open_trades.get(ticker, None)\n",
        "\n",
        "        if trade_data:\n",
        "            buy_price = trade_data['buy_price']\n",
        "            buy_time_str = trade_data['buy_time']\n",
        "\n",
        "            # Преобразуем строку в datetime, если это необходимо\n",
        "            if isinstance(buy_time_str, str):\n",
        "                buy_time = dt.strptime(buy_time_str, \"%Y-%m-%dT%H:%M:%S.%f\")  # Формат соответствует вашему примеру\n",
        "            else:\n",
        "                buy_time = buy_time_str  # Уже datetime\n",
        "\n",
        "            pnl = ((current_price - buy_price) / buy_price) * 100\n",
        "            trade_duration = (dt.utcnow() - buy_time).total_seconds() // 60\n",
        "\n",
        "            open_trades[ticker]['status'] = 'closed'\n",
        "\n",
        "            bot.send_message(\n",
        "                self.stat_tg,\n",
        "                f\"[TRADE CLOSED] Сделка для {ticker} закрыта по цене {current_price}, на отрезке максимальная цена была {max_price} \"\n",
        "                f\"с прибылью {pnl:.2f}% (продолжительность: {trade_duration:.2f} минут). \"\n",
        "                f\"Причина: {reason}.\"\n",
        "            )\n",
        "            self.process_trade(ticker,\n",
        "                               buy_time,\n",
        "                               buy_price,\n",
        "                               dt.utcnow(),\n",
        "                               current_price,\n",
        "                               pnl,\n",
        "                               reason)\n",
        "        else:bot.send_message(self.stats_tg, f\"По тикету {ticker} нет данных об открытых позициях\")'''\n",
        "\n",
        "    async def close_trade(self, ticker, current_price, max_price, reason):\n",
        "        \"\"\"\n",
        "        Закрытие сделки с универсальной обработкой временных меток.\n",
        "        Обрабатывает форматы:\n",
        "        - \"2025-05-30T16:45:00.805244\" (без временной зоны)\n",
        "        - \"2025-05-28T20:30:00+00:00\" (с временной зоной)\n",
        "        \"\"\"\n",
        "        trade_data = open_trades.get(ticker, None)\n",
        "\n",
        "        if trade_data:\n",
        "            buy_price = trade_data['buy_price']\n",
        "            buy_time_str = trade_data['buy_time']\n",
        "\n",
        "            # Универсальное преобразование времени\n",
        "            if isinstance(buy_time_str, str):\n",
        "                try:\n",
        "                    # Удаляем временную зону если она есть\n",
        "                    if '+' in buy_time_str:\n",
        "                        buy_time_str = buy_time_str.split('+')[0]\n",
        "                    # Удаляем микросекунды если они есть\n",
        "                    if '.' in buy_time_str:\n",
        "                        buy_time_str = buy_time_str.split('.')[0]\n",
        "\n",
        "                    # Парсим как naive datetime\n",
        "                    buy_time = dt.strptime(buy_time_str, \"%Y-%m-%dT%H:%M:%S\")\n",
        "\n",
        "                    # Добавляем 3 часа если нужно\n",
        "                    buy_time += timedelta(hours=3)\n",
        "\n",
        "                except Exception as e:\n",
        "                    bot.send_message(self.stat_tg, f\"Ошибка преобразования даты для {ticker}: {str(e)}\")\n",
        "                    return\n",
        "            else:\n",
        "                buy_time = buy_time_str  # Уже datetime\n",
        "                if buy_time.tzinfo is not None:  # Если есть временная зона\n",
        "                    buy_time = buy_time.replace(tzinfo=None)  # Удаляем временную зону\n",
        "                buy_time += timedelta(hours=3)\n",
        "\n",
        "            # Вычисляем PnL\n",
        "            pnl = ((current_price - buy_price) / buy_price) * 100\n",
        "\n",
        "            # Получаем текущее время как naive datetime\n",
        "            current_time = dt.utcnow()\n",
        "            trade_duration = (current_time - buy_time).total_seconds() // 60\n",
        "\n",
        "            open_trades[ticker]['status'] = 'closed'\n",
        "\n",
        "            bot.send_message(\n",
        "                    self.stat_tg,\n",
        "                    f\"[TRADE CLOSED] Сделка для {ticker} закрыта по цене {current_price}, на отрезке максимальная цена была {max_price} \"\n",
        "                    f\"с прибылью {pnl:.2f}% (продолжительность: {trade_duration:.2f} минут). \"\n",
        "                    f\"Причина: {reason}.\"\n",
        "                )\n",
        "\n",
        "            self.process_trade(ticker,\n",
        "                        buy_time,\n",
        "                        buy_price,\n",
        "                        current_time,\n",
        "                        current_price,\n",
        "                        pnl,\n",
        "                        reason)\n",
        "        else:\n",
        "            bot.send_message(self.stat_tg, f\"По тикету {ticker} нет данных об открытых позициях\")\n",
        "\n",
        "    def process_trade(self, ticker, alert_time, alert_price, exit_time, exit_price, pnl, reason):\n",
        "        # Определяем интервал\n",
        "        trade_duration = (exit_time - alert_time) // 1000\n",
        "        interval = '15m'  # if trade_duration < 5 * 3600 else '5m'\n",
        "\n",
        "        # Получаем данные\n",
        "        klines = find_klines(ticker)\n",
        "        #klines = fetch_tinkoff_candles(ticker, alert_time, exit_time, interval)\n",
        "        df = prepare_tinkoff_data(klines)\n",
        "\n",
        "        # Сохраняем график\n",
        "        chart_file_path = f\"/content/drive/MyDrive/t_ml/data/png/{ticker}_{dt.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
        "        save_tinkoff_trade_chart(df, chart_file_path, ticker, alert_price, alert_time, exit_time, exit_price,\n",
        "                                 pnl, reason)\n",
        "\n",
        "        # Отправляем график\n",
        "        caption = f\"{ticker}\\n\\nSG Model; PNL: {pnl:.2f}%\"\n",
        "        sent_message = bot.send_photo('-1002619839070', open(chart_file_path, 'rb'), caption=caption)\n",
        "        message_link = f\"/https://t.me/c/2619839070/{sent_message.message_id}\"\n",
        "\n",
        "        # Сохраняем и отправляем отчет\n",
        "        save_and_send_tinkoff_report(ticker, alert_price, alert_time, exit_time, exit_price, pnl, reason, message_link, tg_channels)\n",
        "\n",
        "    def _load_saved_data(self):\n",
        "        try:\n",
        "            with open(f'{self.path_to_save}open_price.txt', 'r') as f:\n",
        "                self.open_price = json.loads(f.read())\n",
        "\n",
        "            with open(f'{self.path_to_save}close_price.txt', 'r') as f:\n",
        "                self.close_price = json.loads(f.read())\n",
        "\n",
        "            with open(f'{self.path_to_save}high_price.txt', 'r') as f:\n",
        "                self.high_price = json.loads(f.read())\n",
        "\n",
        "            with open(f'{self.path_to_save}low_price.txt', 'r') as f:\n",
        "                self.low_price = json.loads(f.read())\n",
        "\n",
        "            with open(f'{self.path_to_save}volume.txt', 'r') as f:\n",
        "                self.volume = json.loads(f.read())\n",
        "\n",
        "            with open(f'{self.path_to_save}time_last_kline_start.txt', 'r') as f:\n",
        "                self.time_last_kline_start = json.loads(f.read())\n",
        "\n",
        "            with open(f'{self.path_to_save}time_last_kline_end.txt', 'r') as f:\n",
        "                self.time_last_kline_end = json.loads(f.read())\n",
        "\n",
        "            #print(\"Данные успешно загружены.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке данных: {e}\")\n",
        "            self.open_price, self.close_price = {}, {}\n",
        "            self.high_price, self.low_price = {}, {}\n",
        "            self.volume = {}\n",
        "            self.time_last_kline_start, self.time_last_kline_end = {}, {}\n",
        "\n",
        "    async def save_data(self):\n",
        "        \"\"\"Save data to files periodically.\"\"\"\n",
        "        while True:\n",
        "            await asyncio.sleep(30 * self.timestop)\n",
        "            try:\n",
        "                with open(f'{self.path_to_save}open_price.txt', 'w') as f:\n",
        "                    f.write(json.dumps(open_price))\n",
        "\n",
        "                with open(f'{self.path_to_save}close_price.txt', 'w') as f:\n",
        "                    f.write(json.dumps(close_price))\n",
        "\n",
        "                with open(f'{self.path_to_save}high_price.txt', 'w') as f:\n",
        "                    f.write(json.dumps(high_price))\n",
        "\n",
        "                with open(f'{self.path_to_save}low_price.txt', 'w') as f:\n",
        "                    f.write(json.dumps(low_price))\n",
        "\n",
        "                with open(f'{self.path_to_save}volume.txt', 'w') as f:\n",
        "                    f.write(json.dumps(volume))\n",
        "\n",
        "                #print(\"Данные успешно сохранены.\")\n",
        "            except Exception as e:\n",
        "                bot.send_message(self.error_tg, f'Error saving 1\\n{e}')\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "            try:\n",
        "                with open(f'{self.path_to_save}ma.txt', 'w') as f:\n",
        "                    f.write(json.dumps(ma))\n",
        "\n",
        "                with open(f'{self.path_to_save}pmax.txt', 'w') as f:\n",
        "                    f.write(json.dumps(pmax))\n",
        "\n",
        "                with open(f'{self.path_to_save}signals.txt', 'w') as f:\n",
        "                    f.write(json.dumps(signals))\n",
        "\n",
        "                with open(f'{self.path_to_save}open_trades.txt', 'w') as f:\n",
        "                    f.write(json.dumps(open_trades))\n",
        "\n",
        "                with open(f'{self.path_to_save}trading_data.txt', 'w') as f:\n",
        "                    f.write(json.dumps(trading_data))\n",
        "\n",
        "                with open(f'{self.path_to_save}time_last_kline_start.txt', 'w') as f:\n",
        "                    json.dump(time_last_kline_start, f)\n",
        "\n",
        "                with open(f'{self.path_to_save}time_last_kline_end.txt', 'w') as f:\n",
        "                    json.dump(time_last_kline_end, f)\n",
        "\n",
        "            except Exception as e:\n",
        "                bot.send_message(self.error_tg, f'Error saving 2\\n{e}')\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "    async def create_tickers(self):\n",
        "        \"\"\"Create a list of tickers based on specified sector and country.\"\"\"\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            instruments = await client.instruments.shares()\n",
        "            for share in instruments.instruments:\n",
        "                if (share.country_of_risk == self.country):  # and share.sector == self.sector):\n",
        "                    self.tickers[share.ticker] = [share.figi, share.name, share.sector]\n",
        "\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            instruments = await client.instruments.find_instrument(query=self.index)\n",
        "            for instrument in instruments.instruments:\n",
        "                self.indexes[instrument.ticker] = [instrument.figi, instrument.name, instrument.instrument_type]\n",
        "\n",
        "    async def update_trading_statuses(self):\n",
        "        \"\"\"Обновление статусов инструментов и поддержка актуального списка\"\"\"\n",
        "        try:\n",
        "            async with AsyncClient(self.token) as client:\n",
        "                instruments = await client.instruments.shares()\n",
        "                new_active = {}\n",
        "\n",
        "                # Собираем свежие данные\n",
        "                for share in instruments.instruments:\n",
        "                    if share.ticker in self.tickers and share.trading_status == 5:\n",
        "                        new_active[share.ticker] = {\n",
        "                            'figi': share.figi,\n",
        "                            'name': share.name,\n",
        "                            'status': share.trading_status\n",
        "                        }\n",
        "\n",
        "                # Удаляем тикеры с изменившимся статусом\n",
        "                removed_tickers = set(self.active_tickers.keys()) - set(new_active.keys())\n",
        "                for ticker in removed_tickers:\n",
        "                    del self.active_tickers[ticker]\n",
        "                    print(f'Удален тикер {ticker}')\n",
        "\n",
        "                # Добавляем новые тикеры\n",
        "                added_tickers = set(new_active.keys()) - set(self.active_tickers.keys())\n",
        "                for ticker in added_tickers:\n",
        "                    self.active_tickers[ticker] = new_active[ticker]\n",
        "                    print(f'Добавлен тикер {ticker}')\n",
        "\n",
        "            await asyncio.sleep(self.update_interval.total_seconds())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обновлении статусов: {e}\")\n",
        "            await asyncio.sleep(60)\n",
        "\n",
        "     async def compute_model_prediction(self, ticker):\n",
        "        if ticker not in open_trades or open_trades[ticker]['status'] != 'open':\n",
        "            return None\n",
        "\n",
        "        buy_time_str = open_trades[ticker]['buy_time']\n",
        "        buy_dt = datetime.fromisoformat(buy_time_str)\n",
        "\n",
        "        times_iso = time_last_kline_start.get(ticker, [])\n",
        "        dt_scale = [pd.to_datetime(s, utc=True) for s in times_iso]\n",
        "\n",
        "        entry_idx = bisect_left(dt_scale, buy_dt)\n",
        "\n",
        "        if entry_idx >= len(dt_scale):\n",
        "            return None\n",
        "\n",
        "        # Slice from entry to end\n",
        "        start_idx = entry_idx\n",
        "        df = pd.DataFrame({\n",
        "            'time': [pd.to_datetime(t, utc=True) for t in times_iso[start_idx:]],\n",
        "            'open': open_price[ticker][start_idx:],\n",
        "            'high': high_price[ticker][start_idx:],\n",
        "            'low': low_price[ticker][start_idx:],\n",
        "            'close': close_price[ticker][start_idx:],\n",
        "            'volume': volume[ticker][start_idx:],\n",
        "        })\n",
        "\n",
        "        if len(df) < 1:\n",
        "            return None\n",
        "\n",
        "        # Compute regimes\n",
        "        window = int(self.phase_df_params[ticker]['params']['moving_average_length'] * 9.5)\n",
        "        features = extract_features(df, window=window)\n",
        "        scaled = self.scaler_global.transform(features)\n",
        "        labels = self.kmeans_global.predict(scaled)\n",
        "        regime_series = pd.Series(labels, index=df.index)\n",
        "\n",
        "        window_size = int(self.phase_df_params[ticker]['params']['atr_period'] * 5.5)\n",
        "        smoother = FastRollingMode(window_size=window_size)\n",
        "        smoothed = [smoother.update(x) for x in labels]\n",
        "        smoothed_regime = pd.Series(smoothed, index=df.index)\n",
        "\n",
        "        # Generate signals\n",
        "        regime_params = prepare_regime_params(self.phase_df_params[ticker]['params'])\n",
        "        CV = AdaptiveTradingSystem(regime_params['base_params'])\n",
        "        df = CV.generate_adaptive_signals(df, regime_series=smoothed_regime)\n",
        "\n",
        "        # For real-time, set event_time and event_price for the first row (buy)\n",
        "        df.loc[0, 'event_time'] = df.loc[0, 'time']\n",
        "        df.loc[0, 'event_price'] = df.loc[0, 'close']\n",
        "        # No event_sell, but not needed for features\n",
        "\n",
        "        # No pnl, target\n",
        "\n",
        "        # Compute indicators\n",
        "        feats_list = self.final_cols[ticker]\n",
        "        params = build_feature_params(self.final_params[ticker])  # Assuming build_feature_params defined\n",
        "        gh, _ = calculate_indicators(df, feats_list, params=params, multy=False)\n",
        "\n",
        "        gh['batch'] = 0\n",
        "        gh['regime'] = smoothed_regime.astype('object')\n",
        "\n",
        "        # Compute predd using transformer\n",
        "        columns_for_neuro = feats_list + ['batch', 'regime', 'time']\n",
        "        gh_test = gh[columns_for_neuro]\n",
        "        X_test, _, _, _, _ = prepare_data_transformer(gh_test, 'normalized_target')  # y None\n",
        "\n",
        "        with warnings.catch_warnings(record=True):\n",
        "            warnings.simplefilter(\"always\")\n",
        "            predd = self.transformers[ticker].predict(X_test)\n",
        "\n",
        "        gh['predd'] = predd\n",
        "\n",
        "        # Add derived\n",
        "        gh['predd_shift_5'] = gh['predd'].shift(5).fillna(0)\n",
        "        gh['predd_pct'] = gh['predd'].pct_change(3).fillna(0)\n",
        "        gh['predd_var'] = gh['predd'].rolling(10).var().fillna(0)\n",
        "\n",
        "        # Prepare X for final predict (last row)\n",
        "        columns_for_model = feats_list + ['regime', 'predd', 'predd_var', 'predd_shift_5', 'predd_pct']\n",
        "        X_pred = gh[columns_for_model].iloc[-1:]\n",
        "\n",
        "        # Predict\n",
        "        model_data = self.models[ticker]\n",
        "        best_method = model_data['best_method']\n",
        "\n",
        "        if best_method == 'regressor':\n",
        "            pred = model_data['pipe_reg'].predict(X_pred)\n",
        "            pred = apply_linear_calibration(pred, model_data['calib_reg'])\n",
        "        elif best_method == 'ranker':\n",
        "            pred = model_data['pipe_rank'].predict(X_pred)\n",
        "            pred = ranker_postprocess_minus1_1(pred)\n",
        "        elif best_method == 'combined':\n",
        "            pred_reg = model_data['pipe_reg'].predict(X_pred)\n",
        "            pred_reg = apply_linear_calibration(pred_reg, model_data['calib_reg'])\n",
        "            pred_rank = model_data['pipe_rank'].predict(X_pred)\n",
        "            pred_rank = ranker_postprocess_minus1_1(pred_rank)\n",
        "            w = model_data['w_reg']\n",
        "            pred = pred_reg * w + pred_rank * (1 - w)\n",
        "\n",
        "        pred = float(pred[0])\n",
        "\n",
        "        threshold = model_data['threshold']  # Or 0.75 as example; but use from meta\n",
        "\n",
        "        if pred > threshold:\n",
        "            return pred, True  # Sell\n",
        "        else:\n",
        "            return pred, False\n",
        "\n",
        "    async def check_and_update_dicts(self):\n",
        "        required_tickers = self.ticker_to_figi.keys()\n",
        "        dicts = [\n",
        "            open_price,\n",
        "            close_price,\n",
        "            high_price,\n",
        "            low_price,\n",
        "            volume,\n",
        "            time_last_kline_start,\n",
        "            time_last_kline_end,\n",
        "            ma,\n",
        "            open_trades,\n",
        "            trading_data,\n",
        "            pmax,\n",
        "            signals\n",
        "        ]\n",
        "\n",
        "        for d in dicts:\n",
        "            for ticker in required_tickers:\n",
        "                if ticker not in d and d!=open_trades:\n",
        "                    d[ticker] = []\n",
        "                elif ticker not in d and d==open_trades:\n",
        "                    d[ticker] = {}\n",
        "\n",
        "    async def initialize_tickers_stream(self):\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            instruments = await client.instruments.shares()\n",
        "            for share in instruments.instruments:\n",
        "                if share.ticker in TICKERS:\n",
        "                    await asyncio.sleep(0.1)\n",
        "                    status = await client.market_data.get_trading_status(figi=share.figi)\n",
        "                    if status.trading_status !=1:\n",
        "                        self.ticker_to_figi[share.ticker] = share.figi\n",
        "                        self.figi_to_ticker[share.figi] = share.ticker\n",
        "                        self.tickers[share.ticker] = [share.figi, share.name, share.sector]\n",
        "                        self.active_tickers[share.ticker] = {\n",
        "                            'figi': share.figi,\n",
        "                            'name': share.name,\n",
        "                            'status': status.trading_status\n",
        "                        }\n",
        "            await self.check_and_update_dicts()\n",
        "            print(f\"[INIT] Активных тикеров: {len(self.ticker_to_figi)}\")\n",
        "\n",
        "    async def process_candle_stream(self, candle: Candle):\n",
        "        ticker = self.figi_to_ticker.get(candle.figi)\n",
        "        if not ticker:\n",
        "            return\n",
        "\n",
        "        ts = candle.time + timedelta(hours=3)\n",
        "        tc = ts + self.update_interval\n",
        "        open_price[ticker] = open_price[ticker][1:] + [float(candle.open.units + candle.open.nano * 1e-9)]\n",
        "        close_price[ticker] = close_price[ticker][1:] + [float(candle.close.units + candle.close.nano * 1e-9)]\n",
        "        high_price[ticker] = high_price[ticker][1:] + [float(candle.high.units + candle.high.nano * 1e-9)]\n",
        "        low_price[ticker] = low_price[ticker][1:] + [float(candle.low.units + candle.low.nano * 1e-9)]\n",
        "        volume[ticker] = volume[ticker][1:] + [candle.volume]\n",
        "        time_last_kline_start[ticker] = time_last_kline_start[ticker][1:] + [ts.isoformat()]\n",
        "        time_last_kline_end[ticker] = time_last_kline_end[ticker][1:] + [tc.isoformat()]\n",
        "\n",
        "        # Обработка сигналов\n",
        "        await self.calculate_indicators_and_signals(ticker)\n",
        "\n",
        "        #print(f\"[{ticker}] Time Start: {ts:%Y-%m-%d %H:%M} \"\n",
        "        #      f\"O: {open_price[ticker][-1]:.2f} H: {high_price[ticker][-1]:.2f} \"\n",
        "        #      f\"L: {low_price[ticker][-1]:.2f} C: {close_price[ticker][-1]:.2f} \"\n",
        "        #      f\"Signal: {signals[ticker]} Pmax: {pmax[ticker][-1]:.2f} Ma: {ma[ticker][-1]:.2f} \"\n",
        "        #      f\"Time Close: {tc:%Y-%m-%d %H:%M}\")\n",
        "\n",
        "\n",
        "        if signals[ticker] == 'buy':\n",
        "\n",
        "            way = phase_ticker_params[ticker]['way']\n",
        "            ticker_parametrs = prepare_regime_params(phase_ticker_params[ticker]['params'])['calc_params']\n",
        "            stop_loss_y_n = phase_ticker_params[ticker]['change_tp']\n",
        "            stop_loss = phase_ticker_params[ticker]['stop_loss_pst_tp']\n",
        "\n",
        "            if way == 'Базовый':\n",
        "\n",
        "                if stop_loss_y_n == 'No':\n",
        "\n",
        "                    open_trades[ticker] = {\n",
        "                          'buy_price': close_price[ticker][-1],\n",
        "                          'buy_time': (dt.utcnow() + timedelta(hours=3)).isoformat(),\n",
        "                          'max_price': close_price[ticker][-1],\n",
        "                          'status': 'open',\n",
        "                          'reason': 'BASE'\n",
        "                      }\n",
        "                else:\n",
        "                    open_trades[ticker] = {\n",
        "                          'buy_price': close_price[ticker][-1],\n",
        "                          'buy_time': (dt.utcnow() + timedelta(hours=3)).isoformat(),\n",
        "                          'max_price': close_price[ticker][-1],\n",
        "                          'status': 'open',\n",
        "                          'reason': 'BASE',\n",
        "                          'stop_loss': stop_loss\n",
        "                      }\n",
        "\n",
        "                bot.send_message(\n",
        "                    self.stat_tg,\n",
        "                    f\"[TRADE OPENED] {ticker} по {close_price[ticker][-1]} в {dt.utcnow()}\"\n",
        "                )\n",
        "\n",
        "\n",
        "            elif way == 'RSI':\n",
        "                rsi_length = ticker_parametrs['rsi_length']\n",
        "                use_smoothing = ticker_parametrs['use_smoothing']\n",
        "                smoothing_length = ticker_parametrs['smoothing_length']\n",
        "                smoothing_type = ticker_parametrs['smoothing_type']\n",
        "                alma_sigma = ticker_parametrs['alma_sigma']\n",
        "                rsi_overbought = ticker_parametrs['rsi_overbought']\n",
        "                rsi_oversold = ticker_parametrs['rsi_oversold']\n",
        "                use_knn = ticker_parametrs['use_knn']\n",
        "                knn_neighbors = ticker_parametrs['knn_neighbors']\n",
        "                knn_lookback = ticker_parametrs['knn_lookback']\n",
        "                knn_weight = ticker_parametrs['knn_weight']\n",
        "                feature_count = ticker_parametrs['feature_count']\n",
        "                use_filter = ticker_parametrs['use_filter']\n",
        "                filter_method = ticker_parametrs['filter_method']\n",
        "                filter_strength = ticker_parametrs['filter_strength']\n",
        "                sma_length = ticker_parametrs['sma_length']\n",
        "                ema_length = ticker_parametrs['ema_length']\n",
        "                rsi_theasold = ticker_parametrs['rsi_helbuth']\n",
        "\n",
        "                ml_rsi = MachineLearningRSI(\n",
        "                    rsi_length=int(rsi_length),\n",
        "                    use_smoothing=use_smoothing,\n",
        "                    smoothing_length=int(smoothing_length),\n",
        "                    smoothing_type=smoothing_type,\n",
        "                    alma_sigma=int(alma_sigma),\n",
        "                    rsi_overbought=int(rsi_overbought),\n",
        "                    rsi_oversold=int(rsi_oversold),\n",
        "                    use_knn=use_knn,\n",
        "                    knn_neighbors=int(knn_neighbors),\n",
        "                    knn_lookback=int(knn_lookback),\n",
        "                    knn_weight=knn_weight,\n",
        "                    feature_count=int(feature_count),\n",
        "                    use_filter=use_filter,\n",
        "                    filter_method=filter_method,\n",
        "                    filter_strength=filter_strength,\n",
        "                    sma_length=int(sma_length),\n",
        "                    ema_length=int(ema_length)\n",
        "                )\n",
        "\n",
        "                rsi, sma, ma_sma = ml_rsi.fit(pd.Series(close_price[ticker]))\n",
        "\n",
        "                if rsi>=rsi_theasold:\n",
        "                  if stop_loss_y_n == 'No':\n",
        "\n",
        "                      open_trades[ticker] = {\n",
        "                            'buy_price': close_price[ticker][-1],\n",
        "                            'buy_time': (dt.utcnow() + timedelta(hours=3)).isoformat(),\n",
        "                            'max_price': close_price[ticker][-1],\n",
        "                            'status': 'open',\n",
        "                            'reason': 'BASE'\n",
        "                        }\n",
        "                  else:\n",
        "                      open_trades[ticker] = {\n",
        "                            'buy_price': close_price[ticker][-1],\n",
        "                            'buy_time': (dt.utcnow() + timedelta(hours=3)).isoformat(),\n",
        "                            'max_price': close_price[ticker][-1],\n",
        "                            'status': 'open',\n",
        "                            'reason': 'BASE',\n",
        "                            'stop_loss': stop_loss\n",
        "                        }\n",
        "                  bot.send_message(\n",
        "                      self.stat_tg,\n",
        "                      f\"[TRADE OPENED] {ticker} по {close_price[ticker][-1]} в {dt.utcnow()}\"\n",
        "                  )\n",
        "                else:\n",
        "                  bot.send_message(\n",
        "                        self.stat_tg,\n",
        "                        f\"[TRADE NOT OPENED] {ticker} по {close_price[ticker][-1]} в {dt.utcnow()} не прошел проверку на трендовую линию\"\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "            elif way== 'RSI_SMA_EMA':\n",
        "                  rsi_length = ticker_parametrs['rsi_length']\n",
        "                  use_smoothing = ticker_parametrs['use_smoothing']\n",
        "                  smoothing_length = ticker_parametrs['smoothing_length']\n",
        "                  smoothing_type = ticker_parametrs['smoothing_type']\n",
        "                  alma_sigma = ticker_parametrs['alma_sigma']\n",
        "                  rsi_overbought = ticker_parametrs['rsi_overbought']\n",
        "                  rsi_oversold = ticker_parametrs['rsi_oversold']\n",
        "                  use_knn = ticker_parametrs['use_knn']\n",
        "                  knn_neighbors = ticker_parametrs['knn_neighbors']\n",
        "                  knn_lookback = ticker_parametrs['knn_lookback']\n",
        "                  knn_weight = ticker_parametrs['knn_weight']\n",
        "                  feature_count = ticker_parametrs['feature_count']\n",
        "                  use_filter = ticker_parametrs['use_filter']\n",
        "                  filter_method = ticker_parametrs['filter_method']\n",
        "                  filter_strength = ticker_parametrs['filter_strength']\n",
        "                  sma_length = ticker_parametrs['sma_length']\n",
        "                  ema_length = ticker_parametrs['ema_length']\n",
        "                  rsi_theasold = ticker_parametrs['rsi_helbuth']\n",
        "\n",
        "                  ml_rsi = MachineLearningRSI(\n",
        "                    rsi_length=int(rsi_length),\n",
        "                    use_smoothing=use_smoothing,\n",
        "                    smoothing_length=int(smoothing_length),\n",
        "                    smoothing_type=smoothing_type,\n",
        "                    alma_sigma=int(alma_sigma),\n",
        "                    rsi_overbought=int(rsi_overbought),\n",
        "                    rsi_oversold=int(rsi_oversold),\n",
        "                    use_knn=use_knn,\n",
        "                    knn_neighbors=int(knn_neighbors),\n",
        "                    knn_lookback=int(knn_lookback),\n",
        "                    knn_weight=knn_weight,\n",
        "                    feature_count=int(feature_count),\n",
        "                    use_filter=use_filter,\n",
        "                    filter_method=filter_method,\n",
        "                    filter_strength=filter_strength,\n",
        "                    sma_length=int(sma_length),\n",
        "                    ema_length=int(ema_length)\n",
        "                )\n",
        "\n",
        "                  rsi, sma, ma_sma = ml_rsi.fit(pd.Series(close_price[ticker]))\n",
        "\n",
        "                  if rsi>rsi_theasold and sma > close_price[ticker][-1] and ma_sma > close_price[ticker][-1]:\n",
        "\n",
        "                    if stop_loss_y_n == 'No':\n",
        "\n",
        "                        open_trades[ticker] = {\n",
        "                              'buy_price': close_price[ticker][-1],\n",
        "                              'buy_time': (dt.utcnow() + timedelta(hours=3)).isoformat(),\n",
        "                              'max_price': close_price[ticker][-1],\n",
        "                              'status': 'open',\n",
        "                              'reason': 'BASE'\n",
        "                          }\n",
        "                    else:\n",
        "                        open_trades[ticker] = {\n",
        "                              'buy_price': close_price[ticker][-1],\n",
        "                              'buy_time': (dt.utcnow() + timedelta(hours=3)).isoformat(),\n",
        "                              'max_price': close_price[ticker][-1],\n",
        "                              'status': 'open',\n",
        "                              'reason': 'BASE',\n",
        "                              'stop_loss': stop_loss\n",
        "                          }\n",
        "\n",
        "                    bot.send_message(\n",
        "                        self.stat_tg,\n",
        "                        f\"[TRADE OPENED] {ticker} по {close_price[ticker][-1]} в {dt.utcnow()}\"\n",
        "                    )\n",
        "                  else:\n",
        "                    bot.send_message(\n",
        "                        self.stat_tg,\n",
        "                        f\"[TRADE NOT OPENED] {ticker} по {close_price[ticker][-1]} в {dt.utcnow()} не прошел проверку на Трендовая + недельные скользящие\"\n",
        "                    )\n",
        "\n",
        "        if ticker in open_trades and open_trades[ticker]['status'] == 'open':\n",
        "            pred, should_sell = await self.compute_model_prediction(ticker)\n",
        "            if should_sell:\n",
        "                signals[ticker] = 'sell'\n",
        "                open_trades[ticker]['reason'] = 'model'\n",
        "                # Then the sell logic can trigger in the existing if\n",
        "\n",
        "        await self.check_open_trades(ticker)\n",
        "\n",
        "        if signals[ticker] == 'sell' and (isinstance(open_trades, dict)and isinstance(open_trades.get(ticker), dict)and open_trades[ticker].get('status') == 'open'):\n",
        "\n",
        "            reason = open_trades[ticker]['reason']\n",
        "\n",
        "            await self.close_trade(ticker, close_price[ticker][-1], open_trades[ticker]['max_price'], reason)\n",
        "\n",
        "        #check = await self.process_buy_signal(ticker)\n",
        "\n",
        "\n",
        "\n",
        "    async def check_open_trades(self, ticker):\n",
        "        \"\"\"Проверяет состояние открытой позиции по тикеру с защитой от ошибок\"\"\"\n",
        "        # Глобальные переменные (не атрибуты класса!)\n",
        "        global open_trades, close_price, signals\n",
        "\n",
        "        # 1. Безопасная проверка открытой позиции\n",
        "        if not (\n",
        "            isinstance(open_trades, dict)  # open_trades - это словарь\n",
        "            and isinstance(open_trades.get(ticker), dict)  # тикер есть в open_trades\n",
        "            and open_trades[ticker].get('status') == 'open'  # статус 'open'\n",
        "        ):\n",
        "            return\n",
        "\n",
        "        # 2. Безопасный доступ к ценам\n",
        "        if not (\n",
        "            isinstance(close_price, dict)  # close_price - словарь\n",
        "            and isinstance(close_price.get(ticker), list)  # тикер есть в close_price\n",
        "            and len(close_price[ticker]) > 0  # есть хотя бы одна цена\n",
        "        ):\n",
        "            return\n",
        "\n",
        "        trade_info = open_trades[ticker]\n",
        "        current_price = close_price[ticker][-1]\n",
        "\n",
        "        # 3. Обновление максимальной цены\n",
        "        if 'max_price' in trade_info and current_price > trade_info['max_price']:\n",
        "            trade_info['max_price'] = current_price\n",
        "\n",
        "        # 4. Проверка стоп-лосса (с защитой от отсутствия ключей)\n",
        "        if all(k in trade_info for k in ('stop_loss', 'buy_price')):\n",
        "            stop_loss_price = trade_info['buy_price'] * (1 - trade_info['stop_loss'])\n",
        "            if current_price <= stop_loss_price:\n",
        "                signals[ticker] = 'sell'  # Глобальный словарь signals\n",
        "                trade_info.update({\n",
        "                    'reason': 'TAKE_LOSS',\n",
        "                    'status': 'closed',\n",
        "                    'close_price': current_price,\n",
        "                    'close_time': dt.utcnow().isoformat()\n",
        "                })\n",
        "\n",
        "    async def check_missing_periodically(self):\n",
        "        \"\"\"Проверка пропущенных тикеров с учетом ближайшей свечной 5-минутки\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                now = dt.utcnow()\n",
        "\n",
        "                # Рассчитываем ближайшую 5-минутку (или кратную self.timestop минуту)\n",
        "                minutes = (now.minute // self.timestop) * self.timestop\n",
        "                next_interval = now.replace(minute=minutes, second=0, microsecond=0)\n",
        "\n",
        "                if next_interval <= now:\n",
        "                    # Если текущая 5-минутка уже прошла, берем следующую\n",
        "                    next_interval += timedelta(minutes=self.timestop)\n",
        "\n",
        "                # Добавляем 10 секунд к расчетному времени\n",
        "                next_check_time = next_interval + timedelta(seconds=10)\n",
        "\n",
        "                # Рассчитываем время до следующей проверки\n",
        "                time_to_wait = (next_check_time - now).total_seconds()\n",
        "\n",
        "                # Ждем до ближайшей 5-минутки + 10 секунд\n",
        "                await asyncio.sleep(time_to_wait)\n",
        "\n",
        "                # Рассчитываем метку времени завершения предыдущего интервала\n",
        "                expected_end = next_interval\n",
        "                expected_end_ts = int(expected_end.timestamp() * 1000)\n",
        "\n",
        "                # Проверяем, какие тикеры не обновились\n",
        "                missing = []\n",
        "                for ticker in self.active_tickers:\n",
        "                    last_ts = time_last_kline_end.get(ticker, 0)\n",
        "\n",
        "                    # Если last_ts — список или кортеж, берем последний элемент\n",
        "                    if isinstance(last_ts, (list, tuple)):\n",
        "                        last_ts = last_ts[-1] if last_ts else 0\n",
        "\n",
        "                    if last_ts < expected_end_ts:\n",
        "                        missing.append(ticker)\n",
        "\n",
        "                # Выводим предупреждение, если есть пропущенные тикеры\n",
        "                if missing:\n",
        "                    print(f\"\\n[WARNING] Missing data for {len(missing)} tickers \"\n",
        "                          f\"in period ending {expected_end:%H:%M}:\")\n",
        "                    for t in sorted(missing):\n",
        "                        print(f\"- {t}\")\n",
        "                    print(f\"Total missing: {len(missing)}\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in missing tickers check: {e}\")\n",
        "                await asyncio.sleep(10)\n",
        "\n",
        "    '''async def stream_candles_batch_with_backoff(self, figi_batch, chunk_id):\n",
        "        backoff = 1\n",
        "        max_backoff = 60  # секунд\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                async with AsyncClient(self.token) as client:\n",
        "                    async def request_iterator():\n",
        "                        yield MarketDataRequest(\n",
        "                            subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                                subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_SUBSCRIBE,\n",
        "                                instruments=[\n",
        "                                    CandleInstrument(figi=figi, interval=self.candleintervall)\n",
        "                                    for figi in figi_batch\n",
        "                                ],\n",
        "                                waiting_close=True\n",
        "                            )\n",
        "                        )\n",
        "                        while True:\n",
        "                            await asyncio.sleep(1)\n",
        "\n",
        "                    #print(f\"[Stream Init] Чанк {chunk_id} FIGIs: {len(figi_batch)}\")\n",
        "\n",
        "                    stream = client.market_data_stream.market_data_stream(request_iterator())\n",
        "\n",
        "                    # Сохраняем время первого запуска\n",
        "                    self.stream_last_activity[chunk_id] = dt.utcnow()\n",
        "\n",
        "                    async for response in stream:\n",
        "                        # Это критически важно — обновить время активности ТОЛЬКО при получении свечи!\n",
        "                        if response.candle:\n",
        "                            self.stream_last_activity[chunk_id] = dt.utcnow()\n",
        "                            await self.process_candle_stream(response.candle)\n",
        "\n",
        "                        # Пример: получение и логирование метаданных (по возможности)\n",
        "                        if hasattr(stream, \"initial_metadata\"):\n",
        "                            metadata = await stream.initial_metadata()\n",
        "                            if metadata:\n",
        "                                print(f\"[Meta] Chunk {chunk_id}: {metadata}\")\n",
        "\n",
        "                    # Если цикл завершился — сбрасываем backoff\n",
        "                    backoff = 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[BACKOFF] Ошибка в чанке {chunk_id}: {e}\")\n",
        "                bot.send_message(self.error_tg, f\"[Stream BACKOFF] Chunk {chunk_id} error: {e}\")\n",
        "                await asyncio.sleep(backoff + random.uniform(0, 2))\n",
        "                backoff = min(max_backoff, backoff * 2)\n",
        "\n",
        "    async def stream_candles_batch(self, figi_batch: list):\n",
        "        \"\"\"Обрабатывает стрим по группе тикеров.\"\"\"\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            async def request_iterator():\n",
        "                yield MarketDataRequest(\n",
        "                    subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                        subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_SUBSCRIBE,\n",
        "                        instruments=[\n",
        "                            CandleInstrument(figi=figi, interval=self.candleintervall)\n",
        "                            for figi in figi_batch\n",
        "                        ],\n",
        "                        waiting_close=True\n",
        "                    )\n",
        "                )\n",
        "                while True:\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            try:\n",
        "                stream = client.market_data_stream.market_data_stream(request_iterator())\n",
        "                async for response in stream:\n",
        "                    if response.candle:\n",
        "                        await self.process_candle_stream(response.candle)\n",
        "            except Exception as e:\n",
        "                print(f\"[Batch Stream Error] {e}\")\n",
        "                bot.send_message(self.error_tg, f\"[Ошибка потока в чанке]: {e}\")\n",
        "                raise e\n",
        "\n",
        "    async def candle_stream_handler(self):\n",
        "        figi_list = list(self.figi_to_ticker.keys())\n",
        "        chunk_size = 20\n",
        "        figi_chunks = list(split_figis(figi_list, chunk_size))\n",
        "\n",
        "        self.stream_last_activity = {}\n",
        "        self.stream_tasks = {}\n",
        "\n",
        "        for chunk in figi_chunks:\n",
        "            chunk_id = hashlib.md5(\",\".join(chunk).encode()).hexdigest()[:8]\n",
        "            self.stream_last_activity[chunk_id] = dt.utcnow()\n",
        "            task = asyncio.create_task(self.stream_candles_batch_with_backoff(chunk, chunk_id))\n",
        "            self.stream_tasks[chunk_id] = task\n",
        "\n",
        "        # Отдельная задача для watchdog (запускать только один раз!)\n",
        "        if not hasattr(self, '_watchdog_started'):\n",
        "            self._watchdog_started = True\n",
        "            asyncio.create_task(self.watchdog_controller(chunk_size=chunk_size, timeout_minutes=60))\n",
        "\n",
        "    async def watchdog_controller(self, chunk_size=20, timeout_minutes=3):\n",
        "        print(\"[WATCHDOG] Контроллер запущен\")\n",
        "        while True:\n",
        "            now = dt.utcnow()\n",
        "            for chunk_id, last_time in list(self.stream_last_activity.items()):\n",
        "                if (now - last_time) > timedelta(minutes=timeout_minutes):\n",
        "                    print(f\"[WATCHDOG] Чанк завис: {chunk_id}. Перезапуск...\")\n",
        "\n",
        "                    # Завершаем старую задачу\n",
        "                    task = self.stream_tasks.get(chunk_id)\n",
        "                    if task:\n",
        "                        task.cancel()\n",
        "                        await asyncio.sleep(1)\n",
        "\n",
        "                    # Восстанавливаем FIGI для чанка\n",
        "                    all_figi = list(self.figi_to_ticker.keys())\n",
        "                    for chunk in split_figis(all_figi, chunk_size):\n",
        "                        test_id = hashlib.md5(\",\".join(chunk).encode()).hexdigest()[:8]\n",
        "                        if test_id == chunk_id:\n",
        "                            new_task = asyncio.create_task(self.stream_candles_batch_with_backoff(chunk, chunk_id))\n",
        "                            self.stream_tasks[chunk_id] = new_task\n",
        "                            self.stream_last_activity[chunk_id] = dt.utcnow()\n",
        "                            print(f\"[WATCHDOG] Чанк {chunk_id} перезапущен.\")\n",
        "                            break\n",
        "\n",
        "            await asyncio.sleep(30)'''\n",
        "\n",
        "    '''async def candle_stream_handler(self):\n",
        "        \"\"\"Инициализирует несколько параллельных стримов по чанкам FIGI.\"\"\"\n",
        "        #await self.initialize_tickers_stream()\n",
        "        #await self.backfill_missing_candles()\n",
        "\n",
        "        figi_list = list(self.figi_to_ticker.keys())\n",
        "        chunk_size = 15  # 🔧 Настраиваемый размер чанка\n",
        "        figi_chunks = list(split_figis(figi_list, chunk_size))\n",
        "\n",
        "        # Ограничиваем количество параллельных стримов\n",
        "        max_concurrent_streams = 3  # Не более 3 чанков одновременно\n",
        "        semaphore = asyncio.Semaphore(max_concurrent_streams)\n",
        "\n",
        "        async def run_chunk(chunk):\n",
        "            async with semaphore:\n",
        "                await self.stream_candles_batch(chunk)\n",
        "\n",
        "        #tasks = []\n",
        "        #for chunk in figi_chunks:\n",
        "        #    task = asyncio.create_task(self.stream_candles_batch(chunk))\n",
        "        #    tasks.append(task)\n",
        "\n",
        "        tasks = []\n",
        "        for chunk in figi_chunks:\n",
        "            tasks.append(run_chunk(chunk))\n",
        "\n",
        "        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_EXCEPTION)\n",
        "\n",
        "        # Ждём завершения хотя бы одного потока (ошибка/завершение)\n",
        "        #done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_EXCEPTION)\n",
        "\n",
        "        # Обрабатываем завершившиеся задачи\n",
        "        for d in done:\n",
        "            if d.exception():\n",
        "                print(f\"[Stream ERROR] Один из потоков вернул ошибку: {d.exception()}\")\n",
        "                # Отменяем все оставшиеся стримы\n",
        "                for p in pending:\n",
        "                    p.cancel()\n",
        "                raise d.exception()'''\n",
        "\n",
        "    async def candle_stream_handler(self):\n",
        "        \"\"\"\n",
        "        Один устойчивый поток для получения свечей по всем FIGI.\n",
        "        Без разбивки на чанки. Без параллельных стримов.\n",
        "        \"\"\"\n",
        "        await self.initialize_tickers_stream()\n",
        "        await self.backfill_missing_candles()\n",
        "\n",
        "        figi_list = list(self.figi_to_ticker.keys())\n",
        "\n",
        "        async with AsyncClient(self.token) as client:\n",
        "\n",
        "            async def request_iterator():\n",
        "                # Сначала отписываемся от всех FIGI (на случай предыдущих подписок)\n",
        "                yield MarketDataRequest(\n",
        "                    subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                        subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_UNSUBSCRIBE,\n",
        "                        instruments=[\n",
        "                            CandleInstrument(figi=figi, interval=self.candleintervall)\n",
        "                            for figi in figi_list\n",
        "                        ],\n",
        "                        waiting_close=True\n",
        "                    )\n",
        "                )\n",
        "                await asyncio.sleep(0.5)  # Дать API обработать отписку\n",
        "\n",
        "                # Теперь подписываемся на все FIGI одним запросом\n",
        "                yield MarketDataRequest(\n",
        "                    subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                        subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_SUBSCRIBE,\n",
        "                        instruments=[\n",
        "                            CandleInstrument(figi=figi, interval=self.candleintervall)\n",
        "                            for figi in figi_list\n",
        "                        ],\n",
        "                        waiting_close=True\n",
        "                    )\n",
        "                )\n",
        "                # Поддерживаем жизнь стрима\n",
        "                while True:\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            try:\n",
        "                stream = client.market_data_stream.market_data_stream(request_iterator())\n",
        "\n",
        "                async for response in stream:\n",
        "                    if response.candle:\n",
        "                        await self.process_candle_stream(response.candle)\n",
        "\n",
        "            except AioRpcError as e:\n",
        "                status = e.code()\n",
        "                description = e.details()\n",
        "                print(f\"[gRPC Error]: {status}, {description}\")\n",
        "                bot.send_message(self.error_tg, f\"[gRPC Error]: {status}, {description}\")\n",
        "                raise e  # Позволит внешней логике обработать и перезапустить\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[Stream Exception]: {e}\")\n",
        "                bot.send_message(self.error_tg, f\"[Stream Exception]: {e}\")\n",
        "                raise e\n",
        "\n",
        "    '''async def candle_stream_handler(self):\n",
        "        async with AsyncClient(self.token) as client:\n",
        "            await self.initialize_tickers_stream()\n",
        "            await self.backfill_missing_candles()\n",
        "\n",
        "            figi_list = list(self.figi_to_ticker.keys())\n",
        "\n",
        "            yield MarketDataRequest(\n",
        "                subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                    subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_UNSUBSCRIBE,\n",
        "                    instruments=[\n",
        "                        CandleInstrument(figi=figi, interval=self.candleintervall)\n",
        "                        for figi in figi_list\n",
        "                    ],\n",
        "                    waiting_close=True\n",
        "                )\n",
        "            )\n",
        "            await asyncio.sleep(0.5)\n",
        "\n",
        "            async def request_iterator():\n",
        "                yield MarketDataRequest(\n",
        "                    subscribe_candles_request=SubscribeCandlesRequest(\n",
        "                        subscription_action=SubscriptionAction.SUBSCRIPTION_ACTION_SUBSCRIBE,\n",
        "                        instruments=[\n",
        "                            CandleInstrument(\n",
        "                                figi=figi,\n",
        "                                interval=self.candleintervall\n",
        "                            ) for figi in figi_list\n",
        "                        ],\n",
        "                        waiting_close=True\n",
        "                    )\n",
        "                )\n",
        "                while True:\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            try:\n",
        "                stream = client.market_data_stream.market_data_stream(request_iterator())\n",
        "                async for response in stream:\n",
        "                    if response.candle:\n",
        "                        await self.process_candle_stream(response.candle)\n",
        "            except Exception as e:\n",
        "                print(f\"[Stream Error] {e}\")\n",
        "                bot.send_message(self.error_tg, f\"[Ошибка потока]: {e}\")\n",
        "                await asyncio.sleep(10)'''\n",
        "\n",
        "\n",
        "    async def monitor_memory_usage(self):\n",
        "        \"\"\"Функция для мониторинга использования памяти.\"\"\"\n",
        "        process = psutil.Process()  # Получаем текущий процесс\n",
        "        while True:\n",
        "            try:\n",
        "                # Получаем информацию об использовании памяти\n",
        "                mem_info = process.memory_info()\n",
        "                rss_memory = mem_info.rss / 1024 ** 2  # Resident Set Size (в МБ)\n",
        "                vms_memory = mem_info.vms / 1024 ** 2  # Virtual Memory Size (в МБ)\n",
        "                memory_usage_message = (\n",
        "                    f\"Использование памяти:\\n\"\n",
        "                    f\"- RSS (физическая память): {rss_memory:.2f} МБ\\n\"\n",
        "                    f\"- VMS (виртуальная память): {vms_memory:.2f} МБ\"\n",
        "                )\n",
        "                # Отправляем сообщение в Telegram\n",
        "                bot.send_message(self.memmory_id, memory_usage_message)\n",
        "            except Exception as e:\n",
        "                # Логируем ошибки в случае проблем\n",
        "                bot.send_message(self.memmory_id, f\"Ошибка мониторинга памяти: {e}\")\n",
        "            # Ждем 60 секунд до следующей проверки\n",
        "            await asyncio.sleep(5*60)\n",
        "\n",
        "    async def run(self):\n",
        "          while True:\n",
        "              try:\n",
        "                  await self.create_tickers()  # Инициализация тикеров (подписки и т.д.)\n",
        "\n",
        "                  # Выполняется один раз при первом запуске\n",
        "                  #if not getattr(self, \"initialized\", False):\n",
        "                  #    await self.initialize_tickers_stream()\n",
        "                  #    await self.backfill_missing_candles()\n",
        "                  #    self.initialized = True  # Больше не выполнять\n",
        "\n",
        "                  # Основные задачи (стрим и watchdog)\n",
        "                  tasks = [\n",
        "                      self.candle_stream_handler(),\n",
        "                      #self.save_data(),\n",
        "                      self.monitor_memory_usage()\n",
        "                  ]\n",
        "                  results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "                  for res in results:\n",
        "                      if isinstance(res, Exception):\n",
        "                          print(f\"[EXCEPTION] Задача вернула исключение: {res}\")\n",
        "                          raise res  # Это заставит выйти из try и сработает перезапуск\n",
        "\n",
        "                  #await asyncio.gather(*tasks)\n",
        "\n",
        "              except AioRpcError as e:\n",
        "                  # Обработка ошибок gRPC-потока (в том числе RST_STREAM code 8)\n",
        "                  status_code = e.code()\n",
        "                  description = e.details()\n",
        "\n",
        "                  if status_code in (StatusCode.CANCELLED, StatusCode.UNAVAILABLE):\n",
        "                      print(f\"[RESTART] gRPC поток отменён: {status_code}, {description}. Перезапуск через 10 секунд...\")\n",
        "                      bot.send_message(self.error_tg, f\"[RESTART] gRPC ошибка: {status_code}, {description}\")\n",
        "                      await asyncio.sleep(10)\n",
        "                      continue\n",
        "\n",
        "                  else:\n",
        "                      print(f\"[CRITICAL] gRPC критическая ошибка: {status_code} - {description}\")\n",
        "                      bot.send_message(self.error_tg, f\"[CRITICAL] Ошибка gRPC: {status_code} — {description}\")\n",
        "                      await asyncio.sleep(30)\n",
        "                      continue\n",
        "\n",
        "              except Exception as e:\n",
        "                  error_message = str(e)\n",
        "\n",
        "                  if \"Remote end closed connection without response\" in error_message \\\n",
        "                    or \"RST_STREAM\" in error_message \\\n",
        "                    or \"error code 8\" in error_message:\n",
        "                      print(\"[RESTART] Обнаружен обрыв соединения. Перезапуск бота через 10 секунд...\")\n",
        "                      bot.send_message(self.error_tg, \"[RESTART] Обрыв соединения (RST_STREAM). Перезапуск бота.\")\n",
        "                      await asyncio.sleep(10)\n",
        "                      continue\n",
        "\n",
        "                  else:\n",
        "                      # Все остальные ошибки\n",
        "                      print(f\"[CRITICAL] Необработанная ошибка: {e}\")\n",
        "                      bot.send_message(self.error_tg, f\"[CRITICAL] Необработанная ошибка: {e}\")\n",
        "                      await asyncio.sleep(30)\n",
        "                      continue\n",
        "\n",
        "def split_figis(figi_list, chunk_size):\n",
        "    \"\"\"Разбивает список FIGI на чанки фиксированного размера.\"\"\"\n",
        "    for i in range(0, len(figi_list), chunk_size):\n",
        "        yield figi_list[i:i + chunk_size]\n",
        "\n",
        "\n",
        "# Main entry point\n",
        "if __name__ == \"__main__\":\n",
        "    token = 't.bWjxPlU6vz77qoqS754OKy0QorLDaZP-CE091dhGl56v7GHrqgF-mQAdWaeRg2kDRJmmxzvaaOwKUTxW6dnOKg'\n",
        "    path_to_save = path_to_save#'/projects/LONG_ALERT/data/'\n",
        "    sector = SECTOR\n",
        "    country = COUNTRY\n",
        "    period = 15\n",
        "    index = \"IMOEX2\"\n",
        "    tg_channels = {\n",
        "        'error': '-1002596481496',\n",
        "        'a1_screen': '-1002555215009',\n",
        "        'memory': '-1002277392256',\n",
        "        'stat': '-1002681271607',\n",
        "        'sells': '-1002619839070'\n",
        "    }\n",
        "\n",
        "    collector = TinkoffInvestDataCollector(token, path_to_save, sector, country, tg_channels, period, index)\n",
        "\n",
        "    # Запускаем асинхронный код\n",
        "    #asyncio.run(collector.run())\n",
        "    async def run_at_specific_times():\n",
        "        while True:\n",
        "            now = datetime.datetime.now()\n",
        "            current_minute = now.minute\n",
        "            current_second = now.second\n",
        "            target_minutes = [0, 15, 30, 45]\n",
        "\n",
        "            # Находим следующий 15-минутный интервал\n",
        "            next_target = None\n",
        "            for minute in target_minutes:\n",
        "                if minute > current_minute:\n",
        "                    next_target = minute\n",
        "                    break\n",
        "\n",
        "            if next_target is None:  # Если текущая минута больше 45\n",
        "                next_target = 0\n",
        "                wait_minutes = 60 - current_minute\n",
        "            else:\n",
        "                wait_minutes = next_target - current_minute\n",
        "\n",
        "            # Вычисляем оставшиеся секунды до следующего интервала\n",
        "            remaining_seconds = wait_minutes * 60 - current_second\n",
        "\n",
        "            # Логика запуска:\n",
        "            # Если до следующего интервала <= 8 минут (480 секунд) - ждем его наступления\n",
        "            # Если > 8 минут - запускаем немедленно\n",
        "            if remaining_seconds <= 60*9:  # 8 минут = 480 секунд\n",
        "                # Ждем наступления следующего интервала\n",
        "                sleep_seconds = remaining_seconds\n",
        "                print(f\"До следующего интервала {next_target} минут осталось {remaining_seconds//60} мин {remaining_seconds%60} сек - ждем его наступления\")\n",
        "            else:\n",
        "                # Запускаем немедленно\n",
        "                print(f\"До следующего интервала {wait_minutes} мин - запускаем немедленно\")\n",
        "                await collector.run()\n",
        "\n",
        "                # После выполнения вычисляем время до следующего запуска\n",
        "                now_after = datetime.datetime.now()\n",
        "                current_minute_after = now_after.minute\n",
        "                current_second_after = now_after.second\n",
        "\n",
        "                # Находим следующий интервал\n",
        "                for minute in target_minutes:\n",
        "                    if minute > current_minute_after:\n",
        "                        next_target_after = minute\n",
        "                        break\n",
        "                else:\n",
        "                    next_target_after = 0\n",
        "\n",
        "                # Вычисляем сколько ждать\n",
        "                if next_target_after == 0:\n",
        "                    wait_minutes_after = 60 - current_minute_after\n",
        "                else:\n",
        "                    wait_minutes_after = next_target_after - current_minute_after\n",
        "\n",
        "                remaining_seconds_after = wait_minutes_after * 60 - current_second_after\n",
        "                sleep_seconds = remaining_seconds_after\n",
        "\n",
        "            print(f\"Следующая проверка через {sleep_seconds//60} мин {sleep_seconds%60} сек\")\n",
        "            await asyncio.sleep(sleep_seconds)\n",
        "\n",
        "    # Запускаем асинхронный код\n",
        "    asyncio.run(run_at_specific_times())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69IK4iD9fkev"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zKykSME4apGW7b51jnfLmZiT1XHg3wAD",
      "authorship_tag": "ABX9TyPEB+ibZf2aEitgfLkt1xMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}